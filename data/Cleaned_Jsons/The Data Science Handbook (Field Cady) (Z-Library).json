[
    {
        "text": "the data science handbook"
    },
    {
        "text": "the data science handbook field cady"
    },
    {
        "text": "this edition first published 2017 \u00a9 2017 john wiley & sons, inc. all rights reserved."
    },
    {
        "text": "no part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by law."
    },
    {
        "text": "advice on how to obtain permission to reuse material from this title is available at http:// www.wiley.com/go/permissions."
    },
    {
        "text": "the right of field cady to be identified as the author(s) of this work has been asserted in accordance with law."
    },
    {
        "text": "registered offices john wiley & sons, inc., 111 river street, hoboken, nj 07030, usa editorial office 111 river street, hoboken, nj 07030, usa for details of our global editorial offices, customer services, and more information about wiley products visit us at www.wiley.com."
    },
    {
        "text": "wiley also publishes its books in a variety of electronic formats and by print-on-demand."
    },
    {
        "text": "some content that appears in standard print versions of this book may not be available in other formats."
    },
    {
        "text": "limit of liability/disclaimer of warranty matlab\u00ae is a trademark of the mathworks, inc. and is used with permission."
    },
    {
        "text": "the mathworks does not warrant the accuracy of the text or exercises in this book."
    },
    {
        "text": "this work\u2019s use or discussion of matlab\u00ae software or related products does not constitute endorsement or sponsorship by the mathworks of a particular pedagogical approach or particular use of the matlab\u00ae software."
    },
    {
        "text": "while the publisher and authors have used their best efforts in preparing this work, they make no representations or warranties with respect to the accuracy or completeness of the contents of this work and specifically disclaim all warranties, including without limitation any implied warranties of merchantability or fitness for a particular purpose."
    },
    {
        "text": "no warranty may be created or extended by sales representatives, written sales materials or promotional statements for this work."
    },
    {
        "text": "the fact that an organization, website, or product is referred to in this work as a citation and/or potential source of further information does not mean that the publisher and authors endorse the information or services the organization, website, or product may provide or recommendations it may make."
    },
    {
        "text": "this work is sold with the understanding that the publisher is not engaged in rendering professional services."
    },
    {
        "text": "the advice and strategies contained herein may not be suitable for your situation."
    },
    {
        "text": "you should consult with a specialist where appropriate."
    },
    {
        "text": "further, readers should be aware that websites listed in this work may have changed or disappeared between when this work was written and when it is read."
    },
    {
        "text": "neither the publisher nor authors shall be liable for any loss of profit or any other commercial damages, including but not limited to special, incidental, consequential, or other damages."
    },
    {
        "text": "library of congress cataloguing-in-publication data names: cady, field, 1984- author."
    },
    {
        "text": "title: the data science handbook / field cady."
    },
    {
        "text": "description: hoboken, nj: john wiley & sons, inc., 2017."
    },
    {
        "text": "| includes bibliographical references and index."
    },
    {
        "text": "identifiers: lccn 2016043329 (print) | lccn 2016046263 (ebook) | isbn 9781119092940 (cloth) | isbn 9781119092933 (pdf) | isbn 9781119092926 (epub) subjects: lcsh: databases--handbooks, manuals, etc."
    },
    {
        "text": "| statistics--data processing--handbooks, manuals, etc."
    },
    {
        "text": "| big data--handbooks, manuals, etc."
    },
    {
        "text": "| information theory--handbooks, manuals, etc."
    },
    {
        "text": "classification: lcc qa76.9.d32 c33 2017 (print) | lcc qa76.9.d32 (ebook) | ddc 005.74--dc23 lc record available at https://lccn.loc.gov/2016043329 cover image: \u0441\u0435\u0440\u0433\u0435\u0438\u0306 \u0445\u0430\u043a\u0438\u043c\u0443\u043b\u043b\u0438\u043d/gettyimages cover design by wiley printed in the united states of america set in 10/12pt warnock by spi global, chennai, india 10 9 8 7 6 5 4 3 2 1"
    },
    {
        "text": "to my wife, ryna."
    },
    {
        "text": "thank you honey, for your support and for always believing in me."
    },
    {
        "text": "vii contents preface xvii 1 introduction: becoming a unicorn 1 1.1 aren\u2019t data scientists just overpaid statisticians?"
    },
    {
        "text": "2 1.2 how is this book organized?"
    },
    {
        "text": "3 1.3 how to use this book?"
    },
    {
        "text": "3 1.4 why is it all in pythontm, anyway?"
    },
    {
        "text": "4 1.5 example code and datasets 4 1.6 parting words 5 part i the stuff you\u2019ll always use 7 2 the data science road map 9 2.1 frame the problem 10 2.2 understand the data: basic questions 11 2.3 understand the data: data wrangling 12 2.4 understand the data: exploratory analysis 13 2.5 extract features 14 2.6 model 15 2.7 present results 15 2.8 deploy code 16 2.9 iterating 16 2.10 glossary 17 3 programming languages 19 3.1 why use a programming language?"
    },
    {
        "text": "what are the other options?"
    },
    {
        "text": "19 3.2 a survey of programming languages for data science 20 3.2.1 python 20 3.2.2 r 21 3.2.3 matlab\u00ae and octave 21 3.2.4 sas\u00ae 21"
    },
    {
        "text": "contents viii 3.2.5 scala\u00ae 22 3.3 python crash course 22 3.3.1 a note on versions 22 3.3.2 \u201chello world\u201d script 23 3.3.3 more complicated script 23 3.3.4 atomic data types 26 3.4 strings 27 3.4.1 comments and docstrings 28 3.4.2 complex data types 29 3.4.3 lists 29 3.4.4 strings and lists 30 3.4.5 tuples 31 3.4.6 dictionaries 31 3.4.7 sets 32 3.5 defining functions 32 3.5.1 for loops and control structures 33 3.5.2 a few key functions 34 3.5.3 exception handling 35 3.5.4 libraries 35 3.5.5 classes and objects 35 3.5.6 gotcha: hashable and unhashable types 36 3.6 python\u2019s technical libraries 37 3.6.1 data frames 38 3.6.2 series 39 3.6.3 joining and grouping 40 3.7 other python resources 42 3.8 further reading 42 3.9 glossary 43 3a interlude: my personal toolkit 45 4 data munging: string manipulation, regular expressions, and data cleaning 47 4.1 the worst dataset in the world 48 4.2 how to identify pathologies 48 4.3 problems with data content 49 4.3.1 duplicate entries 49 4.3.2 multiple entries for a single entity 49 4.3.3 missing entries 49 4.3.4 nulls 50 4.3.5 huge outliers 50 4.3.6 out\u2010of\u2010date data 50 4.3.7 artificial entries 50"
    },
    {
        "text": "contents ix 4.3.8 irregular spacings 51 4.4 formatting issues 51 4.4.1 formatting is irregular between different tables/columns 51 4.4.2 extra whitespace 51 4.4.3 irregular capitalization 52 4.4.4 inconsistent delimiters 52 4.4.5 irregular null format 52 4.4.6 invalid characters 52 4.4.7 weird or incompatible datetimes 52 4.4.8 operating system incompatibilities 53 4.4.9 wrong software versions 53 4.5 example formatting script 54 4.6 regular expressions 55 4.6.1 regular expression syntax 56 4.7 life in the trenches 60 4.8 glossary 60 5 visualizations and simple metrics 61 5.1 a note on python\u2019s visualization tools 62 5.2 example code 62 5.3 pie charts 63 5.4 bar charts 65 5.5 histograms 66 5.6 means, standard deviations, medians, and quantiles 69 5.7 boxplots 70 5.8 scatterplots 72 5.9 scatterplots with logarithmic axes 74 5.10 scatter matrices 76 5.11 heatmaps 77 5.12 correlations 78 5.13 anscombe\u2019s quartet and the limits of numbers 80 5.14 time series 81 5.15 further reading 85 5.16 glossary 85 6 machine learning overview 87 6.1 historical context 88 6.2 supervised versus unsupervised 89 6.3 training data, testing data, and the great boogeyman of overfitting 89 6.4 further reading 91 6.5 glossary 91"
    },
    {
        "text": "contents x 7 interlude: feature extraction ideas 93 7.1 standard features 93 7.2 features that involve grouping 94 7.3 preview of more sophisticated features 95 7.4 defining the feature you want to predict 95 8 machine learning classification 97 8.1 what is a classifier, and what can you do with it?"
    },
    {
        "text": "97 8.2 a few practical concerns 98 8.3 binary versus multiclass 99 8.4 example script 99 8.5 specific classifiers 101 8.5.1 decision trees 101 8.5.2 random forests 103 8.5.3 ensemble classifiers 104 8.5.4 support vector machines 105 8.5.5 logistic regression 108 8.5.6 lasso regression 110 8.5.7 naive bayes 110 8.5.8 neural nets 112 8.6 evaluating classifiers 114 8.6.1 confusion matrices 114 8.6.2 roc curves 115 8.6.3 area under the roc curve 116 8.7 selecting classification cutoffs 117 8.7.1 other performance metrics 118 8.7.2 lift\u2013reach curves 118 8.8 further reading 119 8.9 glossary 119 9 technical communication and documentation 121 9.1 several guiding principles 122 9.1.1 know your audience 122 9.1.2 show why it matters 122 9.1.3 make it concrete 123 9.1.4 a picture is worth a thousand words 123 9.1.5 don\u2019t be arrogant about your tech knowledge 124 9.1.6 make it look decent 124 9.2 slide decks 124 9.2.1 c.r.a.p."
    },
    {
        "text": "design 125 9.2.2 a few tips and rules of thumb 127 9.3 written reports 128 9.4 speaking: what has worked for me 130"
    },
    {
        "text": "contents xi 9.5 code documentation 131 9.6 further reading 132 9.7 glossary 132 part ii stuff you still need to know 133 10 unsupervised learning: clustering and dimensionality reduction 135 10.1 the curse of dimensionality 136 10.2 example: eigenfaces for dimensionality reduction 138 10.3 principal component analysis and factor analysis 140 10.4 skree plots and understanding dimensionality 142 10.5 factor analysis 143 10.6 limitations of pca 143 10.7 clustering 144 10.7.1 real\u2010world assessment of clusters 144 10.7.2 k\u2010means clustering 145 10.7.3 gaussian mixture models 146 10.7.4 agglomerative clustering 147 10.7.5 evaluating cluster quality 148 10.7.6 siihouette score 148 10.7.7 rand index and adjusted rand index 149 10.7.8 mutual information 150 10.8 further reading 151 10.9 glossary 151 11 regression 153 11.1 example: predicting diabetes progression 153 11.2 least squares 156 11.3 fitting nonlinear curves 157 11.4 goodness of fit: r2 and correlation 159 11.5 correlation of residuals 160 11.6 linear regression 161 11.7 lasso regression and feature selection 162 11.8 further reading 164 11.9 glossary 164 12 data encodings and file formats 165 12.1 typical file format categories 165 12.1.1 text files 166 12.1.2 dense numerical arrays 166 12.1.3 program\u2010specific data formats 166"
    },
    {
        "text": "contents xii 12.1.4 compressed or archived data 166 12.2 csv files 167 12.3 json files 168 12.4 xml files 170 12.5 html files 172 12.6 tar files 174 12.7 gzip files 175 12.8 zip files 175 12.9 image files: rasterized, vectorized, and/or compressed 176 12.10 it\u2019s all bytes at the end of the day 177 12.11 integers 178 12.12 floats 179 12.13 text data 180 12.14 further reading 183 12.15 glossary 183 13 big data 185 13.1 what is big data?"
    },
    {
        "text": "185 13.2 hadoop: the file system and the processor 187 13.3 using hdfs 188 13.4 example pyspark script 189 13.5 spark overview 190 13.6 spark operations 192 13.7 two ways to run pyspark 193 13.8 configuring spark 194 13.9 under the hood 195 13.10 spark tips and gotchas 196 13.11 the mapreduce paradigm 197 13.12 performance considerations 199 13.13 further reading 200 13.14 glossary 200 14 databases 203 14.1 relational databases and mysql\u00ae 204 14.1.1 basic queries and grouping 204 14.1.2 joins 207 14.1.3 nesting queries 208 14.1.4 running mysql and managing the db 209 14.2 key-value stores 210 14.3 wide column stores 211 14.4 document stores 211 14.4.1 mongodb\u00ae 212 14.5 further reading 214 14.6 glossary 214"
    },
    {
        "text": "contents xiii 15 software engineering best practices 217 15.1 coding style 217 15.2 version control and git for data scientists 220 15.3 testing code 222 15.3.1 unit tests 223 15.3.2 integration tests 224 15.4 test-driven development 225 15.5 agile methodology 225 15.6 further reading 226 15.7 glossary 226 16 natural language processing 229 16.1 do i even need nlp?"
    },
    {
        "text": "229 16.2 the great divide: language versus statistics 230 16.3 example: sentiment analysis on stock market articles 230 16.4 software and datasets 232 16.5 tokenization 233 16.6 central concept: bag\u2010of\u2010words 233 16.7 word weighting: tf\u2010idf 235 16.8 n\u2010grams 235 16.9 stop words 236 16.10 lemmatization and stemming 236 16.11 synonyms 237 16.12 part of speech tagging 237 16.13 common problems 238 16.13.1 search 238 16.13.2 sentiment analysis 239 16.13.3 entity recognition and topic modeling 240 16.14 advanced nlp: syntax trees, knowledge, and understanding 240 16.15 further reading 241 16.16 glossary 242 17 time series analysis 243 17.1 example: predicting wikipedia views 244 17.2 a typical workflow 247 17.3 time series versus time-stamped events 248 17.4 resampling an interpolation 249 17.5 smoothing signals 251 17.6 logarithms and other transformations 252 17.7 trends and periodicity 252 17.8 windowing 253 17.9 brainstorming simple features 254 17.10 better features: time series as vectors 255"
    },
    {
        "text": "contents xiv 17.11 fourier analysis: sometimes a magic bullet 256 17.12 time series in context: the whole suite of features 259 17.13 further reading 259 17.14 glossary 260 18 probability 261 18.1 flipping coins: bernoulli random variables 261 18.2 throwing darts: uniform random variables 263 18.3 the uniform distribution and pseudorandom numbers 263 18.4 nondiscrete, noncontinuous random variables 265 18.5 notation, expectations, and standard deviation 267 18.6 dependence, marginal and conditional probability 268 18.7 understanding the tails 269 18.8 binomial distribution 271 18.9 poisson distribution 272 18.10 normal distribution 272 18.11 multivariate gaussian 273 18.12 exponential distribution 274 18.13 log-normal distribution 276 18.14 entropy 277 18.15 further reading 279 18.16 glossary 279 19 statistics 281 19.1 statistics in perspective 281 19.2 bayesian versus frequentist: practical tradeoffs and differing philosophies 282 19.3 hypothesis testing: key idea and example 283 19.4 multiple hypothesis testing 285 19.5 parameter estimation 286 19.6 hypothesis testing: t-test 287 19.7 confidence intervals 290 19.8 bayesian statistics 291 19.9 naive bayesian statistics 293 19.10 bayesian networks 293 19.11 choosing priors: maximum entropy or domain knowledge 294 19.12 further reading 295 19.13 glossary 295 20 programming language concepts 297 20.1 programming paradigms 297 20.1.1 imperative 298 20.1.2 functional 298"
    },
    {
        "text": "contents xv 20.1.3 object\u2010oriented 301 20.2 compilation and interpretation 305 20.3 type systems 307 20.3.1 static versus dynamic typing 308 20.3.2 strong versus weak typing 308 20.4 further reading 309 20.5 glossary 309 21 performance and computer memory 311 21.1 example script 311 21.2 algorithm performance and big\u2010o notation 314 21.3 some classic problems: sorting a list and binary search 315 21.4 amortized performance and average performance 318 21.5 two principles: reducing overhead and managing memory 320 21.6 performance tip: use numerical libraries when applicable 322 21.7 performance tip: delete large structures you don\u2019t need 323 21.8 performance tip: use built\u2010in functions when possible 324 21.9 performance tip: avoid superfluous function calls 324 21.10 performance tip: avoid creating large new objects 325 21.11 further reading 325 21.12 glossary 325 part iii specialized or advanced topics 327 22 computer memory and data structures 329 22.1 virtual memory, the stack, and the heap 329 22.2 example c program 330 22.3 data types and arrays in memory 330 22.4 structs 332 22.5 pointers, the stack, and the heap 333 22.6 key data structures 337 22.6.1 strings 337 22.6.2 adjustable\u2010size arrays 338 22.6.3 hash tables 339 22.6.4 linked lists 340 22.6.5 binary search trees 342 22.7 further reading 343 22.8 glossary 343 23 maximum likelihood estimation and optimization 345 23.1 maximum likelihood estimation 345 23.2 a simple example: fitting a line 346"
    },
    {
        "text": "contents xvi 23.3 another example: logistic regression 348 23.4 optimization 348 23.5 gradient descent and convex optimization 350 23.6 convex optimization 353 23.7 stochastic gradient descent 355 23.8 further reading 355 23.9 glossary 356 24 advanced classifiers 357 24.1 a note on libraries 358 24.2 basic deep learning 358 24.3 convolutional neural networks 361 24.4 different types of layers."
    },
    {
        "text": "what the heck is a tensor?"
    },
    {
        "text": "362 24.5 example: the mnist handwriting dataset 363 24.6 recurrent neural networks 366 24.7 bayesian networks 367 24.8 training and prediction 369 24.9 markov chain monte carlo 369 24.10 pymc example 370 24.11 further reading 373 24.12 glossary 373 25 stochastic modeling 375 25.1 markov chains 375 25.2 two kinds of markov chain, two kinds of questions 377 25.3 markov chain monte carlo 379 25.4 hidden markov models and the viterbi algorithm 380 25.5 the viterbi algorithm 382 25.6 random walks 384 25.7 brownian motion 384 25.8 arima models 385 25.9 continuous\u2010time markov processes 386 25.10 poisson processes 387 25.11 further reading 388 25.12 glossary 388 25a parting words: your future as a data scientist 391 index 393"
    },
    {
        "text": "xvii this book was written to solve a problem."
    },
    {
        "text": "the people who i interview for data science jobs have sterling mathematical pedigrees, but most of them are unable to write a simple script that computes fibonacci numbers (in case you aren\u2019t familiar with fibonacci numbers, this takes about five lines of code)."
    },
    {
        "text": "on the other side, employers tend to view data scientists as either mysterious wizards or used\u2010car salesmen (and when data scientists can\u2019t be trusted to write a basic script, the latter impression has some merit!)."
    },
    {
        "text": "these problems reflect a funda\u00ad mental misunderstanding, by all parties, of what data science is (and isn\u2019t) and what skills its practitioners need."
    },
    {
        "text": "when i first got into data science, i was part of that problem."
    },
    {
        "text": "years of doing academic physics had trained me to solve problems in a way that was long on abstract theory but short on common sense or flexibility."
    },
    {
        "text": "mercifully, i also knew how to code (thanks, googletm internships!"
    },
    {
        "text": "), and this let me limp along while i picked up the skills and mindsets that actually mattered."
    },
    {
        "text": "since leaving academia, i have done data science consulting for companies of every stripe."
    },
    {
        "text": "this includes web traffic analysis for tiny start\u2010ups, manufac\u00ad turing optimizations for fortune 100 giants, and everything in between."
    },
    {
        "text": "the problems to solve are always unique, but the skills required to solve them are strikingly universal."
    },
    {
        "text": "they are an eclectic mix of computer programming, mathematics, and business savvy."
    },
    {
        "text": "they are rarely found together in one per\u00ad son, but in truth they can be learned by anybody."
    },
    {
        "text": "a few interviews i have given stand out in my mind."
    },
    {
        "text": "the candidate was smart and knowledgeable, but the interview made it painfully clear that they were unprepared for the daily work of a data scientist."
    },
    {
        "text": "what do you do as an inter\u00ad viewer when the candidate starts apologizing for wasting your time?"
    },
    {
        "text": "we ended up filling the hour with a crash course on what they were missing and how they could go out and fill the gaps in their knowledge."
    },
    {
        "text": "they went out, learned what they needed to, and are now successful data scientists."
    },
    {
        "text": "i wrote this book in an attempt to help people like that out, by condensing data science\u2019s various skill sets into a single, coherent volume."
    },
    {
        "text": "it is hands\u2010on preface"
    },
    {
        "text": "\ufeff preface xviii and to the point: ideal for somebody who needs to come up to speed quickly or solve a problem on a tight deadline."
    },
    {
        "text": "the educational system has not yet caught up to the demands of this new and exciting field, and my hope is that this book will help you bridge the gap."
    },
    {
        "text": "field cady september 2016 redmond, washington"
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 1 1 \u201cdata science\u201d is a very popular term these days, and it gets applied to so many things that its meaning has become very vague."
    },
    {
        "text": "so i\u2019d like to start this book by giving you the definition that i use."
    },
    {
        "text": "i\u2019ve found that this one gets right to the heart of what sets it apart from other disciplines."
    },
    {
        "text": "here goes: data science means doing analytics work that, for one reason or another, requires a substantial amount of software engineering skills."
    },
    {
        "text": "sometimes, the final deliverable is the kind of thing a statistician or business analyst might provide, but achieving that goal demands software skills that your typical analyst simply doesn\u2019t have."
    },
    {
        "text": "for example, a dataset might be so large that you need to use distributed computing to analyze it or so convoluted in its format that many lines of code are required to parse it."
    },
    {
        "text": "in many cases, data scientists also have to write big chunks of production software that imple- ment their analytics ideas in real time."
    },
    {
        "text": "in practice, there are usually other dif- ferences as well."
    },
    {
        "text": "for example, data scientists usually have to extract features from raw data, which means that they tackle very open\u2010ended problems such as how to quantify the \u201cspamminess\u201d of an e\u2010mail."
    },
    {
        "text": "it\u2019s very hard to find people who can construct good statistical models, hack quality software, and relate this all in a meaningful way to business problems."
    },
    {
        "text": "it\u2019s a lot of hats to wear!"
    },
    {
        "text": "these individuals are so rare that recruiters often call them \u201cunicorns.\u201d the message of this book is that it is not only possible but also relatively straightforward to become a \u201cunicorn.\u201d it\u2019s just a question of acquiring the par- ticular balance of skills required."
    },
    {
        "text": "very few educational programs teach all of those skills, which is why unicorns are rare, but that\u2019s mostly a historical acci- dent."
    },
    {
        "text": "it is perfectly reasonable for a single person to have the whole palette of abilities, provided they\u2019re willing to ignore the traditional boundaries between different disciplines."
    },
    {
        "text": "this book aims to teach you everything you\u2019ll need to know to be a compe- tent data scientist."
    },
    {
        "text": "my guess is that you\u2019re either a computer programmer introduction: becoming a unicorn"
    },
    {
        "text": "1 introduction: becoming a unicorn 2 looking to learn about analytics or more of a mathematician trying to bone up on their coding."
    },
    {
        "text": "you might also be a businessperson who needs the technical skills to answer your business questions or simply an interested layman."
    },
    {
        "text": "whoever you are though, this book will teach you the concepts you need."
    },
    {
        "text": "this book is not comprehensive."
    },
    {
        "text": "data science is too big an area for any per- son or book to cover all of it."
    },
    {
        "text": "besides, the field is changing so fast that any \u201ccomprehensive\u201d book would be out\u2010of\u2010date before it came off the presses."
    },
    {
        "text": "instead, i have aimed for two goals."
    },
    {
        "text": "first, i want to give a solid grounding in the big picture of what data science is, how to go about doing it, and the founda- tional concepts that will stand the test of time."
    },
    {
        "text": "second, i want to give a \u201ccom- plete\u201d skill set, in the sense that you have the nuts\u2010and\u2010bolts knowledge to go out and do data science work (you can code in python, you know the libraries to use, most of the big machine learning models, etc."
    },
    {
        "text": "), even if particular pro- jects or companies might require that you pick up a new skill set from some- where else."
    },
    {
        "text": "1.1\u00ad aren\u2019t data scientists just overpaid statisticians?"
    },
    {
        "text": "nate silver, a statistician famous for accurate forecasting of us elections, once famously said: \u201ci think data scientist is a sexed\u2010up term for statistician.\u201d he has a point, but what he said is only partly true."
    },
    {
        "text": "the discipline of statistics deals mostly with rigorous mathematical methods for solving well\u2010defined prob- lems."
    },
    {
        "text": "data scientists spend most of their time getting data into a form where statistical methods could even be applied."
    },
    {
        "text": "this involves making sure that the analytics problem is a good match to business objectives, extracting meaning- ful features from the raw data and coping with any pathologies of the data or weird edge cases."
    },
    {
        "text": "once that heavy lifting is done, you can apply statistical tools to get the final results, although, in practice, you often don\u2019t even need them."
    },
    {
        "text": "professional statisticians need to do a certain amount of preprocessing them- selves, but there is a massive difference in degree."
    },
    {
        "text": "historically, data science emerged as a field independently from statistics."
    },
    {
        "text": "most of the first data scientists were computer programmers or machine learn- ing experts who were working on big data problems."
    },
    {
        "text": "they were analyzing datasets of the kind that statisticians don\u2019t touch: html pages, image files, e\u2010mails, raw output logs of web servers, and so on."
    },
    {
        "text": "these datasets don\u2019t fit the mold of relational databases or statistical tools, so for decades, they were just piling up without being analyzed."
    },
    {
        "text": "data science came into being as a way to finally milk them for insights."
    },
    {
        "text": "in 20 years, i suspect that statistics, data science, and machine learning will blur into a single discipline."
    },
    {
        "text": "the differences between them are, after all, really"
    },
    {
        "text": "3 1.3\u00ad how to use this book?"
    },
    {
        "text": "just a matter of degree and/or historical accident."
    },
    {
        "text": "but in practical terms, for the time being, solving data science problems requires skills that a normal statisti- cian does not have."
    },
    {
        "text": "in fact, these skills, which include extensive software engi- neering and domain\u2010specific feature extraction, constitute the overwhelming majority of the work that needs to be done."
    },
    {
        "text": "in the daily work of a data scientist, statistics plays second fiddle."
    },
    {
        "text": "1.2\u00ad how is this book organized?"
    },
    {
        "text": "this book is organized into three sections."
    },
    {
        "text": "the first, the stuff you\u2019ll always use, covers topics that, in my experience, you will end up using in almost any data science project."
    },
    {
        "text": "they are core skills, which are absolutely indispensable for data science at any level."
    },
    {
        "text": "the first section was also written with an eye toward people who need data science to answer a specific question but do not aspire to become full\u2010fledged data scientists."
    },
    {
        "text": "if you are in this camp, then there is a good chance that part i of the book will give you everything you need."
    },
    {
        "text": "the second section, stuff you still need to know, covers additional core skills for a data scientist."
    },
    {
        "text": "some of these, such as clustering, are so common that they almost made it into the first section, and they could easily play a role in any project."
    },
    {
        "text": "others, such as natural language processing, are somewhat spe- cialized subjects that are critical in certain domains but superfluous in others."
    },
    {
        "text": "in my judgment, a data scientist should be conversant in all of these subjects, even if they don\u2019t always use them all."
    },
    {
        "text": "the final section, stuff that\u2019s good to know, covers a variety of topics that are optional."
    },
    {
        "text": "some of these chapters are just expansions on topics from the first two sections, but they give more theoretical background and discuss some additional topics."
    },
    {
        "text": "others are entirely new material, which does come up in data science, but which you could go through a career without ever running into."
    },
    {
        "text": "1.3\u00ad how to use this book?"
    },
    {
        "text": "this book was written with three use cases in mind: 1) you can read it cover\u2010to\u2010cover."
    },
    {
        "text": "if you do that, it should give you a self\u2010con- tained course in data science that will leave you ready to tackle real prob- lems."
    },
    {
        "text": "if you have a strong background in computer programming, or in mathematics, then some of it will be review."
    },
    {
        "text": "2) you can use it to come quickly up to speed on a specific subject."
    },
    {
        "text": "i have tried to make the different chapters pretty self\u2010contained, especially the chapters after the first section."
    },
    {
        "text": "1 introduction: becoming a unicorn 4 3) the book contains a lot of sample codes, in pieces that are large enough to use as a starting point for your own projects."
    },
    {
        "text": "1.4\u00ad why is it all in pythontm, anyway?"
    },
    {
        "text": "the example code in this book is all in python, except for a few domain\u2010\u00adspecific languages such as sql."
    },
    {
        "text": "my goal isn\u2019t to push you to use python; there are lots of good tools out there, and you can use whichever ones you want."
    },
    {
        "text": "however, i wanted to use one language for all of my examples."
    },
    {
        "text": "this keeps the book readable, and it also lets readers follow the whole book while only know- ing one language."
    },
    {
        "text": "of the various languages available, there are two reasons why i chose python: 1) python is the most popular language for data scientists."
    },
    {
        "text": "r is its only major competitor, at least when it comes to free tools."
    },
    {
        "text": "i have used both exten- sively, and i think that python is flat\u2010out better (except for some obscure statistics packages that have been written in r and that are rarely needed anyway)."
    },
    {
        "text": "2) i like to say that for any task, python is the second\u2010best language."
    },
    {
        "text": "it\u2019s a jack\u2010 of\u2010all\u2010trades."
    },
    {
        "text": "if you only need to worry about statistics, or numerical com- putation, or web parsing, then there are better options out there."
    },
    {
        "text": "but if you need to do all of these things within a single project, then python is your best option."
    },
    {
        "text": "since data science is so inherently multidisciplinary, this makes it a perfect fit."
    },
    {
        "text": "as a note of advice, it is much better to be proficient in one language, to the point where you can reliably churn out code that is of high quality, than to be mediocre at several."
    },
    {
        "text": "1.5\u00ad example code and datasets this book is rich in example code, in fairly long chunks."
    },
    {
        "text": "this was done for two reasons: 1) as a data scientist, you need to be able to read longish pieces of code."
    },
    {
        "text": "this is a nonoptional skill, and if you aren\u2019t used to it, then this will give you a chance to practice."
    },
    {
        "text": "2) i wanted to make it easier for you to poach the code from this book, if you feel so inclined."
    },
    {
        "text": "you can do whatever you want with the code, with or without attribution."
    },
    {
        "text": "i release it into the public domain in the hope that it can give some people a small leg up."
    },
    {
        "text": "you can find it on my github at www.github.com/field-cady."
    },
    {
        "text": "1.6 parting words 5 the sample data that i used comes in two forms: 1) test datasets that are built into python\u2019s scientific libraries 2) data that is pulled off the internet, from sources such as yahoo and wikipedia."
    },
    {
        "text": "when i do this, the example scripts will include code that pulls the data."
    },
    {
        "text": "1.6 \u00adparting words it is my hope that this book not only teaches you how to do nut\u2010and\u2010bolts data science but also gives you a feel of how exciting this deeply interdisciplinary subject is."
    },
    {
        "text": "please feel free to reach out to me at www.fieldcady.com or field."
    },
    {
        "text": "cady@gmail.com with comments, errata, or any other feedback."
    },
    {
        "text": "7 the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. part 1 the stuff you\u2019ll always use the first section of this book covers core topics that everybody doing data science should know."
    },
    {
        "text": "this includes people who are not interested in being \u00adprofessional data scientists, but need to know just enough to solve some spe- cific problem."
    },
    {
        "text": "these are the subjects that will likely arise in every data science \u00adproject you do."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 9 2 in this chapter, i will give you a high\u2010level overview of the process of data science."
    },
    {
        "text": "i will focus on the different stages of data science work, including com- mon pain points, key things to get right, and where data science parts ways from other disciplines."
    },
    {
        "text": "the process of solving a data science problem is summarized in the following figure, which i called the data science road map."
    },
    {
        "text": "the data science road map the data science road map frame the problem understand the data extract features model and analyse deploy code present results the first step is always to frame the problem: understand the business use case and craft a well\u2010defined analytics problem (or problems) out of it."
    },
    {
        "text": "this is followed by an extensive stage of grappling with the data and the real\u2010world things that it describes, so that we can extract meaningful features."
    },
    {
        "text": "finally, these features are plugged into analytical tools that give us hard numerical results."
    },
    {
        "text": "before i go into more detail about the different stages of the roadmap, i want to point out two things."
    },
    {
        "text": "the first is that \u201cmodel and analyze\u201d loops back to framing the problem."
    },
    {
        "text": "this is one of the key features of data science that differentiate it from tradi- tional software engineering."
    },
    {
        "text": "data scientists write code, and they use many of the same tools as software engineers."
    },
    {
        "text": "however, there is a tight feedback loop between data science work and the real world."
    },
    {
        "text": "questions are always being reframed as new insights become available, and, as a result, data scientists"
    },
    {
        "text": "2 the data science road map 10 must keep their code base extremely flexible and always have an eye toward the real\u2010world problem they are solving."
    },
    {
        "text": "ideally, you will follow the loop back many times, constantly refining your methods and producing new insights."
    },
    {
        "text": "the second point is that there are two different (although not mutually exclu- sive) ways to exit the road map: presenting results and deploying code."
    },
    {
        "text": "my friend michael li, a data scientist who founded the data incubator, likened this to having two different types of clients: humans and machines."
    },
    {
        "text": "they require distinct skill sets and modifications to every stage of the data science road map."
    },
    {
        "text": "if your clients are humans, then usually you are trying to use available data sources to answer some kind of business problem."
    },
    {
        "text": "examples would be the following: \u25cf \u25cfidentifying leading indicators of spikes in the price of a stock, so that people can understand what causes price spikes \u25cf \u25cfdetermining whether customers break down into natural subtypes and what characteristics each type has \u25cf \u25cfassessing whether traffic to one website can be used to predict traffic to another site."
    },
    {
        "text": "typically, the final deliverable for work such as this will be a powerpoint slide deck or a written report."
    },
    {
        "text": "the goal is to give business insights, and often these insights will be used for making key decisions."
    },
    {
        "text": "this kind of data science also functions as a way to test the waters and see whether some analytics approach is worth a larger follow\u2010up project that may result in production software."
    },
    {
        "text": "if your clients are machines, then you are doing something that blends into software engineering, where the deliverable is a piece of software that performs some analytics work."
    },
    {
        "text": "examples would be the following: \u25cf \u25cfimplementing the algorithm that chooses which ad to show to a customer and training it on real data \u25cf \u25cfwriting a batch process that generates daily reports based on company records generated that day, using some kind of analytics to point out salient patterns in these cases, your main deliverable is a piece of software."
    },
    {
        "text": "in addition to performing a useful task, it had better work well in terms of performance, robustness to bad inputs, and so on."
    },
    {
        "text": "once you understand who your clients are, the next step is to determine what you\u2019ll be doing for them."
    },
    {
        "text": "in the next section, i will show you how to do this all\u2010important step."
    },
    {
        "text": "2.1\u00ad frame the problem the difference between great and mediocre data science is not about math or engineering: it is about asking the right question(s)."
    },
    {
        "text": "alternately, if you\u2019re trying"
    },
    {
        "text": "11 2.2 understand the data: basic questions to build some piece of software, you need to decide what exactly that software should do."
    },
    {
        "text": "no amount of technical competence or statistical rigor can make up for having solved a useless problem."
    },
    {
        "text": "if your clients are humans, most projects start with some kind of extremely open\u2010ended question."
    },
    {
        "text": "perhaps there is a known pain point, but it\u2019s not clear what a solution would look like."
    },
    {
        "text": "if your clients are machines, then the business problem is usually pretty clear, but there can be a lot of ambiguity about what constraints there might be on the software (languages to use, runtime, how accurate predictions need to be, etc.)."
    },
    {
        "text": "before diving into actual work, it\u2019s important to clarify exactly what would constitute a solution to this problem."
    },
    {
        "text": "a \u201cdefinition of done\u201d is a good way to put it: what criteria constitute a com- pleted project, and (most importantly) what would be required to make the project a success?"
    },
    {
        "text": "for large projects, these criteria are typically laid out in a document."
    },
    {
        "text": "writing that document is a collaborative process involving a lot of back\u2010and\u2010forth with stakeholders, negotiation, and sometimes heated disagreement."
    },
    {
        "text": "in consulting, these documents are often called \u201cstatements of work\u201d or sows."
    },
    {
        "text": "within a company that is creating a product (as opposed to just a stand\u2010alone investiga- tion), they are often referred to as \u201cproject requirements documents\u201d or prds."
    },
    {
        "text": "the main purpose of an sow is to get everybody on the same about what exactly work should be done, what the priorities are, and what expecta- tions are realistic."
    },
    {
        "text": "business problems are typically very vague to start off with, and it takes a lot of time and effort to follow a course of action through to the final result."
    },
    {
        "text": "so before investing that effort, it is critical to make sure that you are working on the right problem."
    },
    {
        "text": "there is, however, also an element of self\u2010defense."
    },
    {
        "text": "sometimes, it ends up being impossible to solve a problem with the available data."
    },
    {
        "text": "or, the data is cor- rupted."
    },
    {
        "text": "maybe clients decide that the project isn\u2019t important anymore."
    },
    {
        "text": "a good sow makes it impossible for people to attack or sue you for wasting time and money on work that they now claim that they didn\u2019t want done."
    },
    {
        "text": "having an sow doesn\u2019t set things in stone."
    },
    {
        "text": "there are course corrections based on preliminary discoveries."
    },
    {
        "text": "sometimes, people change their minds after the sow has been signed."
    },
    {
        "text": "it happens."
    },
    {
        "text": "but crafting an sow is the best way to make sure that all efforts are pointed in the most useful direction."
    },
    {
        "text": "2.2\u00ad understand the data: basic questions once you have access to the data you\u2019ll be using, it\u2019s good to have a battery of standard questions that you always ask about it."
    },
    {
        "text": "this is a good way to hit the ground running with your analyses, rather than risk analysis paralysis."
    },
    {
        "text": "it is also a good safeguard to identify problems with the data as quickly as possible."
    },
    {
        "text": "2 the data science road map 12 a few good generic questions to ask are as follows: \u25cf \u25cfhow big is the dataset?"
    },
    {
        "text": "\u25cf \u25cfis this the entire dataset?"
    },
    {
        "text": "\u25cf \u25cfis this data representative enough?"
    },
    {
        "text": "for example, maybe data was only col- lected for a subset of users."
    },
    {
        "text": "\u25cf \u25cfare there likely to be gross outliers or extraordinary sources of noise?"
    },
    {
        "text": "for example, 99% of the traffic from a web server might be a single denial\u2010of\u2010ser- vice attack."
    },
    {
        "text": "\u25cf \u25cfmight there be artificial data inserted into the dataset?"
    },
    {
        "text": "this happens a lot in industrial settings."
    },
    {
        "text": "\u25cf \u25cfare there any fields that are unique identifiers?"
    },
    {
        "text": "these are the fields you might use for joining between datasets, etc."
    },
    {
        "text": "\u25cf \u25cfare the supposedly unique identifiers actually unique?"
    },
    {
        "text": "what does it mean if they aren\u2019t?"
    },
    {
        "text": "\u25cf \u25cfif there are two datasets a and b that need to be joined, what does it mean if something in a doesn\u2019t matching anything in b?"
    },
    {
        "text": "\u25cf \u25cfwhen data entries are blank, where does that come from?"
    },
    {
        "text": "\u25cf \u25cfhow common are blank entries?"
    },
    {
        "text": "sows will often include an appendix that describes the available data."
    },
    {
        "text": "if any of them can\u2019t be answered up front, it is common to clear them up as a first round of analysis and make sure that everybody agrees that these answers are sane."
    },
    {
        "text": "the most important question to ask about the data is whether it can solve the business problem that you are trying to tackle."
    },
    {
        "text": "if not, then you might need to look into additional sources of data or modify the work that you are planning."
    },
    {
        "text": "speaking from personal experience, i have been inclined to neglect these preliminary questions."
    },
    {
        "text": "i am excited to get into the actual analysis, so i\u2019ve some- times jumped right in without taking the time to make sure that i know what i\u2019m doing."
    },
    {
        "text": "for example, i once had a project where there was a collection of motors and time series data monitoring their physical characteristics: one time series per motor."
    },
    {
        "text": "my job was to find leading indicators of failure, and i started doing this by comparing the last day\u2019s worth of time series for a given motor (i.e., the data taken right before it failed) against its previous data."
    },
    {
        "text": "well, i real- ized a couple weeks in that sometimes the time series stopped long before the motor actually failed, and in other cases, the time series data continued long after the motor was dead."
    },
    {
        "text": "the actual times the motors had died were listed in a separate table, and it would have been easy for me to double\u2010check early on that they corresponded to the ends of the time series."
    },
    {
        "text": "2.3 \u00adunderstand the data: data wrangling data wrangling is the process of getting the data from its raw format into something suitable for more conventional analytics."
    },
    {
        "text": "this typically means"
    },
    {
        "text": "13 2.4 understand the data: exploratory analysis creating a software pipeline that gets the data out of wherever it is stored, does any cleaning or filtering necessary, and puts it into a regular format."
    },
    {
        "text": "data wrangling is the main area where data scientists need skills that a tradi- tional statistician or analyst doesn\u2019t have."
    },
    {
        "text": "the data is often stored in a special\u2010 purpose database that requires specialized tools to access."
    },
    {
        "text": "there could be so much of it that big data techniques are required to process it."
    },
    {
        "text": "you might need to use performance tricks to make things run quickly."
    },
    {
        "text": "especially with messy data, the preprocessing pipelines are often so complex that it is very difficult to keep the code organized."
    },
    {
        "text": "speaking of messy data, i should tell you this upfront: industrial datasets are always more convoluted than you would think they reasonably should be."
    },
    {
        "text": "the question is not whether the problems exist but whether they impact your work."
    },
    {
        "text": "my recipe for figuring out how a particular dataset is broken includes the following: 1) if the raw data is text, look directly at the plain files in a text editor or some- thing similar."
    },
    {
        "text": "things such as irregular date formats, irregular capitaliza- tions, and lines that are clearly junk will jump out at you."
    },
    {
        "text": "2) if there is a tool that is supposed to be able to open or process the data, make sure that it can actually do it."
    },
    {
        "text": "for example, if you have a csv file, try opening it in something that reads data frames."
    },
    {
        "text": "did it read all the rows in?"
    },
    {
        "text": "if not, maybe some rows have the wrong number of entries."
    },
    {
        "text": "did the column that is supposed to be a datetime get read in as a datetime?"
    },
    {
        "text": "if not, then maybe the formatting is irregular."
    },
    {
        "text": "3) do some histograms and scatterplots."
    },
    {
        "text": "are these numbers realistic, given what you know about the real\u2010life situation?"
    },
    {
        "text": "are there any massive outliers?"
    },
    {
        "text": "4) take some simple questions that you already know the (maybe approxi- mate) answer to, answer them based on this data, and see if the results agree."
    },
    {
        "text": "for example, you might try to calculate the number of customers by counting how many unique customer ids there are."
    },
    {
        "text": "if these numbers don\u2019t agree, then you\u2019ve probably misunderstood something about the data."
    },
    {
        "text": "2.4\u00ad understand the data: exploratory analysis once you have the data digested into a usable format, the next step is explora- tory analysis."
    },
    {
        "text": "this basically means poking around in the data, visualizing it in lots of different ways, trying out different ways to transform it, and seeing what there is to see."
    },
    {
        "text": "this stage is very creative, and it\u2019s a great place to let your curi- osity run a little wild."
    },
    {
        "text": "feel free to calculate some correlations and similar things, but don\u2019t break out the fancy machine learning classifiers."
    },
    {
        "text": "keep things simple and intuitive."
    },
    {
        "text": "2 the data science road map 14 there are two things that you typically get out of exploratory analysis: 1) you develop an intuitive feel for the data, including what the salient patterns look like visually."
    },
    {
        "text": "this is especially important if you\u2019re going to be working with similar data a lot in the future."
    },
    {
        "text": "2) you get a list of concrete hypotheses about what\u2019s going on in the data."
    },
    {
        "text": "oftentimes, a hypothesis will be motivated by a compelling graphic that you generated: a snapshot of a time series that shows an unmistakable pattern, a scatterplot demonstrating that two variables are related to each other, or a histogram that is clearly bimodal."
    },
    {
        "text": "a common misconception is that data scientists don\u2019t need visualizations."
    },
    {
        "text": "this attitude is not only inaccurate: it is very dangerous."
    },
    {
        "text": "most machine learn- ing algorithms are not inherently visual, but it is very easy to misinterpret their outputs if you look only at the numbers; there is no substitute for the human eye when it comes to making intuitive sense of things."
    },
    {
        "text": "2.5 \u00adextract features this stage has a lot of overlap with exploratory analysis and data wrangling."
    },
    {
        "text": "a feature is really just a number or a category that is extracted from your data and describes some entity."
    },
    {
        "text": "for example, you might extract the average word length from a text document or the number of characters in the document."
    },
    {
        "text": "or, if you have temperature measurements, you might extract the average temperature for a particular location."
    },
    {
        "text": "in practical terms, feature extraction means taking your raw datasets and distilling them down into a table with rows and columns."
    },
    {
        "text": "this is called \u201ctabular data.\u201d each row corresponds to some real\u2010world entity, and each column gives a single piece of information (generally a number) that describes that entity."
    },
    {
        "text": "virtually all analytics techniques, from lowly scatterplots to fancy neural net- works, operate on tabular data."
    },
    {
        "text": "extracting good features is the most important thing for getting your analysis to work."
    },
    {
        "text": "it is much more important than good machine learning classifiers, fancy statistical techniques, or elegant code."
    },
    {
        "text": "especially if your data doesn\u2019t come with readily available features (as is the case with web pages, images, etc."
    },
    {
        "text": "), how you reduce it to numbers will make the difference between success and failure."
    },
    {
        "text": "feature extraction is also the most creative part of data science and the one most closely tied to domain expertise."
    },
    {
        "text": "typically, a really good feature will cor- respond to some real\u2010world phenomenon."
    },
    {
        "text": "data scientists should work closely with domain experts and understand what these phenomena mean and how to distill them into numbers."
    },
    {
        "text": "sometimes, there is also room for creativity as to what entities you are extracting features about."
    },
    {
        "text": "for example, let\u2019s say that you have a bunch of"
    },
    {
        "text": "15 2.7 present results transaction logs, each of which gives a person\u2019s name and e\u2010mail address."
    },
    {
        "text": "do you want to have one row per human or one row per e\u2010mail address?"
    },
    {
        "text": "for many real\u2010world situations, you want one row per human (in which case, the number of unique e\u2010mail addresses they have might be a good feature to extract!"
    },
    {
        "text": "), but that opens the very thorny question of how you can tell when two people are the same based on their names."
    },
    {
        "text": "most features that we extract will be used to predict something."
    },
    {
        "text": "however, you may also need to extract the thing that you are predicting, which is also called the target variable."
    },
    {
        "text": "for example, i was once tasked with predicting whether my client\u2019s customers would lose their brand loyalty."
    },
    {
        "text": "there was no \u201cloyalty\u201d field in the data: it was just a log of various customer interactions and transactions."
    },
    {
        "text": "i had to figure out a way to measure \u201cloyalty.\u201d 2.6\u00ad model once features have been extracted, most data science projects involve some kind of machine learning model."
    },
    {
        "text": "maybe this is a classifier that guesses whether a customer is still loyal, a regression model that predicts a stock\u2019s price on the next day, or a clustering algorithm that breaks customers into different segments."
    },
    {
        "text": "in many data science projects, the modeling stage is quite simple: you just take a standard suite of models, plug your data into each one of them, and see which one works best."
    },
    {
        "text": "in other cases, a lot of care is taken to carefully tune a model and eek out every last bit of performance."
    },
    {
        "text": "really this should happen at every stage of a data science project, but it becomes especially crucial when analyzing the results of the modeling stage."
    },
    {
        "text": "if you have identified different clusters, what do they correspond to?"
    },
    {
        "text": "does your classifier work well enough to be useful?"
    },
    {
        "text": "is there anything interesting about the cases in which it fails?"
    },
    {
        "text": "this stage is what allows for course corrections in a project and gives ideas for what to do differently if there is another iteration."
    },
    {
        "text": "if your client is a human, it is common to use a variety of models, tuned in different ways, to examine different aspects of your data."
    },
    {
        "text": "if your client is a machine though, you will probably need to zero in on a single, canonical model that will be used in production."
    },
    {
        "text": "2.7\u00ad present results if your client is a human, then you will probably have to give either a slide deck or a written report describing the work you did and what your results were."
    },
    {
        "text": "you are also likely to have to do this even if your main clients are machines."
    },
    {
        "text": "2 the data science road map 16 communication in slide decks and prose is a difficult, important skill set in itself."
    },
    {
        "text": "but it is especially tricky with data science, where the material you are communicating is highly technical and you are presenting to a broad audience."
    },
    {
        "text": "data scientists must communicate fluidly with business stakeholders, domain experts, software engineers, and business analysts."
    },
    {
        "text": "these groups tend to have different knowledge bases coming in, different things they will be paying atten- tion to, and different presentation styles to which they are accustomed."
    },
    {
        "text": "2.8 \u00addeploy code if your ultimate clients are computers, then it is your job to produce code that will be run regularly in the future by other people."
    },
    {
        "text": "typically, this falls into one of two categories: 1) batch analytics code."
    },
    {
        "text": "this will be used to redo an analysis similar to the one that has already been done, on data that will be collected in the future."
    },
    {
        "text": "sometimes, it will produce some human\u2010readable analytics reports."
    },
    {
        "text": "other times, it will train a statistical model that will be referenced by other code."
    },
    {
        "text": "2) real\u2010time code."
    },
    {
        "text": "this will typically be an analytical module in a larger soft- ware package, written in a high\u2010performance programming language and adhering to all the best practices of software engineering."
    },
    {
        "text": "there are three typical deliverables from this stage: 1) the code itself."
    },
    {
        "text": "2) some documentation of how to run the code."
    },
    {
        "text": "sometimes, this is a stand\u2010 alone work document, often called a \u201crun book.\u201d other times, the documen- tation is embedded in the code itself."
    },
    {
        "text": "3) usually, you need some way to test code that ensures that your code oper- ates correctly."
    },
    {
        "text": "especially for real\u2010time code, this will normally take the form of unit tests."
    },
    {
        "text": "for batch processes, it is sometimes a sample input dataset (designed to illustrate all the relevant edge cases) along with what the out- put should look like."
    },
    {
        "text": "in deploying code, data scientists often take on a dual role as full\u2010fledged software engineers."
    },
    {
        "text": "especially with very intricate algorithms, it often just isn\u2019t practical to have one person spec it out and another implement the same thing for production."
    },
    {
        "text": "2.9 \u00aditerating data science is a deeply iterative process, even more so than typical software engineering."
    },
    {
        "text": "this is because in software, you generally at least know what you\u2019re ultimately aiming to create, even if you take an iterative approach to"
    },
    {
        "text": "17 2.10 glossary writing it."
    },
    {
        "text": "but in data science, it is usually an open question of what features will end up being useful to extract and what model you will train."
    },
    {
        "text": "for this \u00adreason, the data science process should be built around the goal of being able to change things painlessly."
    },
    {
        "text": "my recommendations are as follows: \u25cf \u25cftry to get preliminary results as quickly as possible after you\u2019ve understood the data."
    },
    {
        "text": "a scatterplot or histogram that shows you that there is a clear \u00adpattern in the data."
    },
    {
        "text": "maybe a simple model based on crude preliminary fea- tures that nonetheless works."
    },
    {
        "text": "sometimes an analysis is doomed to failure, because there just isn\u2019t much signal in the data."
    },
    {
        "text": "if this is the case, you want to know sooner rather than later, so that you can change your focus."
    },
    {
        "text": "\u25cf \u25cfautomate your analysis in a single script so that it\u2019s easy to run the whole thing with one command."
    },
    {
        "text": "this is a point that i\u2019ve learned the hard way: it is really, really easy after several hours at the command line to lose track of exactly what processing you did to get your data into its current form."
    },
    {
        "text": "keep things reproducible from the beginning."
    },
    {
        "text": "\u25cf \u25cfkeep your code modular and broken out into clear stages."
    },
    {
        "text": "this makes it easy to modify, add in, and take out steps as you experiment."
    },
    {
        "text": "notice how much of this comes down to considerations of software, not ana- lytics."
    },
    {
        "text": "the code must be flexible enough to solve all manner of problems, pow- erful enough to do it efficiently, and comprehensible enough to edit quickly if objectives change."
    },
    {
        "text": "doing this requires that data scientists use flexible, power- ful programming languages, which i will discuss in the next chapter."
    },
    {
        "text": "2.10\u00ad glossary data wrangling the nitty\u2010gritty task of cleaning data and getting it into a standard format that is suitable for downstream analysis."
    },
    {
        "text": "exploratory analysis a stage of analysis that focuses on exploring the data to generate hypotheses about it."
    },
    {
        "text": "exploratory analysis relies heavily on visualizations."
    },
    {
        "text": "feature a small piece of data, usually a number or a label, that is extracted from your data and characterizes some entity in your dataset."
    },
    {
        "text": "product requirements document (prd) a document that specifies exactly what functionality a planned product should have."
    },
    {
        "text": "production code software that is run repeatedly and maintained."
    },
    {
        "text": "it especially refers to source code of software product that is distributed to other people."
    },
    {
        "text": "statement of work (sow) a document that specifies what work is to be done in a project, relevant timelines, and specific deliverables."
    },
    {
        "text": "target variable a feature that you are trying to predict in machine learning."
    },
    {
        "text": "sometimes, it is already in your data, and other times, you must construct it yourself."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 19 3 one of the most obvious things that separate data scientists from traditional business analysts and (to a lesser degree) statisticians is that they spend a lot of their time writing code in a more\u2010or\u2010less normal programming language, as software engineers do."
    },
    {
        "text": "sometimes, it\u2019s a statistically oriented language such as r, but even that is a far cry from something such as excel or a graphical pack- age such as tableau."
    },
    {
        "text": "this chapter will discuss why that is and give a brief survey of some of the more popular languages."
    },
    {
        "text": "it will then dive into the weeds of python, my per- sonal language of choice and the most popular option among data scientists."
    },
    {
        "text": "if you already know python and its technical libraries, then feel free to skim."
    },
    {
        "text": "if not though, then this chapter will give you the foundation in python to under- stand the example code in the rest of the book."
    },
    {
        "text": "3.1\u00ad why use a programming language?"
    },
    {
        "text": "what are the other options?"
    },
    {
        "text": "to date, i have never worked on a data science project that could be done com- pletely within a graphical package such as excel or tableau."
    },
    {
        "text": "there is always something \u2013 a weird formatting issue that requires coding up the edge cases, a dataset that\u2019s too large to fit into memory, an unconventional feature that i want to extract, or something else \u2013 that forces me to roll up my sleeves and write some code."
    },
    {
        "text": "this will be your experience too, almost certainly."
    },
    {
        "text": "to put it glibly, data sci- ence is turing complete."
    },
    {
        "text": "many data scientists (like me) find it\u2019s more expedient to just work completely in programming languages, supplemented by numeri- cal libraries."
    },
    {
        "text": "others though find that it\u2019s worthwhile to do their data wrangling and feature extraction in a programming language but then load the datasets into another tool for their exploratory analysis."
    },
    {
        "text": "programming languages"
    },
    {
        "text": "3 programming languages 20 here are some tools besides programming languages that you might want to incorporate into your workflow: \u25cf \u25cfexcel."
    },
    {
        "text": "microsoft products often get a bad rap in the data science world, and it is completely undeserved."
    },
    {
        "text": "for simple data analysis, excel is probably the best piece of software ever made."
    },
    {
        "text": "\u25cf \u25cftableau."
    },
    {
        "text": "this is a tool for visualizing the data in relational databases."
    },
    {
        "text": "it\u2019s pretty limited in its functionality in my experience, but when it works, the graphics are absolutely beautiful."
    },
    {
        "text": "\u25cf \u25cfweka."
    },
    {
        "text": "this is a tool for applying pre\u2010canned machine learning algorithms to datasets that are already well formatted and contain the relevant features."
    },
    {
        "text": "an advantage of weka is that it\u2019s really just a thin gui wrapper around some java libraries, so it\u2019s easy to use the same models in your exploratory analysis and later production code (assuming that you work in java)."
    },
    {
        "text": "there is something critical that all of these tools have in common: they assume that your data is already in tabular form!"
    },
    {
        "text": "tabular datasets, and the things you can do with them, are sufficiently standardized that people can eas- ily write reusable tools that streamline the most common operations."
    },
    {
        "text": "however, each dataset often requires its own idiosyncratic data wrangling."
    },
    {
        "text": "furthermore, each new problem will require creativity and flexibility in what features you extract from the raw data \u2013 especially if your raw data is very far from tabular form."
    },
    {
        "text": "this is the reason that every data scientist needs to be pro- ficient in at least one programming language."
    },
    {
        "text": "3.2\u00ad a survey of programming languages for data science there are many programming language options available for data scientists."
    },
    {
        "text": "this section will give you a run\u2010down of some of the most popular ones."
    },
    {
        "text": "3.2.1 python the example code in this book is generally in python, for a number of reasons."
    },
    {
        "text": "in my opinion it is the best programming language available for general\u2010pur- pose use, but that\u2019s largely a matter of personal taste."
    },
    {
        "text": "it is also a very popular choice among data scientists, who feel like it balances the flexibility of a con- ventional scripting language with the numerical muscles of a good mathemat- ics package (at least, when it\u2019s paired with its scientific computing libraries)."
    },
    {
        "text": "python was developed by guido van rossum and first released in 1991. the language itself is a high\u2010level scripting language, with functionality similar to perl and ruby and with an unusually clean and self\u2010consistent syntax."
    },
    {
        "text": "outside of the core language, python has several open\u2010source technical computing libraries that make it a powerful tool for analytics."
    },
    {
        "text": "21 3.2 a survey of programming languages for data science 3.2.2 r aside from python, r is probably the most popular programming language among data scientists."
    },
    {
        "text": "python is a scripting language designed for computer programmers, which has been augmented with libraries for technical comput- ing."
    },
    {
        "text": "in contrast, r was designed by and for statisticians, and it is natively inte- grated with graphics capabilities and extensive statistical functions."
    },
    {
        "text": "it is based on s, which was developed at bell labs in 1976. r was brilliant for its time and a huge step up from the fortran routines that it was competing with."
    },
    {
        "text": "in fact, many of python\u2019s technical computing libraries are just ripping off the good ideas in r. but almost 40 years later, r is showing its age."
    },
    {
        "text": "specifically, there are areas where the syntax is very clunky, the support for strings is terrible, and the type system is antiquated."
    },
    {
        "text": "in my mind, the main reason to use r is just that there are so many special libraries that have been written for it over the years, and python has not cov- ered all the little special use cases yet."
    },
    {
        "text": "i no longer use r for my own work, but it is a major force in the data science community and will continue to be for the foreseeable future."
    },
    {
        "text": "in the statistics community, r is still the lingua franca."
    },
    {
        "text": "you should know about it, even if you don\u2019t use it yourself."
    },
    {
        "text": "3.2.3 matlab\u00ae and octave the data science community skews strongly toward open\u2010source software, so good proprietary programs such as matlab\u00ae often get less credit than they deserve."
    },
    {
        "text": "developed and sold by the mathworks corporation, matlab\u00ae is an excellent package for numerical computing."
    },
    {
        "text": "it has a more consistent (and, in my opinion, nicer) syntax compared to r and more numerical muscle com- pared to python."
    },
    {
        "text": "a lot of people coming from physics or mechanical/electrical engineering backgrounds are well\u2010versed in matlab\u00ae."
    },
    {
        "text": "it is not as well\u2010suited to large software frameworks or string\u2010based data munging, but it is best\u2010in\u2010 class for numerical computing."
    },
    {
        "text": "if you like matlab\u2019s syntax, but don\u2019t like paying for software, then you could also consider octave."
    },
    {
        "text": "it is an open\u2010source version of matlab\u00ae."
    },
    {
        "text": "it doesn\u2019t capture all of matlab\u2019s functionality and certainly doesn\u2019t have the same support infrastructure, but it\u2019s a fine option."
    },
    {
        "text": "3.2.4 sas\u00ae sas (statistical analysis software) is a proprietary statistics framework that dates back to the 1960s."
    },
    {
        "text": "similar to r, there is a tremendous amount of entrenched legacy code written in sas and a wide range of functionality that has been put into it."
    },
    {
        "text": "however, the language itself is very alien to somebody more used to modern language."
    },
    {
        "text": "sas can be great for the business statistics applications that it is so popular in, but i don\u2019t recommend it for general\u2010\u00ad purpose data science."
    },
    {
        "text": "3 programming languages 22 3.2.5 scala\u00ae scala is an up\u2010and\u2010coming language that shows a lot of promise."
    },
    {
        "text": "it is not currently a suitable general\u2010purpose tool for data scientists because it doesn\u2019t have the library support for analytics and visualizations."
    },
    {
        "text": "however, that could easily change in the same way that it did with python."
    },
    {
        "text": "scala is similar to java under the hood, but has a much simpler syntax with a lot of powerful features borrowed from other languages (especially functional lan- guages)."
    },
    {
        "text": "it works both for general\u2010purpose scripting and for large\u2010scale pro- duction software."
    },
    {
        "text": "many of the most popular big data technologies are written in scala."
    },
    {
        "text": "3.3\u00ad python crash course this section will give a quick tutorial on the python language."
    },
    {
        "text": "my goal is to get you up\u2010and\u2010running quickly with the basics of the language, especially so that you can understand the example code in the book."
    },
    {
        "text": "the tutorial is far from exhaustive."
    },
    {
        "text": "there are many aspects of python that i don\u2019t discuss, and in particular, i ignore most of its many built\u2010in libraries."
    },
    {
        "text": "some of this material will be covered later in the book when it becomes relevant."
    },
    {
        "text": "the next section will give you an introduction to python\u2019s technical libraries, which elevate it from a solid scripting language to a one\u2010stop\u2010shop for data science."
    },
    {
        "text": "3.3.1 a note on versions there are a number of versions of the python language out there."
    },
    {
        "text": "as of this writing, the python 2.7 series was by far the most popular for data scientists."
    },
    {
        "text": "the main reason for this is that all of the numerical computing libraries work with it."
    },
    {
        "text": "in 2008, python 3.0 was released, and it broke backward compatibility with python 2.7. this was a big deal because the python community tends to be very careful about keeping things mutually consistent."
    },
    {
        "text": "this book is written assuming python 2.7, but most of what i will say applied equally well to 3.x."
    },
    {
        "text": "several of the key places where 3.x differs are as follows: \u25cf \u25cfprint is treated as a function."
    },
    {
        "text": "so you would say >>> print(\"hello world\") instead of >>> print \"hello world\""
    },
    {
        "text": "23 3.3 python crash course \u25cf \u25cfarithmetic operations are treated as decimal operations even when they are done on integers."
    },
    {
        "text": "that way 3/2 will equal 1.5 rather than 1. if you want to do integer division, then say // \u25cf \u25cfstring and unicode are removed as separate classes."
    },
    {
        "text": "it\u2019s all unicode now, and you can use the bytearray type if you want to manipulate bytes directly."
    },
    {
        "text": "3.3.2 \u201chello world\u201d script a common way to learn a new programming language is to first write a \u201chello world!\u201d program: this is a program that just prints the text \u201chello world!\u201d to the screen."
    },
    {
        "text": "if you can write it and run it, then you know you have your software environment set up correctly and you know how to use it."
    },
    {
        "text": "after that point, you\u2019re ready to roll with the serious code."
    },
    {
        "text": "there are two ways you can run python code, and i\u2019ll walk you through hello world in both of them."
    },
    {
        "text": "either you can open up the python interpreter and enter your commands one at a time, which is very useful for exploring data and experimenting with what you want to do, or you can put your code into a file and run it all at once."
    },
    {
        "text": "to run code in the interpreter on a mac or linux system, do the following: 1) go to the command terminal."
    },
    {
        "text": "2) type \u201cpython\u201d and press enter."
    },
    {
        "text": "this will display the command prompt >>>."
    },
    {
        "text": "3) type \u201cprint \u2018hello world\u2019\u201d and press enter."
    },
    {
        "text": "the phrase \u201chello world\u201d should print on the screen."
    },
    {
        "text": "4) the whole thing should appear as follows: >>> print \u201chello world!\u201d hello world!"
    },
    {
        "text": "5) congratulations!"
    },
    {
        "text": "you\u2019ve just run a line of python code."
    },
    {
        "text": "6) press ctrl\u2010d to close the interpreter."
    },
    {
        "text": "the process is very similar if you are working in a windows environment."
    },
    {
        "text": "in place of the command terminal you are likely to use powershell \u2014 it is the windows equivalent of a bash terminal."
    },
    {
        "text": "for editing your source code, visual studio is a powerful ide that is ubiquitous among windows programmers."
    },
    {
        "text": "personally i tend to write my scripts in plain text editors if it\u2019s at all practical, but especially for larger codebases a good ide becomes invaluable."
    },
    {
        "text": "3.3.3 more complicated script ok, now that you\u2019ve got python running, let\u2019s jump into the deep end."
    },
    {
        "text": "here is a more complicated python script."
    },
    {
        "text": "it has a data structure that describes"
    },
    {
        "text": "3 programming languages 24 employees of a company."
    },
    {
        "text": "it goes through the employee records, gives each one a 5% raise, and updates the record with the name of the state they live in."
    },
    {
        "text": "it then prints out information describing the updated employee data."
    },
    {
        "text": "don\u2019t worry if you can\u2019t read the whole thing right now: i\u2019ll explain what all the parts are."
    },
    {
        "text": "after we walk through this script, i\u2019ll give a more comprehensive overview of python\u2019s data types and how to work with them; the script doesn\u2019t show it all."
    },
    {
        "text": "salary_raise_factor = 0.05 state_code_map = {'wa': 'washington', 'tx': 'texas'} def update_employee_record(rec): old_sal = rec['salary'] new_sal = old_sal * (1 + salary_raise_factor) rec['salary'] = new_sal state_code = rec['state_code'] rec['state_name'] = state_code_map[state_code] input_data = [ {'employee_name': 'susan', 'salary': 100000.0, 'state_code': 'wa'}, {'employee_name': 'ellen', 'salary': 75000.0, 'state_code': 'tx'}, ] for rec in input_data: update_employee_record(rec) name = rec['employee_name'] salary = rec['salary'] state = rec['state_name'] print name + ' now lives in ' + state print ' and makes $' + str(salary) if you run this script, you will see the following output: susan now lives in washington and makes $110250.0 ellen now lives in texas and makes $78750.0 the first line of the script defines the variable salary_raise_factor to be the decimal number 0.05. the next line defines what\u2019s called a dict (short for dictionary) called state_ code_map, which maps the postal abbreviations of several states to their full names."
    },
    {
        "text": "a dict maps \u201ckeys\u201d to \u201cvalues,\u201d and they are enclosed within curly braces."
    },
    {
        "text": "there are commas between each key/value pair, and the key and value are separated by a colon."
    },
    {
        "text": "the keys in a dict are usually strings, but they can also"
    },
    {
        "text": "25 3.3 python crash course be numbers or any other atomic\u2010type except none (which we\u2019ll see in a min- ute)."
    },
    {
        "text": "the values can be any python object whatsoever, and different values can have different types."
    },
    {
        "text": "but in this case, the values are all strings."
    },
    {
        "text": "dicts are one of python\u2019s three main \u201ccontainer\u201d data types (i.e., it contains other data), the other two being lists and tuples."
    },
    {
        "text": "next up, the line def update_employee_record(rec): says that we are defining a function called update_employee_record that takes in a single argument and that the argument is called rec within the scope of this function."
    },
    {
        "text": "in our code, rec will always be a dict, but we have not specified that in the function declaration."
    },
    {
        "text": "you can pass an integer, string, or anything else into update_employee_record."
    },
    {
        "text": "in this case, it so happens that we later do operations to rec that will fail if it\u2019s not a dictionary (or some- thing that behaves like one), but python won\u2019t know anything is amiss until the operation fails."
    },
    {
        "text": "here we come to the most famous gotcha of python."
    },
    {
        "text": "the rest of the body of the function is all indented the same way: exactly four spaces."
    },
    {
        "text": "it could have been two spaces, or a tab, or any other whitespace combinations, but it must be consistent."
    },
    {
        "text": "consistency such as this is good practice in any programming lan- guage since it makes the code easier to read, but python requires it."
    },
    {
        "text": "this is the single most controversial thing about python, and it can get confusing if you\u2019re in a situation where tabs and spaces are getting mixed."
    },
    {
        "text": "in the body of the function, when we say old_sal = rec['salary'] we are pulling out the \u201csalary\u201d field in rec."
    },
    {
        "text": "this is how you get data out of a dict."
    },
    {
        "text": "by doing this, we are also tacitly assuming that there is a \u201csalary\u201d field: the code will throw an error if there isn\u2019t one."
    },
    {
        "text": "later when we say rec['salary'] = new_sal we are assigning to the \u201csalary\u201d field \u2013 creating the field if there isn\u2019t one, and overwriting it if there\u2019s already one there."
    },
    {
        "text": "the input_data variable is a list."
    },
    {
        "text": "lists can contain elements of any type, but in this case, they are all dictionaries."
    },
    {
        "text": "note that in this case, the values in the dictionaries are not all the same type: some are strings, but there is also a float field."
    },
    {
        "text": "in the last part of the script, the line for rec in input_data: will loop over all of the elements of input_data in a order, executing the body of the loop for each one."
    },
    {
        "text": "similar to function declarations, the body of the loop must be indented consistently."
    },
    {
        "text": "3 programming languages 26 the print statement here deserves a special mention."
    },
    {
        "text": "when we say print ' and makes $' + str(salary) there are three things going on: \u25cf \u25cfstr(salary) takes salary, which is a float like 75,000.0, and returns a string like \u201c75,000\u201d."
    },
    {
        "text": "str() is a function that takes many python objects and returns a string representation of them."
    },
    {
        "text": "\u25cf \u25cfadding two strings with + just concatenates them."
    },
    {
        "text": "adding a string to a float would have given an error, which is why we had to say str(salary)."
    },
    {
        "text": "\u25cf \u25cfthe print statement in python is a little weird."
    },
    {
        "text": "most built\u2010in functions in python are called with parentheses, but print doesn\u2019t use them."
    },
    {
        "text": "this rare inconsistency was remedied in python 3.0."
    },
    {
        "text": "3.3.4 atomic data types python has five main atomic data types that you\u2019ll have to worry about."
    },
    {
        "text": "if you have used a programming language in the past, they should mostly sound pretty familiar: \u25cf \u25cfint: a mathematical integer \u25cf \u25cffloat: a floating\u2010point number \u25cf \u25cfbool: a true/false flag \u25cf \u25cfstring: a piece of text of arbitrarily many characters (possibly 0 or 1) \u25cf \u25cfnonetype: this is a special type with only a single value none."
    },
    {
        "text": "it is often used as a placeholder when there is missing data or some process failed."
    },
    {
        "text": "declaring a variable that is an int or a float is very straightforward: my_integer = 2 my_other_integer = 2 + 3 my_float = 2.0 boolean values are similarly uncomplicated: my_true_bool = true my_false_bool = false this_is_true = (0 < 100) this_is_false = (0 > 100) nonetype is special."
    },
    {
        "text": "the only value it can take is called none, and this is often used as a placeholder when a variable should exist, but you don\u2019t want it to have a meaningful value."
    },
    {
        "text": "functions that fail in some way will also often return none to signify that there was a problem."
    },
    {
        "text": "27 3.4 strings 3.4\u00ad strings by far the most complicated of the atomic data types is string."
    },
    {
        "text": "a string is a piece of text of arbitrary length."
    },
    {
        "text": "you declare a string by enclosing the text in quotation marks."
    },
    {
        "text": "you can use single quotes or double quotes \u2013 they\u2019re equivalent to each other."
    },
    {
        "text": "however, you might want to enclose your string in double quotes if the string contains the single quote character, and vice versa."
    },
    {
        "text": "a_string = \"hello\" same_as_previous = 'hello' an_empty_string = \"\" w_a_single_quote = \"hello's\" in place of a single quotation character, you can also enclose a string in a triple of characters."
    },
    {
        "text": "unlike normal strings, one enclosed in triple quotes is also allowed to extend over multiple lines multi_line_string = \"\"\"line 1 line 2\"\"\" it\u2019s common to use triple quoted strings for things such as large pieces of text that are embedded in your code (say, some html that you\u2019re using a lot because your script is writing an html document)."
    },
    {
        "text": "if you want to put special characters, such as a tab, a newline, or a weird hex code into your string, you can do it by a process called \u201cescaping.\u201d when the \u201c\\\u201d character is written in the string, it and the next character together encode a single nonstandard character."
    },
    {
        "text": "the most common of these are the new line \u201c\\n\u201d, the tab \u201c\\t\u201d, and the slash character itself \u201c\\\\.\u201d to take a substring of a string in python, you use bracket notation as follows: >>> \"abcd\"[0] 'a' >>> \"abcd\"[0:2] 'ab' >>> \"abcd\"[1:3] 'bc' if you want to pull a single character out of a string, then you can do it with brackets such as this, passing in the index of the character you want: >>> \"abcd\"[0] 'a'"
    },
    {
        "text": "3 programming languages 28 note that the indices start at 0, not 1. if you want a substring of length greater than 1, you put starting and ending indices in the brackets: >>> \"abcd\"[0:2] 'ab' >>> \"abcd\"[1:3] 'bc' the first number says which index to start at, and the second number says which index to stop just short of."
    },
    {
        "text": "if the first number is omitted, then you will start at the beginning."
    },
    {
        "text": "if the second is omitted, you will continue to the end."
    },
    {
        "text": "so we can say >>> \"abcd\"[1:] 'bcd' you can also use negative indices."
    },
    {
        "text": "\u20131 will refer to the last element in the list, \u20132 to the one before, etc."
    },
    {
        "text": "so you can drop the last character in a string such as this: >>> \"abcd\"[:-1] 'abc' the next chapter will go into a lot more detail about the various tools that python has for working with strings."
    },
    {
        "text": "3.4.1 comments and docstrings there are two kinds of comments in python: \u25cf \u25cfthose denoted by a # character, such as this: # this whole line is a comment a = 5 # and the last part of this line is too \u25cf \u25cfstrings that take up a line (or more) in your code but aren\u2019t assigned to a variable."
    },
    {
        "text": "it\u2019s common practice to have a string at the beginning of a python file that describes what the file does and how to use it."
    },
    {
        "text": "such a string is called a doc- string."
    },
    {
        "text": "if you import the file as a library, then that library will have a field called __doc__ that acts as built\u2010in documentation."
    },
    {
        "text": "these things come in handy!"
    },
    {
        "text": "a function can also have a doc string, such as this: def sqr(x): \"this function just squares its input \" return x * x"
    },
    {
        "text": "29 3.4 strings 3.4.2 complex data types python has three main data containers: lists, tuples, and dicts."
    },
    {
        "text": "there is also one called a set that you will use less often."
    },
    {
        "text": "each of them contains other data struc- tures, hence the name."
    },
    {
        "text": "the first thing to know about containers in python is that, unlike many other languages, you can mix and match the types."
    },
    {
        "text": "a list can consist entirely of ints."
    },
    {
        "text": "but it can also contain tuples, dictionaries, user\u2010defined types, and even other lists."
    },
    {
        "text": "all of python\u2019s container types are classes, in the object\u2010oriented sense."
    },
    {
        "text": "however, they also all act as functions, which try to coerce their arguments into the appropriate type."
    },
    {
        "text": "for example, my_list = [\"a \", \"b \", \"c \"] my_set = set(my_list) my_tuple = tuple(my_list) will create a list and then create a set and a tuple that contain identical data."
    },
    {
        "text": "3.4.3 lists a list is just what it sounds like: an ordered list of variables."
    },
    {
        "text": "the following code shows basic usage: my_list = [\"a \", \"b \", \"c \"] print my_list[0] # prints \"a \" my_list[0] = \"a \" # changes that element of the list my_list.append(\"d \") # adds new element to the end # list elements can be anything mixed_list = [\"a \", 5.7, \"b \", [1,2,3]] there is a special operation called a list comprehension, which lets us create one list from another by applying the same operation to all of its elements (and possibly filtering out some of those elements): original_list = [1,2,3,4,5,6,7,8] squares = [x*x for x in original_list] squares_of_evens = [x*x for x in original list] if x%2==0] if you have not seen it before, there is one very important convention with list indexing that can be confusing at first: the first element in the list is element number 0. the second is element number 1, and so on."
    },
    {
        "text": "there are reasons (some of them historical) for this convention, and if it mystifies you, you\u2019re not alone."
    },
    {
        "text": "but you will have to get used to it with python."
    },
    {
        "text": "3 programming languages 30 if you want to select a subset of a list, then you can do it with a colon: my_list = [\"a\", \"b\", \"c\"] first_two_elements = my_list[0:3] the first number says which index to start at, and the second number says which index to stop just short of."
    },
    {
        "text": "if the first number is omitted, then you will start at the beginning."
    },
    {
        "text": "if the second is omitted, you will continue to the end."
    },
    {
        "text": "so we can say my_list = [\"a\", \"b\", \"c\"] first_two_elements = my_list[:3] last_two_elements = my_list[1:] you can also use negative indices."
    },
    {
        "text": "\u20131 will refer to the last element in the list, \u20132 to the one before, etc."
    },
    {
        "text": "so we can say my_list = [\"a\", \"b\", \"c\"] all_but_last_element = my_list[:-1] 3.4.4 strings and lists for complex string manipulation, one of the most flexible methods that you can call on strings is split()."
    },
    {
        "text": "it will break a string up on whitespace and return those parts of it as a list."
    },
    {
        "text": "alternatively, you can pass another string as an argu- ment, which will cause you to split on that string instead."
    },
    {
        "text": "it works such as this: >>> \"abc def\".split() ['abc', 'def'] >>> \"abc \\tdef\".split() ['abc', 'def'] >>> \"abc \\tdef\".split(' ') ['abc', '\\tdef'] >>> \"abcabd\".split(\"ab\") ['', 'c', 'd'] the inverse of split() is the join() method."
    },
    {
        "text": "it is called on a string, and you pass in a list of other strings."
    },
    {
        "text": "the strings are all then concatenated into one string, using the string as a delimiter."
    },
    {
        "text": "for example, >>> \",\".join([\"a\", \"b\", \"c\"]) 'a,b,c' you might have noticed that the syntax for selecting characters in a string is the same as that for selecting elements in a list."
    },
    {
        "text": "in general, it is called \u201cslice"
    },
    {
        "text": "31 3.4 strings notation,\u201d and it is possible to create other python objects that use the same notation."
    },
    {
        "text": "most generally, a slice takes in a start index, an end index, and how big the spacing should be."
    },
    {
        "text": "for example, >>> start, end, count_by = 1, 7, 2 >>> \"abcdefg\"[start: end: count_by] 'bdf' 3.4.5 tuples a tuple is conceptually a list that cannot be modified (no changing the ele- ments, no adding/removing elements)."
    },
    {
        "text": "having them may seem redundant, but tuples are much more efficient than lists in some cases, and they play a central role in the operation of python under the hood."
    },
    {
        "text": "there are also several things that, for technical reasons, you can do with tuples that you can\u2019t with lists."
    },
    {
        "text": "the most obvious of these is that the keys in a dictionary cannot be lists, but they can be tuples."
    },
    {
        "text": "my_tuple = (1, 2, \"hello world\") print my_tuple[0] # prints 1 my_tuple[1] = 5 # this will give an error!"
    },
    {
        "text": "there is one important piece of syntactic sugar to know that is often used with tuples."
    },
    {
        "text": "oftentimes, we want to give names to the different fields in a tuple, and it is clunky to explicitly define a new variable for each of them."
    },
    {
        "text": "in these cases, we can do multiple assignment as follows: my_tuple = (1, 2) zeroth_field, first_field = my_tuple 3.4.6 dictionaries a dictionary is a structure that takes in a key and returns a value."
    },
    {
        "text": "the keys for a dictionary are usually strings, but they can also be any other atomic data type or tuples (but they can\u2019t be lists or dictionaries)."
    },
    {
        "text": "the values can be anything at all \u2013 integers, other dictionaries, external libraries, etc."
    },
    {
        "text": "in defining a dictionary, you use curly braces, with a colon separating the key and its value: my_dict = {\"january\": 1, \"february\":2} print my_dict[\"january\"] # prints 1 my_dict[\"march\"] = 3 # add new element my_dict[\"january\"] = \"start of the year\" # overwrite old value as an interesting note, the python language itself is largely built out of dic- tionaries (or slight variations of them)."
    },
    {
        "text": "the namespace that stores all of your"
    },
    {
        "text": "3 programming languages 32 variables, for example, is a dictionary mapping the variables\u2019 names to the objects themselves."
    },
    {
        "text": "you can also create a dictionary by passing in a list of tuples to the function dict(), and you can create a list of tuples by calling the items() method on a dictionary: pairs = [(\"one\",1), (\"two\",2)] as_dict = dict(pairs) same_as_pairs = as_dict.items() 3.4.7 sets a set is somewhat similar to a dictionary with only keys and no values."
    },
    {
        "text": "it stores a collection of unique objects that are of atomic types."
    },
    {
        "text": "you can add new values to a set, which will do nothing if the value is already in it."
    },
    {
        "text": "you can also query the set to see if a value is in it."
    },
    {
        "text": "a simple shell script shows how this works: >>> s = set() >>> 5 in s false >>> s.add(5) >>> 5 in s true >>> s.add(5) # does nothing 3.5\u00ad defining functions a function in python is defined and called as follows: def my_function(x): y = x+1 x_sqrd = x*x return x_sqrd five_plus_one_sqrd = my_function(5) this is a so\u2010called \u201cpure function,\u201d meaning that it takes some input, returns an output, and does nothing else."
    },
    {
        "text": "a function can also have side effects, such as printing something to the screen or operating on a file."
    },
    {
        "text": "in our example script earlier, modifying the input dictionary was a side effect."
    },
    {
        "text": "if no return value is specified, the function will return none."
    },
    {
        "text": "you can also define optional arguments in a function, using this syntax: def raise(x, n=2): return pow(x,n)"
    },
    {
        "text": "33 3.5 defining functions two_sqrd = raise(2) two_cubed = raise(2, n=3) if the function you are defining only contains one line, and has no side effects, you can also define it using a so\u2010called lambda expression: sqr = lambda x : x*x five_sqrd = sqr(5) assigning a lambda expression to \"sqr\" is equivalent to the normal syntax for function definitions."
    },
    {
        "text": "the term \u201clambda\u201d is a reference to the lisp program- ming language, which defines functions using the \u201clambda\u201d keyword in a simi- lar way."
    },
    {
        "text": "lambda functions are mostly used if you\u2019re passing a one\u2010off function as an argument to another function, and there\u2019s no need to pollute the namespace with a new function name."
    },
    {
        "text": "for example, def apply_to_evens(a_list, a_func): return [a_func(x) for x in a_list if x%2==0] my_list = [1,2,3,4,5] sqrs_of_evens = apply_to_evens(my_list, lambda x:x*x) functions such as this, which are defined on the fly and never given an actual name, are called \u201canonymous functions.\u201d they can be very handy in data sci- ence, especially in big data."
    },
    {
        "text": "3.5.1 for loops and control structures the main control structure you do in practice is to loop over a list, as follows: my_list = [1, 2, 3] for x in my_list: print \"the number is \", x if you are iterating over a list of tuples (as you might if you\u2019re working with a dictionary), you can use the shorthand tuple notation i mentioned previously: for key, value in my_dict.items(): print \"the value for \", key, \" is \", value more generally, any data structure that allows for\u2010loops such as this is called \u201citerable.\u201d lists are the most prominent iterable data type, but they are far from the only one."
    },
    {
        "text": "3 programming languages 34 if statements are handled this way, if i < 3: print \"i is less than three\" elif i < 5: print \"i is between 3 and 5\" else: print \"i is greater than 5\" you don\u2019t see it that often in practice, but python also allows for while\u2010loops, which are similar to this: i = 0 while i < 5: print \"i is still less than five\" i = i+1 3.5.2 a few key functions python has a small number of built\u2010in functions that you should be aware of."
    },
    {
        "text": "function name action examples int cast to an int int(5.7) # rounds down int(\"5\") float cast to a float float(5) float(\"5.7\") bool cast to a bool bool(\"\") # false bool(\"asdf\") # true str cast to a str dict turns list of key/value tuples into dictionary dict([ (\"january\", 1), (\"february\", 2) ]) range range(n) gives a list of integers from 0 to n \u2212 1. that is, it starts at 0 and has length n range(5) # 0 to 4 range(4,18) # 4 to 17 zip take in two lists and pair off the elements into one list of tuples zip([\"sunday\", \"monday\", \"tuesday\"], range(3)) open opens a text file for reading or writing."
    },
    {
        "text": "the second argument is an \u201cr\u201d for reading the file and a \u201cw\u201d for writing it."
    },
    {
        "text": "you can also use \u201ca\u201d to just append to the end of a file # get file contents # as one big string open(\u201cfile.txt\u201d, \u201cr\u201d).read() # get file contents # as list of strings open(\u201cfile.txt\u201d, \"r\").readlines() # write open(\"file.txt\", \"w\")."
    },
    {
        "text": "write(\"hello world! \")"
    },
    {
        "text": "35 3.5 defining functions function name action examples len give the length of something."
    },
    {
        "text": "for a list or tuple, it will be the length."
    },
    {
        "text": "for a string, it will be the number of characters len(\"sdf\") # 3 len([1,2,3,4]) # 4 enumerate pass in some indexable object (usually a list)."
    },
    {
        "text": "get out index/value tuples, which give indices in the object and their corresponding values."
    },
    {
        "text": "useful if you are looping over a list, but you also need to keep track of the index for ind, val in mylist: print \"at %i\" % i print val 3.5.3 exception handling if python code fails, sometimes, we want to have the script be prepared for that and act accordingly (rather than just dying)."
    },
    {
        "text": "that is illustrated here: try: lines = input_text.split(\"\\n\") print \"tenth line was: \", lines[9] except: print \"there were < 10 lines\" 3.5.4 libraries to import functionality from an existing library, you use any of the following syntax: from my_lib import f1, f2 # f1 & f2 in namespace import other_lib as ol # ol.f1 is the f1 func from other_lib import * # f1 is in namespace generally, the first and second methods of importing a library make for the most readable code; if you import * from several libraries and then call f1 later on in your code, it\u2019s not obvious which library f1 came from."
    },
    {
        "text": "to write your own library, just write a .py file in which your functions, classes, or other objects are defined."
    },
    {
        "text": "it can then be imported using the aforementioned syntax."
    },
    {
        "text": "just make sure that your library is in the directory you are running your code from or in some other place that python can find it."
    },
    {
        "text": "3.5.5 classes and objects strictly speaking, everything in python (and i mean everything \u2013 integers, functions, classes, imported libraries, etc.)"
    },
    {
        "text": "is what\u2019s called an object."
    },
    {
        "text": "however, most of the language is built around a few high\u2010powered classes (such as lists"
    },
    {
        "text": "3 programming languages 36 and dictionaries) that do most of the heavy lifting, so it\u2019s common to use python only as a scripting language."
    },
    {
        "text": "however, if you want to define your own classes, you can do it this way: class dog: def __init__(self, name): self.name = name def respond_to_command(self, command): if command == self.name: self.speak() def speak(self): print \"bark bark!!\""
    },
    {
        "text": "fido = dog(\"fido\") fido.respond_to_command(\"spot\") # does nothing fido.respond_to_command(\"fido\") # prints bark bark here __init__ is a special function that gets called whenever an instance of the class is created."
    },
    {
        "text": "it does all of the initial setup required for the object."
    },
    {
        "text": "the one thing that throws a lot of people off is the \u201cself\u201d keyword that gets passed in as the first argument for every function in the class."
    },
    {
        "text": "when i call fido."
    },
    {
        "text": "respond_to_command, the \u201cself\u201d argument in respond_to_command refers to fido himself, that is, the dog object whose method is being called."
    },
    {
        "text": "this allows us to refer specifically to fido\u2019s data elements, such as self.name."
    },
    {
        "text": "for many object\u2010oriented languages, just saying \u201cname\u201d in resond_to_command will implicitly refer to fido\u2019s name, but python requires that it be explicit."
    },
    {
        "text": "it\u2019s similar to the keyword \u201cthis\u201d that you will see in languages such as c++."
    },
    {
        "text": "3.5.6 gotcha: hashable and unhashable types when i first started learning python, there was one big gotcha that i ran into."
    },
    {
        "text": "it caused me a lot of grief for a few days as i tried to figure out why my code was failing, and i would like to spare you my pain."
    },
    {
        "text": "python\u2019s data type falls into two categories: \u25cf \u25cfhashable types."
    },
    {
        "text": "this includes ints, floats, strings, tuples, and a few more obscure ones."
    },
    {
        "text": "these are generally low\u2010level data types, and instances of them are immutable."
    },
    {
        "text": "\u25cf \u25cfunhashable types include lists, dictionaries, and libraries."
    },
    {
        "text": "generally, unhash- able types are for larger, more complex objects, which have internal structure that can be modified."
    },
    {
        "text": "the biggest difference between hashable and unhashable types is illustrated in this shell session: >>> a = 5 # a is a hashable int >>> b = a # b points to a copy of a"
    },
    {
        "text": "3.6 python\u2019s technical libraries 37 >>> a = a + 1 >>> print b # b has not been incremented 5 >>> a = [] # a is an unhashable list >>> b = a # b points to the same list as a."
    },
    {
        "text": ">>> a.append(5) >>> b [5] when i say b = a, a copy of the hashable int is made in memory, and the vari- able name b is set to point to it."
    },
    {
        "text": "but when i\u2019m using unhashable lists and say b = a, the variable b is set to point to the exact same list!"
    },
    {
        "text": "if i had truly wanted to make a copy of a, so that appending to a didn\u2019t affect b, i could have said something like the following: >>> b = [x for x in a] which would have constructed a new list in memory."
    },
    {
        "text": "if a was a list of inte- gers, then a and b would be incapable of stepping on each other\u2019s toes: they would have their own separate copies of the numbers."
    },
    {
        "text": "however, if the elements of a were themselves unhashable types, then b would be distinct from a, but they would be pointing to the same objects."
    },
    {
        "text": "for example, >>> a = [{}, {}] # list of dicts >>> b = [x for x in a] >>> a[0][\"name\"] = \"bob\" >>> b[0][\"name\"] \"bob\" the other thing about hashable types is that the keys in a dictionary must be hashable."
    },
    {
        "text": "3.6 \u00adpython\u2019s technical libraries python was designed mostly as a tool for software engineers, but there is an excellent suite of libraries available that make it a first\u2010class environment for technical computing, competing with the likes of matlab\u00ae and r. the main ones, which will be covered in this book, are as follows: \u25cf \u25cfpandas: this is the big one for you to know."
    },
    {
        "text": "it stores and operates on data in data frames, very efficiently and with a sleek, intuitive api."
    },
    {
        "text": "\u25cf \u25cfnumpy: this is a library for dealing with numerical arrays in ways that are fast and memory efficient, but it\u2019s clunky and low level for a user."
    },
    {
        "text": "under the hood, pandas operates on numpy arrays."
    },
    {
        "text": "3 programming languages 38 \u25cf \u25cfscikit\u2010learn: this is the main machine learning library, and it operates on numpy arrays."
    },
    {
        "text": "you can take pandas objects, turn them into numpy arrays, and then plug them into scikit\u2010learn."
    },
    {
        "text": "\u25cf \u25cfmatplotlib: this is the big plotting and visualization library."
    },
    {
        "text": "similar to numpy, it is low level and a bit clunky to use directly."
    },
    {
        "text": "pandas provides human\u2010friendly wrappers that call matplotlib routines."
    },
    {
        "text": "\u25cf \u25cfscipy: this provides a suite of functions that perform fancy numerical oper- ations on numpy arrays."
    },
    {
        "text": "these aren\u2019t the only technical computing libraries available in python, but they\u2019re by far the most popular, and together they form a cohesive, powerful tool suite."
    },
    {
        "text": "numpy is the most fundamental library; it defines the core numerical arrays that everything else operates on."
    },
    {
        "text": "however, most of your actual code (especially data munging and feature extraction) will be working within pandas, only switching to the other libraries as needed."
    },
    {
        "text": "the rest of this chapter will be a quick crash course on the basic data structures of pandas."
    },
    {
        "text": "3.6.1 data frames the central kind of object in pandas is called a dataframe, which is similar to sql tables or r data frames."
    },
    {
        "text": "a data frame is a table with rows and columns, where each column holds data of a particular type (such as integers, strings, or floats)."
    },
    {
        "text": "dataframes make it easy and efficient to apply a function to every ele- ment in a column or to calculate aggregates such as the sum of a column."
    },
    {
        "text": "some of the basic operations on data frames are shown in this code: import pandas as pd # making data frame from a dictionary # that maps column names to their values df = pd.dataframe({ \"name\": [\"bob\", \"alex\", \"janice\"], \"age\": [60, 25, 33] }) # reading a dataframe from a file other_df = pd.read_csv(\u201cmyfile.csv\u201d) # making new columns from old ones # is really easy df[\"age_plus_one\"] = df[\"age\"] + 1 df[\"age_times_two\"] = 2 * df[\"age\"] df[\"age_squared\"] = df[\"age\"] * df[\"age\"] df[\"over_30\"] = (df[\"age\"] > 30) # this col is bools # the columns have various built-in aggregate functions"
    },
    {
        "text": "3.6 python\u2019s technical libraries 39 total_age = df[\"age\"].sum() median_age = df[\"age\"].quantile(0.5) # you can select several rows of the dataframe # and make a new dataframe out of them df_below50 = df[df[\"age\"] < 50] # apply a custom function to a column df[\"age_squared\"] = df[\"age\"].apply(lambda x: x*x) one important thing about dataframes is the notion of an index."
    },
    {
        "text": "this is basically a name (not necessarily unique) that is given to every row of the data frame."
    },
    {
        "text": "by default, the indexes are just the line numbers (starting at 0), but you can set the index to be other columns if you like: df = pd.dataframe({ \"name\": [\"bob\", \"alex\", \"jane\"], \"age\": [60, 25, 33] }) print df.index # prints 0\u20102, the line numbers # create a dataframe containing the same data, # but where name is the index df_w_name_as_ind = df.set_index(\"name\") print df_w_name_as_ind.index # prints their names # get the row for bob bobs_row = df_w_name_as_ind.ix[\"bob\"] print bobs_row[\"age\"] # prints 60 3.6.2 series besides dataframes, the other big data structure in pandas is the series."
    },
    {
        "text": "really, i\u2019ve already shown them to you: a column in a dataframe is a series."
    },
    {
        "text": "conceptually, a series is just an array of data objects, all the same type, with an index associated."
    },
    {
        "text": "the columns of a dataframe are series objects that all hap- pen to share the same index."
    },
    {
        "text": "the following code shows you some of the basic series operations, inde- pendent of their function in dataframes: >>> # import pandas."
    },
    {
        "text": "i always alias it as pd >>> import pandas as pd >>> s = pd.series([1,2,3]) # make series from list >>> >>> # display the values in s >>> # note index is to the far left >>> s"
    },
    {
        "text": "3 programming languages 40 0 1 1 2 2 3 dtype: int64 >>> s+2 # add a number to each element of s 0 3 1 4 2 5 dtype: int64 >>> s.index # you can access the index directly int64index([0, 1, 2], dtype='int64') >>> # adding two series will add corresponding elements to each other >>> s + pd.series([4,4,5]) 0 5 1 6 2 8 dtype: int64 now technically, i lied to you a minute ago when i said that a series object\u2019s elements all have to be the same type."
    },
    {
        "text": "they have to be the same type if you want all the performance benefits of pandas, but we have actually already seen a series object that mixes its types: >>> bobs_row = df_w_name_as_ind.ix[\"bob\"] >>> type(bobs_row) <class 'pandas.core.series.series'> >>> bobs_row age 60 age_plus_one 61 age_times_two 120 age_squared 3600 over_30 true name: tom, dtype: object so we can see that this row of a data frame was actually a series object."
    },
    {
        "text": "but instead of int64 or something similar, its dtype is \u201cobject.\u201d this means that under the hood, it\u2019s not storing a low\u2010level integer representation or anything similar; it\u2019s storing a reference to an arbitrary python object."
    },
    {
        "text": "3.6.3 joining and grouping so far we\u2019ve focused on the following dataframe operations: \u25cf \u25cfcreating data frames"
    },
    {
        "text": "3.6 python\u2019s technical libraries 41 \u25cf \u25cfadding new columns that are derived from basic operations on existing columns \u25cf \u25cfusing simple conditions to select rows in a dataframe \u25cf \u25cfaggregating columns \u25cf \u25cfsetting columns to function as an index, and using the index to pull out rows of the data."
    },
    {
        "text": "this section discusses two more advanced operations: joining and grouping."
    },
    {
        "text": "these may be familiar to you from working with sql."
    },
    {
        "text": "joining is used if you want to combine two separate data frames into a single frame containing all the data."
    },
    {
        "text": "we take two data frames, match up rows that have a common index, and combine them into a single frame."
    },
    {
        "text": "this shell session shows it: >>> df_w_age = pd.dataframe({ \"name\": [\"tom\", \"tyrell\", \"claire\"], \"age\": [60, 25, 33] }) >>> df_w_height = pd.dataframe({ \"name\": [\"tom\", \"tyrell\", \"claire\"], \"height\": [6.2, 4.0, 5.5] }) >>> joined = df_w_age.set_index(\"name\").join( df_w_height.set_index(\"name\")) >>> print joined age height name tom 60 6.2 tyrell 25 4.0 claire 33 5.5 >>> print joined.reset_index() name age height 0 tom 60 6.2 1 tyrell 25 4.0 2 claire 33 5.5 the other thing we often want to do is to group the rows based on some property and aggregate each group separately."
    },
    {
        "text": "this is done with the groupby() function, the use of which is shown here: >>> df = pd.dataframe({ \"name\": [\"tom\", \"tyrell\", \"claire\"], \"age\": [60, 25, 33], \"height\": [6.2, 4.0, 5.5],"
    },
    {
        "text": "3 programming languages 42 \"gender\": [\"m\", \"m\", \"f\"] }) >>> # use built-in aggregates >>> print df.groupby(\"gender\").mean() age height gender f 33.0 5.5 m 42.5 5.1 >>> medians = df.groupby(\"gender\").quantile(0.5) >>> # use a custom aggregation function >>> def agg(ddf): return pd.series({ \"name\": max(ddf[\"name\"]), \"oldest\": max(ddf[\"age\"]), \"mean_height\": ddf[\"height\"].mean() }) >>> print df.groupby(\"gender\").apply(agg) mean_height name oldest gender f 5.5 claire 33 m 5.1 tom 60 3.7\u00ad other python resources one of the benefits of using python is that there is a huge amount of very clear documentation available online."
    },
    {
        "text": "it\u2019s extremely easy to just google around and find the right syntax or libraries to do whatever it is you need to get done."
    },
    {
        "text": "besides just searching around, i recommend the following resources: \u25cf \u25cfhttps://docs.python.org/2/ this is the main resource for documentation of python version 2\u2019s syntax."
    },
    {
        "text": "\u25cf \u25cfhttp://pandas.pydata.org/ the official documentation for the pandas library."
    },
    {
        "text": "\u25cf \u25cfhttp://scikit\u2010learn.org/stable/index.html the documentation for scikit\u2010 learn."
    },
    {
        "text": "this is some of the best documentation i\u2019ve ever seen for software."
    },
    {
        "text": "most of it is example scripts that show off all the various things you can do with scikit\u2010learn."
    },
    {
        "text": "3.8\u00ad further reading one of the benefits of learning python (or any programming language) is that there is a huge amount of very clear documentation available online."
    },
    {
        "text": "it\u2019s extremely easy to just search around and find the right syntax or libraries to do whatever it is you need to get done."
    },
    {
        "text": "43 3.9 glossary besides general browsing, i can recommend several specific resources that are great for coming up to speed: 1 pilgrim, m, 2004, dive into python: python from novice to pro, viewed 7 august 2016, http://www.diveintopython.net/."
    },
    {
        "text": "2 pandas: python data analysis library, viewed 7 august 2016, http://pandas."
    },
    {
        "text": "pydata.org/."
    },
    {
        "text": "3 https://www.python.org/, viewed 7 august 2016, the python software foundation."
    },
    {
        "text": "4 scott, m, programming language pragmatics, 4th edn, 2015, morgan kaufmann, burlington, ma."
    },
    {
        "text": "3.9\u00ad glossary anonymous function a function that is never given a name."
    },
    {
        "text": "dataframe the main pandas data structure."
    },
    {
        "text": "it stores a dataset as a table with rows and columns."
    },
    {
        "text": "dict a python object, which maps keys (which must be of a hashable type) to values (which can be any type)."
    },
    {
        "text": "hashable type ints, floats, strings, and a couple other low\u2010level python data types."
    },
    {
        "text": "index identifiers for each row in a dataframe or element in a series."
    },
    {
        "text": "join an operation that takes two data frames and concatenates matching rows into a large data frame."
    },
    {
        "text": "rows match if they have the same entries in whatever column you are joining on."
    },
    {
        "text": "list a python object that stores an ordered list of objects."
    },
    {
        "text": "it is an unhashable type, so we can do things such as appending now elements."
    },
    {
        "text": "numpy a low\u2010level python library for efficiently processing numerical arrays."
    },
    {
        "text": "pandas a high\u2010level python library for manipulating data."
    },
    {
        "text": "it defines the dataframe and series types and is implemented using numpy under the hood."
    },
    {
        "text": "pure function a function with no side effects."
    },
    {
        "text": "series a pandas data type for storing a sequence of objects."
    },
    {
        "text": "the columns of a dataframe are actually series objects."
    },
    {
        "text": "set a python container type that acts as a mathematical set."
    },
    {
        "text": "side effect a modification that is made to an existing object in memory, as opposed to creating a new object while leaving existing ones intact."
    },
    {
        "text": "operations such as print to the screen and file interactions are also side effects."
    },
    {
        "text": "tuple a python object that stores an ordered sequence of objects."
    },
    {
        "text": "unlike lists, tuples are immutable and hashable."
    },
    {
        "text": "unhashable type any python type that is not hashable."
    },
    {
        "text": "examples include lists, dicts, and user\u2010defined classes."
    },
    {
        "text": "when you assign an unhashable object to a variable name, you will get a pointer to the original object, rather than to a copy of it."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 45 every data scientist has their own set of preferred programming languages, libraries, and other tools."
    },
    {
        "text": "you will have to decide what works best for you."
    },
    {
        "text": "to give you a data point though, here is how i work when do data analysis: \u25cf \u25cfmy main programming language for data science is python."
    },
    {
        "text": "i know it, i love it, and i can do just about anything with it."
    },
    {
        "text": "i also use it for production coding whenever i am choosing the tools and there\u2019s no good reason not to."
    },
    {
        "text": "\u25cf \u25cfi use pandas as my main data analysis library, and i supplement it with scikit\u2010 learn for machine learning."
    },
    {
        "text": "\u25cf \u25cfi usually use matplotlib for visualizations, but i\u2019m looking to branch out."
    },
    {
        "text": "in particular, bokeh is an extremely promising recent arrival to the visualization scene."
    },
    {
        "text": "it is designed particularly for making interactive graphs that you access with a web browser."
    },
    {
        "text": "\u25cf \u25cfa lot of people use an integrated development environment (ide) for python, such as spyder or pycharm."
    },
    {
        "text": "personally though, i\u2019m a little old school: i open up python from the command line, and i edit my scripts in a plain text editor such as sublime or textwrangler."
    },
    {
        "text": "i\u2019m considering switching to a browser\u2010based notebook though, such as jupyter."
    },
    {
        "text": "\u25cf \u25cfi do most of my work on a mac, but that\u2019s just because it\u2019s what my employ- ers tend to use."
    },
    {
        "text": "i usually do hobby projects using linux, and i\u2019m hoping to do more work on windows in the future because they have a famously great set of tools for developers."
    },
    {
        "text": "\u25cf \u25cfwhen i\u2019m doing big data i use pyspark, which i\u2019ll talk about in the chapter on big data."
    },
    {
        "text": "i used to use r, but not anymore if i can avoid it."
    },
    {
        "text": "the syntax has always annoyed me, but the breaking point came a few years ago."
    },
    {
        "text": "i had an r script operating on a massive dataset that i had run and debugged several times, and it would always fail several hours in because of some memory issues."
    },
    {
        "text": "it was extremely frustrating, and i was getting close to a deadline."
    },
    {
        "text": "so, finally, i gave up on r and rewrote the entire thing in python; it finished in 45 minutes the first time i ran it."
    },
    {
        "text": "in fairness, i wrote my python code to be pretty efficient in its interlude: my personal toolkit"
    },
    {
        "text": "\ufeff interlude: my personal toolkit 46 memory usage, and my r code had used the notoriously inefficient plyr library, but it left a lasting impression."
    },
    {
        "text": "by the time you read this, my toolkit may have changed."
    },
    {
        "text": "new toys are con- stantly becoming available, and it\u2019s important to stay abreast of them."
    },
    {
        "text": "some people are forever trying out the newest libraries, always eager to find slightly better ways of doing things."
    },
    {
        "text": "personally, i\u2019m more inclined to wait until it\u2019s clear that a new tool is better before jumping on the bandwagon, so as not to spend a lot of time learning things that become obsolete."
    },
    {
        "text": "but no matter how you do it, one of the coolest parts of data science is the constant learning of new techniques."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 47 4 this chapter is about some of the pathologies that you will see in real\u2010world data."
    },
    {
        "text": "it talks about some of the most common (and notorious!)"
    },
    {
        "text": "ones, where they come from, and how they can be addressed."
    },
    {
        "text": "data pathologies come in roughly two types."
    },
    {
        "text": "the first are formatting issues."
    },
    {
        "text": "this includes inconsistent capitalization, extraneous whitespaces, and things of that nature."
    },
    {
        "text": "often, these are straightforward to solve with appropriate pre- processing of the data."
    },
    {
        "text": "the second category involves the actual content of the data."
    },
    {
        "text": "duplicate entries, major outliers, and null values are all examples."
    },
    {
        "text": "it often requires some detective work to figure out what these issues mean in a particular situation and hence how they should be addressed."
    },
    {
        "text": "my goals in this chapter are twofold."
    },
    {
        "text": "firstly, i want to give you an apprecia- tion for the breadth of issues that can be present in real\u2010world data and equip you to quickly identify and diagnose problems."
    },
    {
        "text": "secondly, i want to teach you tools that can be used to solve the problems."
    },
    {
        "text": "specifically, i will discuss various types of string manipulation."
    },
    {
        "text": "manipulating strings of text might seem boring at first glance, but it\u2019s one of the most powerful tools a data scientist can have."
    },
    {
        "text": "i would put it on par with machine learning itself."
    },
    {
        "text": "string manipulation can be used to address any data formatting problems, and in many cases, it is the only suitable solution."
    },
    {
        "text": "but it is also invaluable for creating scripts to pull information out of raw data."
    },
    {
        "text": "sometimes, when you encounter a new dataset, there is a \u201cright\u201d way to pro- cess it, which requires learning a new organizational paradigm and compli- cated tools that implement it."
    },
    {
        "text": "alternatively, the quick\u2010and\u2010dirty way is to spend an hour hacking together a script that pulls out the specific data you need."
    },
    {
        "text": "you can guess which of these approaches is often more expedient if you need pre- liminary results by tomorrow."
    },
    {
        "text": "the first part of this chapter will discuss a number of usual suspects when it comes to data issues."
    },
    {
        "text": "i will start with problems involving the data content, including some of the reasons they often arise."
    },
    {
        "text": "i will then move on to format- ting issues and discuss how they can be addressed using strings."
    },
    {
        "text": "finally, i will data munging: string manipulation, regular expressions, and data cleaning"
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 48 discuss the \u201cbig guns\u201d in string manipulation: pattern matching via regular expressions."
    },
    {
        "text": "4.1 \u00adthe worst dataset in the world the worst industrial dataset that i ever worked with was the first one."
    },
    {
        "text": "it was a collection of server logs, describing queries that had been received by a large collection of servers that my client owned."
    },
    {
        "text": "a given server could be referred to by a number of different names."
    },
    {
        "text": "most lines of the logs were gobbledygook that were useless to me."
    },
    {
        "text": "some of the key fields were encoded in weird hexadecimal."
    },
    {
        "text": "there were no rows or columns; instead, each line had its own structure."
    },
    {
        "text": "it was awful."
    },
    {
        "text": "then i worked with my second industrial dataset and discovered that they\u2019re all like that."
    },
    {
        "text": "your worst dataset will probably be your first one too."
    },
    {
        "text": "whenever there is a large organization, a complicated data collection process, or several datasets that have been merged, issues tend to pile up."
    },
    {
        "text": "they are rarely docu- mented and often only come to light when some poor data scientist is tasked with analyzing them."
    },
    {
        "text": "you have been warned."
    },
    {
        "text": "4.2 \u00adhow to identify pathologies one of the most embarrassing things that can happen in data science is to have to retract results that you\u2019ve presented because you realize that you processed the data incorrectly."
    },
    {
        "text": "given how convoluted datasets often are, you should have a healthy degree of paranoia about this happening."
    },
    {
        "text": "to identify these issues early, i have four pieces of advice: \u25cf \u25cfif the data is text, look directly at the raw file rather than just reading it into your script."
    },
    {
        "text": "\u25cf \u25cfread supporting documentation, if it\u2019s available."
    },
    {
        "text": "often, the data is hard to understand because it uses strange codes or conventions, whose meaning is documented in some accompanying pdf files or something."
    },
    {
        "text": "in other cases, the data seems pretty self\u2010explanatory, but there are nonobvious problems that only show up when you read the details."
    },
    {
        "text": "\u25cf \u25cfhave a battery of standard diagnostic questions you ask about the data."
    },
    {
        "text": "does this column contain nulls?"
    },
    {
        "text": "are all the identifiers in table a present in table b, and vice versa?"
    },
    {
        "text": "things like that."
    },
    {
        "text": "\u25cf \u25cfdo sanity checks, where you use the data to derive things that you already know."
    },
    {
        "text": "if you count the customers in the dataset and it isn\u2019t equal to the num- ber of customers you know the company has, then chances are you weren\u2019t identifying unique customers correctly."
    },
    {
        "text": "49 4.3 problems with data content 4.3\u00ad problems with data content 4.3.1 duplicate entries you should always check for duplicate entries in a dataset."
    },
    {
        "text": "sometimes, they are important in some real\u2010world way."
    },
    {
        "text": "in those cases, you usually want to con- dense them into one entry, adding an additional column that indicates how many unique entries there were."
    },
    {
        "text": "in other cases, the duplication is purely a result of how the data was gener- ated."
    },
    {
        "text": "for example, it might be derived by selecting several columns from a larger dataset, and there are no duplicates if you count the other columns."
    },
    {
        "text": "4.3.2 multiple entries for a single entity this case is a little more interesting than duplicate entries."
    },
    {
        "text": "often, each real\u2010 world entity logically corresponds to one row in the dataset, but some entities are repeated multiple times with different data."
    },
    {
        "text": "the most common cause of this is that some of the entries are out of date, and only one row is currently correct."
    },
    {
        "text": "in other cases, there actually should be duplicate entries."
    },
    {
        "text": "for example, each \u201centity\u201d might be a power generator with several identical motors in it."
    },
    {
        "text": "each motor could give its own status report, and all of them will be present in the data with the same serial number."
    },
    {
        "text": "another field in the data might tell you which motor is actually which."
    },
    {
        "text": "in the cases where the motor isn\u2019t specified in a data field, the different rows will often come in a fixed order."
    },
    {
        "text": "another case where there can be multiple entries is if, for some reason, the same entity is occasionally processed twice by whatever gathered the data."
    },
    {
        "text": "this happens in many manufacturing settings, because they will retool broken com- ponents and send them through the assembly line multiple times rather than scrapping them outright."
    },
    {
        "text": "4.3.3 missing entries most of the time when some entities are not described in a dataset, they have some common characteristics that kept them out."
    },
    {
        "text": "for example, let\u2019s say that there is a log of all transactions from the past year."
    },
    {
        "text": "we group the transactions by customer and add up the size of the transactions for each customer."
    },
    {
        "text": "this dataset will have only one row per customer, but any customer who had no transactions in the past year will be left out entirely."
    },
    {
        "text": "in a case such as this, you can join the derived data up against some known set of all customers and fill in the appropriate values for the ones who were missing."
    },
    {
        "text": "in other cases, missing data arises because data was never gathered in the first place for some entities."
    },
    {
        "text": "for example, maybe two factories produce a par- ticular product, but only one of them gathers this particular data about them."
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 50 4.3.4 nulls null entries typically mean that we don\u2019t know a particular piece of informa- tion about some entity."
    },
    {
        "text": "the question is: why?"
    },
    {
        "text": "most simply, nulls can arise because the data collection process was botched in some way."
    },
    {
        "text": "what this means depends on the context."
    },
    {
        "text": "when it comes time to do analytics, nulls cannot be processed by many algorithms."
    },
    {
        "text": "in these cases, it is often necessary to replace the missing values with some reasonable proxy."
    },
    {
        "text": "what you will see most often is that it is guessed from other data fields, or you simply plug in the mean of all the non\u2010null values."
    },
    {
        "text": "in other cases, the null values arise because that data was never collected."
    },
    {
        "text": "for example, some measurements might be taken at one factory that produces widgets but not at another."
    },
    {
        "text": "the table of all collected data for all widgets will then contain nulls for whenever the widget\u2019s factory didn\u2019t collect that data."
    },
    {
        "text": "for this reason, whether a variable is null can sometimes be a very powerful feature."
    },
    {
        "text": "the factory that produced the widget is, after all, potentially a very important determinant for whatever it is you want to predict, independent of whatever other data you gathered."
    },
    {
        "text": "4.3.5 huge outliers sometimes, a massive outlier in the data is there because there was truly an aberrant event."
    },
    {
        "text": "how to deal with that depends on the context."
    },
    {
        "text": "sometimes, the outliers should be filtered out of the dataset."
    },
    {
        "text": "in web traffic, for example, you are usually interested in predicting views by humans."
    },
    {
        "text": "a huge spike in recorded traffic is likely to come from a bot attack, rather than any activities of humans."
    },
    {
        "text": "in other cases, outliers just mean missing data."
    },
    {
        "text": "some storage systems don\u2019t allow the explicit concept of a null value, so there is some predetermined value that signifies missing data."
    },
    {
        "text": "if many entries have identical, seemingly arbi- trary values, then this might be what\u2019s happening."
    },
    {
        "text": "4.3.6 out\u2010of\u2010date data in many databases, every row has a timestamp for when it was entered."
    },
    {
        "text": "when an entry is updated, it is not replaced in the dataset; instead, a new row is put in that has an up\u2010to\u2010date timestamp."
    },
    {
        "text": "for this reason, many datasets include entries that are no longer accurate and only useful if you are trying to recon- struct the history of the database."
    },
    {
        "text": "4.3.7 artificial entries many industrial datasets have artificial entries that have been deliberately inserted into the real data."
    },
    {
        "text": "this is usually done for purposes of testing the soft- ware systems that process the data."
    },
    {
        "text": "4.4 formatting issues 51 4.3.8 irregular spacings many datasets include measurements taken at regular spacings."
    },
    {
        "text": "for example, you could have the traffic to a website every hour or the temperature of a physi- cal object measured at every inch."
    },
    {
        "text": "most of the algorithms that process data such as this assume that the data points are equally spaced, which presents a major problem when they are irregular."
    },
    {
        "text": "if the data is from sensors measuring something such as temperature, then typically you have to use interpolation techniques (which i discuss in a later chapter) to generate new values at a set of equally spaced points."
    },
    {
        "text": "a special case of irregular spacings happens when two entries have identical timestamps but different numbers."
    },
    {
        "text": "this usually happens because the times- tamps are only recorded to finite precision."
    },
    {
        "text": "if two measurements happen within the same minute, and time is only recorded up to the minute, then their timestamps will be identical."
    },
    {
        "text": "4.4 \u00adformatting issues 4.4.1 formatting is irregular between different tables/columns this happens a lot, typically because of how the data was stored in the first place."
    },
    {
        "text": "it is an especially big issue when joinable/groupable keys are irregularly formatted between different datasets."
    },
    {
        "text": "4.4.2 extra whitespace for such a small issue, it is almost comical how often random whitespace con- founds analyses when people try to, say, join the identifier \u201cabc\u201d against \u201cabc \u201d for two different datasets."
    },
    {
        "text": "whitespace is especially insidious because when you print the data to the screen to examine it, the whitespace might be impos- sible to discern."
    },
    {
        "text": "in python, every string object has a strip() method that removes whitespace from the front and end of a string."
    },
    {
        "text": "the methods lstrip() and rstrip() will remove whitespace only from the front and end, respectively."
    },
    {
        "text": "if you pass a character as an argument into the strip functions, only that character will be stripped."
    },
    {
        "text": "for example, >>> \"abc\\t\".strip() 'abc' >>> \" abc\\t\".lstrip() 'abc\\t' >>> \" abc\\t\".rstrip() ' abc' >>> \"abc\".strip(\"c\") 'ab'"
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 52 4.4.3 irregular capitalization python strings have lower() and upper() methods, which will return a copy of the original string with all letters set to uppercase or lowercase."
    },
    {
        "text": "4.4.4 inconsistent delimiters usually, a dataset will have a single delimiter, but sometimes, different tables will use different ones."
    },
    {
        "text": "the most common delimiters you will see are as follows: \u25cf \u25cfcommas \u25cf \u25cftabs \u25cf \u25cfpipes (the vertical line \u201c|\u201d)."
    },
    {
        "text": "4.4.5 irregular null format there are a number of different ways that missing entries are encoded into csv files, and they should all be interpreted as nulls when the data is read in."
    },
    {
        "text": "some popular examples are the empty string \u201c\u201d, \u201cna,\u201d and \u201cnull.\u201d occasionally, you will see others such as \u201cunavailable\u201d or \u201cunknown\u201d as well."
    },
    {
        "text": "4.4.6 invalid characters some data files will randomly have invalid bytes in the middle of them."
    },
    {
        "text": "some programs will throw an error if you try to open up anything that isn\u2019t valid text."
    },
    {
        "text": "in these cases, you may have to filter out the invalid bytes."
    },
    {
        "text": "the following python code will create a string called s, which is not validly formatted text."
    },
    {
        "text": "the decode() method takes in two arguments."
    },
    {
        "text": "the first is the text format that the string should be coerced into (there are several, which i will discuss later in the chapter on file formats)."
    },
    {
        "text": "the second is what should be done when such coercion isn\u2019t possible; saying \u201cignore\u201d means that invalid characters simply get dropped."
    },
    {
        "text": ">>> s = \"abc\\xff\" >>> print s # note how last character isn\u2019t a letter abc\u25a1 >>> s.decode(\"ascii\", \"ignore\") u'abc' 4.4.7 weird or incompatible datetimes datetimes are one of the most frequently mangled types of data field."
    },
    {
        "text": "some of the date formats you will see are as follows: \u25cf \u25cfaugust 1, 2013 \u25cf \u25cfaug 1, \u201813 \u25cf \u25cf2013\u201008\u201013"
    },
    {
        "text": "4.4 formatting issues 53 there is an important way that dates and times are different from other for- matting issues."
    },
    {
        "text": "most of the time you have two different ways of expressing the same information, and a perfect translation is possible from the one to the other."
    },
    {
        "text": "but with dates and times, the information content itself can be different."
    },
    {
        "text": "for example, you might have just the date, or there could also be a time associ- ated with it."
    },
    {
        "text": "if there is a time, does it go out to the minute, hour, second, or something else?"
    },
    {
        "text": "what about time zones?"
    },
    {
        "text": "most scripting languages include some kind of built\u2010in datetime data struc- ture, which lets you specify any of these different parameters (and uses reason- able defaults if you don\u2019t specify)."
    },
    {
        "text": "generally speaking, the best way to approach datetime data is to get it into the built\u2010in data types as quickly as possible, so that you can stop worrying about string formatting."
    },
    {
        "text": "the easiest way to parse dates in python is with a package called dateutil, which works as follows: >>> import dateutil.parser as p >>> p.parse(\"august 13, 1985\") datetime.datetime(1985, 8, 13, 0, 0) >>> p.parse(\"2013-8-13\") datetime.datetime(2013, 8, 13, 0, 0) >>> p.parse(\"2013-8-13 4:15am\") datetime.datetime(2013, 8, 13, 4, 15) it takes in a string, uses some reasonable rules to determine how that string is encoding dates and times, and coerces it into the datetime data type."
    },
    {
        "text": "note that it rounds down \u2013 august 13th becomes 12:00 am on august 13th, and so on."
    },
    {
        "text": "4.4.8 operating system incompatibilities different operating systems have different file conventions, and sometimes, that is a problem when opening a file that was generated on one os on a com- puter that runs a different one."
    },
    {
        "text": "probably, the most notable place where this occurs is newlines in text files."
    },
    {
        "text": "in mac and linux, a newline is conventionally denoted by the single character \u201c\\n.\u201d on windows, it is often two characters \u201c\\r\\n.\u201d many data processing tools check what operating system they are being run on so that they know which convention to use."
    },
    {
        "text": "4.4.9 wrong software versions sometimes, you will have a file of a format that is designed to be handled by a specific software package."
    },
    {
        "text": "however, when you try to open it, a very mystifying error is thrown."
    },
    {
        "text": "this happens, for example, with data compression formats."
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 54 oftentimes the culprit ends up being that the file was originally gener- ated with one version of the software."
    },
    {
        "text": "however, the software has changed in the meantime, and you are now trying to open the file with a different version."
    },
    {
        "text": "4.5 \u00adexample formatting script the following script illustrates how you can use hacked\u2010together string for- matting to clean up disgusting data and load it into a pandas dataframe."
    },
    {
        "text": "let\u2019s say we have the following data in a file: name|age|birthdate ms. janice joplin|65|january 19, 1943 bob dylan |74 years| may 24 1941 billy ray joel|66yo|feb."
    },
    {
        "text": "9, 1941 it\u2019s clear to a human looking at the data what it\u2019s supposed to mean, but it\u2019s the kind of thing that might be terrible if you opened it with a csv file reader."
    },
    {
        "text": "the following code will take care of the pathologies and make things more explicit."
    },
    {
        "text": "it\u2019s not exactly pretty or efficient, but it gets the job done, it\u2019s easy to understand, and it would be easy to modify if it needed changing: def get_first_last_name(s): invalid_name_parts = [\"mr\", \"ms\", \"mrs\", \"dr\", \"jr\", \"sir\"] parts = s.lower().replace(\"."
    },
    {
        "text": "\",\"\").strip().split() parts = [p for p in parts if p not in invalid_name_parts] if len(parts)==0: raise valueerror( \"name %s is formatted wrong\" % s) first, last = parts[0], parts[-1] first = first[0].upper() + first[1:] last = last[0].upper() + last[1:] return first, last def format_age(s): chars = list(s) # list of characters digit_chars = [c for c in chars if c.isdigit()] return int(\"\".join(digit_chars)) def format_date(s): month_map = {"
    },
    {
        "text": "4.6 regular expressions 55 \"jan\": \"01\", \"feb\": \"02\", \"may\": \"03\"} s = s.strip().lower().replace(\",\", \"\") m, d, y = s.split() if len(y) == 2: y = \"19\" + y if len(d) == 1: d = \"0\" + d return y + \"-\" + month_map[m[:3]] + \"-\" + d import pandas as pd df = pd.read_csv(\"file.tsv\", sep=\"|\") df[\"first name\"] = df[\"name\"].apply( lambda s: get_first_last_name(s)[0]) df[\"last name\"] = df[\"name\"].apply( lambda s: get_first_last_name(s)[1]) df[\"age\"] = df[\"age\"].apply(format_age) df[\"birthdate\"] = df[\"birthdate\"].apply( format_date).astype(pd.datetime) print df 4.6 \u00adregular expressions regular expressions are one of the \u201cbig guns\u201d standard tools in data processing."
    },
    {
        "text": "they take many of the operations we just discussed (split, index, etc."
    },
    {
        "text": "), which take in specific strings as arguments, and generalize them to apply to a pattern."
    },
    {
        "text": "for example, say we want to pull all phone numbers that match the (xxx) xxx\u2010xxxx pattern out of a document."
    },
    {
        "text": "that would be very onerous with nor- mal strings but a synch with regular expressions."
    },
    {
        "text": "the \u201cregular expression\u201d is a string that encodes the pattern you\u2019re looking for."
    },
    {
        "text": "before we go much farther, i should let you know that regular expressions are a bit notorious for being finicky to use and debug."
    },
    {
        "text": "this is because while it is possible to express a stupendous array of patterns within regular expressions, the expressions themselves quickly become complicated enough that it\u2019s hard for humans to wrap their heads around."
    },
    {
        "text": "this is a fairly fundamental prob- lem \u2013 \u201cunderstanding\u201d a pattern you want is much, much easier than specify- ing every jot and tittle of what constitutes that pattern."
    },
    {
        "text": "i\u2019m reminded of the phrase \u201cdamn it computer: do what i want, not what i say.\u201d the way around this is to avoid regular expressions that are overly complex."
    },
    {
        "text": "so as long as you keep them short enough that they are easy to understand, they are extremely powerful."
    },
    {
        "text": "the other caveat about regular expression is that they are computationally expensive."
    },
    {
        "text": "there are many different ways that a piece of text can potentially match a complicated pattern, and it takes a while to check them all."
    },
    {
        "text": "in fact, even the process of compiling the regular expression itself into a computa- tion\u2010ready data structure takes a while."
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 56 4.6.1 regular expression syntax let\u2019s start with a very simple regular expression: \u201cab*\u201d."
    },
    {
        "text": "this means that the pattern is exactly one occurrence of the letter \u201ca,\u201d followed by some number (possibly 0) of \u201cb\u201ds \u2013 the \u201c*\u201d means arbitrary repetition of whatever letter came right before it."
    },
    {
        "text": "in the string \"abcd abb,\" the pattern occurs twice: the initial \"ab\" and the \"abb\" at the end."
    },
    {
        "text": "right there though we run into the first subtle point of regular expressions: how do we pick which matches to find?"
    },
    {
        "text": "we said that the \u201cab\u201d at the start was a match, because it was an \u201ca\u201d followed by one \u201cb\u201d."
    },
    {
        "text": "but technically the \u201ca\u201d would have been a match on its own \u2013 it\u2019s just a match with zero \u201cb\u201ds rather than one."
    },
    {
        "text": "in situations such as this, do we want to find all possible matches even if they overlap?"
    },
    {
        "text": "if we only want nonoverlapping matches, how do we pick which ones?"
    },
    {
        "text": "the general answer is to start at the left\u2010most part of the text and find the larg- est possible match."
    },
    {
        "text": "then you chop this match out of the text and find the larg- est possible match from the remaining text, and so on."
    },
    {
        "text": "this is called the \"greedy\" approach \u2013 there are times when you want to override it, but they\u2019re typically not common."
    },
    {
        "text": "greedy parsing is fantastic from a performance perspective, because it requires a single pass through a piece of text and generally only requires a small portion of it to be kept in memory at any one time."
    },
    {
        "text": "regular expression is still inefficient, but this does much to ease the pain."
    },
    {
        "text": "in many applications, the matches will be returned as they are found, with the whole parser acting as one giant iterator."
    },
    {
        "text": "especially for large pieces of text where many matches are expected, this is the most efficient way to do it."
    },
    {
        "text": "the match objects returned by the iterator will include several pieces of information such as the matching string itself and its start/end indices."
    },
    {
        "text": "however, in my own work, i usually end up using simpler approaches that just return all matches as a list of strings, rather than an iterator of match objects."
    },
    {
        "text": "now let\u2019s see some of the more complicated types of pattern that can be specified: type of pattern regular expression example matches notes a fixed string abc123 abc123 \u201cabc123\u201d contains no special characters, so it\u2019s just a string to be matched."
    },
    {
        "text": "arbitrary repetition a*b b ab aaab \u201c*\u201d means that you can have an arbitrary number (possibly 0) of the previous character."
    },
    {
        "text": "repeat character at least once a+b ab aaaab"
    },
    {
        "text": "4.6 regular expressions 57 type of pattern regular expression example matches notes repeat character at most once a?b b ab repeat a character a fixed number of times a{5} aaaaa repeat a pattern a fixed number of times (a*b){3} baabab ababaaaab repeat a character or pattern a variable number of times a{2,4} aa aaa aaaa note that the range is inclusive."
    },
    {
        "text": "choice of several characters [ab]c ac bc the brackets means that you can have any single character from within the brackets."
    },
    {
        "text": "arbitrary mixture of several characters [ab]*c c aac abbac in this case, the * is applied to the whole [ab] expression."
    },
    {
        "text": "ranges of characters [a\u2010h][a\u2010z]* aasdfalsd hb g [a\u2010h] is shorthand for the characters from a to h. you can do the same thing with digits."
    },
    {
        "text": "characters other than a particular one [^ab] c d the ^ as the first argument in [] means to match any character not in that group."
    },
    {
        "text": "if ^ is not the first character, then it has no special meaning."
    },
    {
        "text": "choice of several expressions dr|mr|ms|mrs dr mr ms mrs here you can select."
    },
    {
        "text": "nesting expressions ([a\u2010z]|[a\u2010z]|[0\u20109])* a azsdfcvfg this matches any alphanumeric string."
    },
    {
        "text": "in python, \\w is shorthand for this."
    },
    {
        "text": "start of a line ^ab matches any \"ab\" that occurs at the start of your text or just after a newline."
    },
    {
        "text": "end of a line ab$ matches an \"ab\" that is at the end of the document or just after a newline."
    },
    {
        "text": "(continued )"
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 58 type of pattern regular expression example matches notes special characters \\[ [ if you want to include one of the special characters in your pattern, you can escape it with a \\."
    },
    {
        "text": "any character except newline ."
    },
    {
        "text": "a * _ nongreedy evaluation <.*>?"
    },
    {
        "text": "<h1> </h2 name=\"foo\"> this causes it to find the shortest possible path, rather than the longest."
    },
    {
        "text": "whitespace \\s this matches any whitespace character, such as spaces, tabs, or a newline there are others, and regular expression syntax can vary a little bit between languages and libraries."
    },
    {
        "text": "however, these should be enough to get you started."
    },
    {
        "text": "the nongreedy character deserves some special explanation."
    },
    {
        "text": "let\u2019s say that we have the following xml data: <name>jane</name><name>bob</name> and we want to pull out the name fields."
    },
    {
        "text": "you might try to use the regular expression <name>."
    },
    {
        "text": "*</name> but that will end up matching the entire string."
    },
    {
        "text": "this is because \u201c</ name><name\u201d in the middle matches the \u201c."
    },
    {
        "text": "*\u201d, and regular expression try to match as much text as possible."
    },
    {
        "text": "if instead you say <name>."
    },
    {
        "text": "*?</name> then you will get the two matches, because it tries to match \u201c."
    },
    {
        "text": "*\u201d to as little text as possible."
    },
    {
        "text": "python\u2019s implementation is a relatively lightweight library called re."
    },
    {
        "text": "the fol- lowing python code shows how to read in a file and use regular expressions to look for street addresses."
    },
    {
        "text": "it\u2019s not perfect, but it will work pretty well."
    },
    {
        "text": "import re # this matches \"1600 pennsylvania ave.\" # it does not match \"5 stony brook st\" # cuz there is a space in \"stony brook\" street_pattern = r\"^[0-9]\\s[a-z][a-z]*\" + \\ r\"(street|st|rd|road|ave|avenue|blvd|way|wy)\\."
    },
    {
        "text": "?$\" # like the one above, this assumes # there is no space in the town name"
    },
    {
        "text": "4.6 regular expressions 59 city_pattern = r\"^[a-z][a-z]*,\\s[a-z]{2},[0-9]{5}$\" address_pattern = street_pattern + r\"\\n\" \\ + city_pattern # compile the string into a regular expression object address_re = re.compile(address_pattern) text = open(\"some_file.txt\", \"r\").read() matches = re.findall(address_re, text) # list of all strings that match open(\"addresses_w_space_between.txt\", \"w\").write(\"\\n\\n\".join(matches)) you should notice the following things about that code: 1) it\u2019s very powerful!"
    },
    {
        "text": "this is only a few lines, but it is doing a very complicated task."
    },
    {
        "text": "2) it\u2019s limited."
    },
    {
        "text": "there are many idiosyncrasies of addresses that the human eye can spot that will elude this regular expression."
    },
    {
        "text": "it won\u2019t handle apartment numbers, multiword street names, or even \"32nd street.\""
    },
    {
        "text": "you can patch these problems up as you find them, but you risk the code becoming unwieldy."
    },
    {
        "text": "3) we are declaring our strings as \"raw strings,\" by putting an r in front of the opening quote."
    },
    {
        "text": "the last thing is a practical measure when doing regular expressions in python, because using the escape character \\ can become a massive pain."
    },
    {
        "text": "the problem is that if we say pattern = \"\\n\" my_re = re.compile(pattern) # trying to match a newline we have not done what we intended to do."
    },
    {
        "text": "the string called pattern is a one\u2010 character string, consisting of the newline character."
    },
    {
        "text": "but re.compile would require a two\u2010character string, with the first character being a slash and the second being an n. we could instead have said # escape the slash w another slash pattern = \"\\\\n\" # this matches a newline newline_re = re.compile(pattern) but this becomes extremely unwieldy if we want to, say, include the slash character in the pattern we are looking for."
    },
    {
        "text": "the pattern to match a single slash would be \"\\\\\\\\\"."
    },
    {
        "text": "4 data munging: string manipulation, regular expressions, and data cleaning 60 putting the r before the quotes in python creates a \"raw string,\" meaning that the exact contents of the quotes is the string."
    },
    {
        "text": "life is just easier that way."
    },
    {
        "text": "4.7\u00ad life in the trenches this is a fairly short chapter, because there isn\u2019t a lot to say about data cleaning that generalizes well."
    },
    {
        "text": "in a lot of ways, it is the boring part of data science, a price we must pay to get things into a format where we can ask the real questions."
    },
    {
        "text": "at the same time though, it is an intellectually challenging, problem\u2010solving activity \u2013 often more so than the analysis itself."
    },
    {
        "text": "in many data science projects, there is a staggering amount of detective work and coding required to just get the data into clean tabular form, but after that all you do is fit a line or some- thing equally trivial."
    },
    {
        "text": "data cleaning code is one of the areas where data science blurs into produc- tion coding."
    },
    {
        "text": "there is a lot of room for creativity and experimentation in how you extract features from data or what analyses you run."
    },
    {
        "text": "generally though, there is only one \u201cright\u201d way to clean the data, and the code tends to get written once and then reused between different iterations of analysis and feature extraction."
    },
    {
        "text": "once you understand the data itself and have written the cleaning scripts, it is time to move on to understanding the world it is describing."
    },
    {
        "text": "this is the world of visualizations and exploratory analysis."
    },
    {
        "text": "4.8 \u00adglossary regular expression a way to specific a general pattern that strings can match."
    },
    {
        "text": "regular expressions can be finicky to use, but they are extremely powerful."
    },
    {
        "text": "string formatting a nifty way in python and many other languages to insert content into template strings."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 61 5 a rule of thumb for data science deliverables is this: if there isn\u2019t a picture, then you\u2019re doing it wrong."
    },
    {
        "text": "typically, a good analytics project starts (after cleaning and understanding the data) with exploratory visualizations that help you develop hypotheses and get a feel for the data, and it ends with carefully manicured figures that make the final results visually obvious."
    },
    {
        "text": "the actual number crunching is hidden in the middle, sometimes almost as an aside."
    },
    {
        "text": "i\u2019ve had a number of projects where there was never even any actual machine learning: people needed to know whether there was signal in the data and which directions were most promising for further work (which would poten- tially include machine learning), and graphics showed that more clearly than a number ever could."
    },
    {
        "text": "this fact is very underappreciated outside of the data analysis community."
    },
    {
        "text": "many people think of data scientists as numerical badasses, working black magic from a command line."
    },
    {
        "text": "but that\u2019s just not the way the human brain pro- cesses data, generates hypotheses, or develops familiarity with an area."
    },
    {
        "text": "pictures are plans a\u2013c for everything except the last stages of statistically validating results."
    },
    {
        "text": "i\u2019ve often joked that if humans were able to visualize things in a thou- sand dimensions, then my job as a data scientist would consist entirely of generating and looking at scatterplots."
    },
    {
        "text": "this chapter will take you through several of the most important visualiza- tions."
    },
    {
        "text": "you\u2019ve probably seen most of this before, but it\u2019s always good to revisit the basics."
    },
    {
        "text": "we will also cover some exploratory metrics (such as correlations), which capture, in crude numerical form, some of the patterns that are clear from a good visual."
    },
    {
        "text": "there are many techniques not covered in this chapter, and you would do well to learn them."
    },
    {
        "text": "however, my experience is that these core ones will cover most of your needs."
    },
    {
        "text": "i strongly recommend memorizing the syntax for basic visualizations in your programming language of choice."
    },
    {
        "text": "in exploratory analysis especially, it\u2019s useful to be able to chug through vari- ous ways of visualizing your data without needing to consult a reference on the syntax."
    },
    {
        "text": "visualizations and simple metrics"
    },
    {
        "text": "5 visualizations and simple metrics 62 there are, however, still times when we need a number."
    },
    {
        "text": "there are two rea- sons for this: \u25cf \u25cfour eyes can trick us, so it\u2019s important to have a cold hard statistic too."
    },
    {
        "text": "\u25cf \u25cfoften, you don\u2019t have time to sift through every possible picture, and you need some way to put a number on it so that the computer can make deci- sions of some sort automatically (even if the decision is only which pictures are worth your time to look at)."
    },
    {
        "text": "besides visualization techniques, this chapter will cover some standard sta- tistical metrics that strive to capture, in numerical form, some of the meaning that you can get out of a picture."
    },
    {
        "text": "5.1 \u00ada note on python\u2019s visualization tools the main visualization tool for python is a library called matplotlib."
    },
    {
        "text": "while matplotlib is powerful and flexible, it is probably the weakest link in python\u2019s technical stack."
    },
    {
        "text": "the graphs can be a bit cartoonish, in some ways the syntax is nonintuitive, and the interactivity (zooming in, etc.)"
    },
    {
        "text": "leaves something to be desired."
    },
    {
        "text": "most of the appearance issues can be fixed by tweaking a graphic\u2019s configuration, but the default settings are not great."
    },
    {
        "text": "i\u2019m sticking with matplotlib for this book because it is by far the most stand- ard tool, it is sufficient for most data science (especially if you learn some of the ways you can make the plots look prettier), and it integrates well with the other libraries."
    },
    {
        "text": "but there are other libraries out there that are gaining ground, espe- cially browser\u2010based ones such as bokeh and plot.ly."
    },
    {
        "text": "example code in this chapter will use pandas whenever possible."
    },
    {
        "text": "however, pandas\u2019 visualizations are a wrapper\u2010around matplotlib, and sometimes, we have to use matplotlib directly."
    },
    {
        "text": "typically, you make an image by calling the plot() method on a pandas object, and pandas does all the image formatting under the hood."
    },
    {
        "text": "then you use matplotlib\u2019s pyplot module for things such as setting the title and the final act of either displaying the image or saving it to a file."
    },
    {
        "text": "5.2 \u00adexample code to illustrate the visualization techniques we discuss in this chapter, we will apply them to the famous iris dataset, which you may have seen in a statistics textbook."
    },
    {
        "text": "it describes physical measurements taken of flower specimens, drawn from three different species of iris."
    },
    {
        "text": "there are 150 data points, 50 from each species, and each data point gives the length and width of the pedals and sepals."
    },
    {
        "text": "5.3 pie charts 63 the following code sets the stage for all of the example code in this chapter."
    },
    {
        "text": "it imports the relevant libraries and creates a dataframe containing the sample dataset (which comes built\u2010in to scikit\u2010learn): import pandas as pd from matplotlib import pyplot as plt import sklearn.datasets def get_iris_df(): ds = sklearn.datasets.load_iris() df = pd.dataframe(ds['data'], columns = ds['feature_names']) code_species_map = dict(zip( range(3), ds['target_names'])) df['species'] = [code_species_map[c] for c in ds['target']] return df df = get_iris_df() 5.3 \u00adpie charts pity the poor pie chart."
    },
    {
        "text": "i feel like i never see it used in \u201cserious\u201d applications, almost as if it\u2019s looked down on as being too simple."
    },
    {
        "text": "but pie charts are really one of the clearest ways to present data, and i recommend using them whenever they\u2019re applicable."
    },
    {
        "text": "technically, everything you get from a pie chart you could get equally well from looking at a list of numbers, but making sense of the numbers requires cognitive effort and attention."
    },
    {
        "text": "on the other hand, lower\u2010level neural circuits make immediate sense of pie charts."
    },
    {
        "text": "this is perhaps the clearest illustration of the guid- ing principle behind all visualizations: it\u2019s not about conveying information, but about conveying it in a way that the human brain will understand and care about."
    },
    {
        "text": "in my own work, the most mileage i\u2019ve gotten out of pie charts is when i\u2019m either doing exploratory analysis of a dataset (how many of our customers are senior citizens?"
    },
    {
        "text": "how many of the views came from the united states?)"
    },
    {
        "text": "or communicating the results of a binary classifier."
    },
    {
        "text": "the code to generate a basic pie chart using pandas is very simple: sums_by_species = df.groupby('species').sum() var = 'sepal width (cm)' sums_by_species[var].plot(kind='pie', fontsize=20) plt.ylabel(var, horizontalalignment='left') plt.title('breakdown for ' + var, fontsize=25) plt.savefig('iris_pie_for_one_variable.jpg') plt.close()"
    },
    {
        "text": "5 visualizations and simple metrics 64 it will produce this figure: breakdown for sepal width (cm) setosa virginica versicolor sepal width (cm) note that some of the text overlaps."
    },
    {
        "text": "little things such as this can happen in matplotlib if you use the default settings, and, in general, you will need to tweak the graph\u2019s configurations if you want things to look polished."
    },
    {
        "text": "the previous figure was made by calling the plot() method on a pandas series object, whose index gave the flower species."
    },
    {
        "text": "if we instead call it on a dataframe with multiple columns, we can generate a different chart for each column all in the same figure: sums_by_species = df.groupby('species').sum() sums_by_species.plot(kind='pie', subplots=true, layout=(2,2), legend=false) plt.title('total measurements, by species') plt.savefig('iris_pie_for_each_variable.jpg') plt.close() that code will give us the following: setosa virginica versicolor sepal length (cm) setosa virginica versicolor petal length (cm) setosa virginica versicolor sepal width (cm) setosa virginica versicolor total measurements, by species petal width (cm)"
    },
    {
        "text": "5.4 bar charts 65 5.4 \u00adbar charts the same information that is in a pie chart could equally well be conveyed in a bar chart."
    },
    {
        "text": "in this particular case, it\u2019s actually a much more sensible visualiza- tion, since we\u2019re interested in the relative sizes of the different flowers rather than how big a slice of the \u201cflower pie\u201d they each take up."
    },
    {
        "text": "the following code sums_by_species = df.groupby('species').sum() var = 'sepal width (cm)' sums_by_species[var].plot(kind='bar', fontsize=15, rot=30) plt.title('breakdown for ' + var, fontsize=20) plt.savefig('iris_bar_for_one_variable.jpg') plt.close() sums_by_species = df.groupby('species').sum() sums_by_species.plot( kind='bar', subplots=true, fontsize=12) plt.suptitle('total measurements, by species') plt.savefig('iris_bar_for_each_variable.jpg') plt.close() will produce the following visualizations: 180 breakdown for sepal width (cm) 160 140 120 100 80 60 40 20 0 setosa versicolor virginica"
    },
    {
        "text": "5 visualizations and simple metrics 66 350 300 250 200 150 100 50 0 300 250 200 150 100 50 0 120 petal width (cm) petal length (cm) sepal width (cm) sepal length (cm) petal width (cm) petal length (cm) sepal width (cm) sepal length (cm) total measurements, by species 100 80 60 40 20 0 140 160 180 120 100 80 60 40 200 setosa versicolor virginica note the following pieces of python\u2019s plotting syntax that let us tweak the appearance of the figure: \u25cf \u25cfthe \u201cfont size\u201d optional argument controls how big a piece of font is."
    },
    {
        "text": "it\u2019s usually too small by default."
    },
    {
        "text": "\u25cf \u25cfthe \u201crot\u201d optional argument lets us rotate the text."
    },
    {
        "text": "\u25cf \u25cfwe used suptitle() to give the title for the overall figure \u2013 pandas will by default label each subplot with its corresponding column in the dataframe being plotted."
    },
    {
        "text": "there are others available if you are trying to make the figures look really polished."
    },
    {
        "text": "5.5 \u00adhistograms histograms are probably my personal favorite visualization tool, partly because it seems like they usually contain something interesting."
    },
    {
        "text": "there are often dis- tinct bumps in the histogram, which might correspond to several distinct"
    },
    {
        "text": "5.5 histograms 67 classes of real\u2010world entities."
    },
    {
        "text": "you can get a sense of whether there are a few distinct outliers, how much variation is in the population, and so on."
    },
    {
        "text": "a histo- gram is almost always a meaningful thing to make; it works for floating values, or integers, and unlike scatterplots, you only need one numerical field."
    },
    {
        "text": "the following code will produce histograms for all the columns and put them together in one figure: df.plot(kind='hist', subplots=true, layout=(2,2)) plt.suptitle('iris histograms', fontsize=20) plt.show() the final image appears as follows: 50 100 iris histograms 80 60 40 20 0 40 30 20 degree degree degree degree 10 0 45 35 25 15 5 40 30 20 10 0 0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8 60 50 40 30 20 10 0 sepal width (cm) sepal length (cm) petal length (cm) petal width (cm) what jumps out at you is that the petal length has a clearly bimodal distribu- tion, suggesting that this one species has almost categorically longer petals."
    },
    {
        "text": "we can confirm this by plotting each species separately, but on the same axes and in different colors: for spec in df['species'].unique(): forspec = df[df['species']==spec] forspec['petal length (cm)'].plot( kind='hist', alpha=0.4, label=spec)"
    },
    {
        "text": "5 visualizations and simple metrics 68 plt.legend(loc='upper right') plt.suptitle('petal length by species') plt.savefig('iris_hist_by_spec.jpg') it yields the following graph."
    },
    {
        "text": "i know you can\u2019t see the color in the book you\u2019re holding, but sure enough the peak on the left is only the iris setosa variety."
    },
    {
        "text": "14 petal length by species 12 10 8 degree 6 4 2 00 1 2 3 4 5 6 7 setosa virginica versicolor there are two big problems that occur with histograms."
    },
    {
        "text": "the first one is the number and size of the bins you use."
    },
    {
        "text": "if the bins are too large, then you can obscure fascinating patterns that occur within a single bucket."
    },
    {
        "text": "if they are too small, then many of your buckets will contain no points, and your bell\u2010shaped curve will turn into a bunch of one\u2010unit high bars."
    },
    {
        "text": "the second problem is that sometimes your data can mar the picture."
    },
    {
        "text": "there might be one bucket that contains so many points, for example, that every other bucket is squashed down to what looks like noise."
    },
    {
        "text": "there might, for exam- ple, be a massive spike at 0.0, and you have to filter out those points before you draw the histogram."
    },
    {
        "text": "the other visual problem is outliers, which can smash the overwhelming majority of the points to the far left of the graph."
    },
    {
        "text": "in some cases, this is pretty simple to deal with \u2013 you have a handful of points that are massive outliers, and all you need to do is filter out those points."
    },
    {
        "text": "those points are aberrations, and it makes sense to remove them before drawing analytical conclusions."
    },
    {
        "text": "but in my experience, it\u2019s usually not that simple."
    },
    {
        "text": "rather than a handful of massive outliers, there is often a fat tail, representing a very real phenomenon in your data."
    },
    {
        "text": "you can cut part of the tail out of your dataset for purposes of making the visualization clearer, and you will probably have to, but in doing so, you will be cutting out very real, meaningful signal."
    },
    {
        "text": "you are not throwing out a"
    },
    {
        "text": "5.6 means, standard deviations, medians, and quantiles 69 few points that are clearly aberrations; you\u2019re picking a more\u2010or\u2010less arbitrary threshold and looking only at the part of your dataset that falls below it."
    },
    {
        "text": "5.6 \u00admeans, standard deviations, medians, and quantiles sometimes, of course, you must summarize a distribution down to just a few numbers."
    },
    {
        "text": "usually, these summaries are based on the assumption that your data\u2019s distribution is bell\u2010shaped, and your goal is to give some idea of where the peak of the bell is and how widely it spreads."
    },
    {
        "text": "within this vein, there are two main options: 1) give the mean and standard deviation."
    },
    {
        "text": "these are the more historically popular metrics, and they are much easier to compute."
    },
    {
        "text": "2) give the median, 25th percentile, and 75th percentile."
    },
    {
        "text": "these metrics are more robust to pathologies in the data, but they are computationally more expensive (since you must sort a list)."
    },
    {
        "text": "they can be calculated as follows: col = df['petal length (cm)'] average = col.mean() std = col.std() median = col.quantile(0.5) percentile25 = col.quantile(0.25) percentile75 = col.quantile(0.75) these numbers all still exist even if your data has multiple peaks in the dis- tribution, but their usual intuitive interpretation breaks down."
    },
    {
        "text": "the other pathology that deserves some discussion is outliers in the data."
    },
    {
        "text": "this isn\u2019t a huge problem for medians and quantiles, but it can be game\u2010chang- ing with mean and standard deviation."
    },
    {
        "text": "this is shown in the following figure, where i have simulated data from a log\u2010normal distribution (log\u2010normals are prone to outliers), made a histogram, and plotted the mean as a dashed vertical line."
    },
    {
        "text": "the handful of very large outliers have pulled the mean well to the right of the actual hump in the distribution."
    },
    {
        "text": "outliers make it very hard to give an intuitive interpretation of the mean, but in fact, the situation is even worse than that."
    },
    {
        "text": "for a real\u2010world distribution, there always is a mean (strictly speaking, you can define distributions with no mean, but they\u2019re not realistic), and when we take the average of our data points, we are trying to estimate that mean."
    },
    {
        "text": "but when there are massive outli- ers, just a single data point is likely to dominate the value of the mean and standard deviation, so much more data is required to even estimate the mean, let alone make sense of it."
    },
    {
        "text": "5 visualizations and simple metrics 70 a common solution is to, before calculating the mean, throw out all data points that are deemed to be outliers: a common criterion is anything below the 25th percentile or above the 75th percentile."
    },
    {
        "text": "you can do that as follows: col = df['petal length (cm)'] perc25 = col.quantile(0.25) perc75 = col.quantile(0.75) clean_avg = col[(col>perc25)&(col<perc75)].mean() this workaround corresponds to the idea that these outliers are pathological data points, which really should be discarded if we\u2019re trying to understand the underlying phenomena."
    },
    {
        "text": "if we\u2019re dealing with measurements from a physical sensor, for example, they might have been caused by a malfunction of our hard- ware."
    },
    {
        "text": "in other situations though, such as the amount of money in a transaction, the outliers are extremely important data points that can\u2019t be discarded."
    },
    {
        "text": "the median is not perfect either."
    },
    {
        "text": "in the case of outlier data, or even just a lopsided bell curve, it moves away from the hump in the bell curve."
    },
    {
        "text": "the median also does not change at all if you perturb the outlier values."
    },
    {
        "text": "however, it still keeps its user\u2010friendly meaning: half the values are greater and half are less."
    },
    {
        "text": "personally, i generally use median if i want to know what\u2019s \u201ctypical,\u201d but i use mean if the average behavior is really what i care about from a business perspective."
    },
    {
        "text": "5.7 \u00adboxplots boxplots are a convenient way to summarize a dataset by showing the median, quantiles, and min/max values for each of the variables."
    },
    {
        "text": "the following code snippet makes a boxplot of the sepal length for each of the species in the iris 0 0 5 10 15 20 25 30 35 mean is not at the hump 10 20 30 40 50"
    },
    {
        "text": "5.7 boxplots 71 dataset."
    },
    {
        "text": "the boxplot makes it glaringly obvious that the three species are dif- ferent from one another."
    },
    {
        "text": "col = \u2019sepal length (cm)\u2019 df['ind'] = pd.series(df.index).apply(lambda i: i% 50) df.pivot('ind','species')[col].plot(kind='box') plt.show() 4.0 setosa versicolor virginica 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 an advantage of boxplots is that major outliers are very visually obvious."
    },
    {
        "text": "here is a boxplot of the data we put in a histogram in the previous section: 0 none 10 20 30 40 50"
    },
    {
        "text": "5 visualizations and simple metrics 72 note that the upper quantile is much farther from the median compared to the lower quantile, and the effect is even more pronounced for the min and max values."
    },
    {
        "text": "if you just use the histogram, outliers can show up as a deceptively slight increase in the thickness of the tails."
    },
    {
        "text": "5.8 \u00adscatterplots i\u2019ve often joked that if humans could see things in an arbitrary number of dimensions, then all of my data science work would consist of making and interpreting scatterplots."
    },
    {
        "text": "in my experience, they are one of the simplest but most powerful ways to visualize relationships within a dataset, so they\u2019re a great first step when you\u2019re finding your feet with a new project."
    },
    {
        "text": "a simple scatterplot is very easy to generate in python: df.plot(kind=\"scatter\", x=\"sepal length (cm)\", y=\"sepal width (cm)\") plt.title(\"length vs width\") plt.show() 1.54.0 4.5 5.0 5.5 6.0 sepal length (cm) length versus width sepal width (cm) 6.5 7.0 7.5 8.0 8.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 besides the basic plotting, scatterplots can have several other bells and whis- tles, which allow more than just the two dimensions to be packed in."
    },
    {
        "text": "they include the following: \u25cf \u25cfcolor coding."
    },
    {
        "text": "often data points that fall into different categories are given different colors."
    },
    {
        "text": "you can also use color in a continuous way, such as different mixtures of red and blue."
    },
    {
        "text": "5.8 scatterplots 73 \u25cf \u25cfsize."
    },
    {
        "text": "changing the size of data points communicates another dimension of information, similarly to color coding."
    },
    {
        "text": "it also has the often\u2010desirable ability to draw attention disproportionately to some points instead of others."
    },
    {
        "text": "\u25cf \u25cfopacity."
    },
    {
        "text": "in scatterplots and other visualizations, it is often useful to make things partially transparent in case they overlap with other parts of the visualization."
    },
    {
        "text": "these parameters are often useful when doing exploratory analysis of a data- set, but they can be especially compelling when you\u2019re putting together final visualizations for use in final reports and presentations."
    },
    {
        "text": "if you want to control the formatting of a plot in python, you can do it by passing optional arguments into the scatter() function."
    },
    {
        "text": "the ones of most inter- est are as follows: \u25cf \u25cfc: a string indicating the color to make the dots."
    },
    {
        "text": "you can also pass in a sequence of such strings if the dots are to be of different colors."
    },
    {
        "text": "\u25cf \u25cfs: the size that each point should be, in pixels."
    },
    {
        "text": "alternatively, you can pass in a sequence of sizes."
    },
    {
        "text": "\u25cf \u25cfmarker: a string indicating what marker should be used in the plot."
    },
    {
        "text": "\u25cf \u25cfalpha: the transparency."
    },
    {
        "text": "1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 sepal length (cm) sepal width (cm) 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0 8.5 setosa versicolor virginica for example, the following script will produce the figure indicated: plt.close() colors = [\"r\", \"g\", \"b\"] markers= [\"."
    },
    {
        "text": "\", \"*\", \"^\"]"
    },
    {
        "text": "5 visualizations and simple metrics 74 fig, ax = plt.subplots(1, 1) for i, spec in enumerate(df['species'].unique() ): ddf = df[df['species']==spec] ddf.plot(kind=\"scatter\", x=\"sepal width (cm)\", y=\"sepal length (cm)\", alpha=0.5, s=10*(i+1), ax=ax, color=colors[i], marker=markers[i], label=spec) plt.legend() plt.show() it is immediately clear from the picture that the iris setosa flowers stand apart as having sepals that are markedly longer and narrower."
    },
    {
        "text": "5.9 \u00adscatterplots with logarithmic axes a key variation on scatterplots is using logarithmic axes."
    },
    {
        "text": "in many applications, the numbers being plotted are all positive (or at least nonnegative), but they can vary by orders of magnitude."
    },
    {
        "text": "this might happen if you are looking at traffic to a collection of websites, where some sites receive vastly more views than others or personal income."
    },
    {
        "text": "in a scatterplot of data such as this, all but the largest data points will be squashed to one side, making the plot essentially unreadable."
    },
    {
        "text": "here is a good example using a dataset from scikit\u2010learn."
    },
    {
        "text": "i am making a scat- terplot of the crime rate in a neighborhood versus the median home value: import pandas as pd import sklearn.datasets as ds import matplotlib.pyplot as plt # make pandas dataframe bs = ds.load_boston() df = pd.dataframe(bs.data, columns=bs.feature_names) df['medv'] = bs.target # normal scatterplot df.plot(x='crim',y='medv',kind='scatter') plt.title(\u2019crime rate on normal axis\u2019) plt.show() note how almost all of the data points are squashed to the left, making the graph hard to read for all but the most high\u2010crime neighborhoods."
    },
    {
        "text": "instead, we could have made the x\u2010axis logarithmic, as follows: df.plot(x='crim',y='medv',kind='scatter',logx=true) plt.title('crime rate on logarithmic axis') plt.show()"
    },
    {
        "text": "5.9 scatterplots with logarithmic axes 75 0 10\u20133 10\u20132 10\u20131 100 101 102 crim crime rate on logarithmic axis medv 10 20 30 40 50 60 in the second plot the tick marks on the x\u2010axis are irregular."
    },
    {
        "text": "they are all equally spaced in the figure."
    },
    {
        "text": "however, they correspond to small changes in numbers on the left but large numbers on the right."
    },
    {
        "text": "this has the effect of taking our original graph and widening out the left\u2010hand part while we shorten out the right\u2010hand part."
    },
    {
        "text": "the two code snippets will create the following scatterplots 0\u201320 0 20 40 60 80 100 crim crime rate on normal axis medv 10 20 30 40 50 60"
    },
    {
        "text": "5 visualizations and simple metrics 76 with this rescaling, we can see that there is a clear inverse relationship between crime rate and median home value that exists across all levels of crime."
    },
    {
        "text": "mathematically, we make a logarithmic plot by taking the log of the raw data and using that to tell where to place the points."
    },
    {
        "text": "the caveat to this is that loga- rithmic plots only work when all values are greater than 0. in many situations, for example, if you are counting events, the data can be 0 but is guaranteed to never be negative."
    },
    {
        "text": "in this case, it\u2019s common to just add 1 to the data and plot that instead."
    },
    {
        "text": "but in situations where the data can be arbitrarily negative, loga- rithmic plots are not appropriate."
    },
    {
        "text": "5.10 \u00adscatter matrices the biggest problem with scatterplots is that we often have many different variables to compare, and human visualization abilities top out at three dimen- sions."
    },
    {
        "text": "a partial solution to this is to do a scatterplot comparing every pair of features, arranging them in what\u2019s sometimes called a \u201cscatter matrix\u201d as shown here: 7.5 7.0 6.5 6.0 5.5 5.0 4.5 4.0 3.5 3.0 2.5 2.07 6 5 4 3 2 1 2.5 2.0 1.5 1.0 0.5 4.5 5.0 5.5 6.0 6.5 7.0 7.5 2.0 2.5 3.0 3.5 4.0 1 2 3 4 5 6 7 0.5 1.0 1.5 petal width (cm) petal width (cm) petal length (cm) petal length (cm) sepal length (cm) sepal length (cm) sepal width (cm) sepal width (cm) 2.0 2.5 note that along the diagonal, we have a histogram of each feature, rather than a scatterplot of the feature versus itself (which would just be a straight line)."
    },
    {
        "text": "5.11 heatmaps 77 the code that generated that visual is plt.close() from pandas.tools.plotting import scatter_matrix scatter_matrix(df) plt.show() 5.11 \u00adheatmaps another problem with scatterplots is that they can become visually cluttered if you have a lot of data points."
    },
    {
        "text": "you can ameliorate the problem by reducing the size of the data points (maybe a good idea anyway \u2013 default point sizes are often annoyingly big), but that only goes so far."
    },
    {
        "text": "it\u2019s an especially useless worka- round if many of your points are exactly on top of each other, which can easily happen if your data is integers rather than floats."
    },
    {
        "text": "eventually, your scatterplot becomes just a big mass of overlapping points with no background visible, and there is no way to tell which areas have more or fewer points."
    },
    {
        "text": "it is possible to use the alpha parameter to adjust the transparency of the points, so that they become darker when there is more overlap, but this becomes very clunky."
    },
    {
        "text": "in that situation, we don\u2019t actually care about the actual points themselves."
    },
    {
        "text": "we care about the density of points in the different regions, and the correct 7.5 7.0 6.5 6.0 5.5 5.0 4.5 2.0 2.5 3.0 sepal width (cm) sepal length (cm) 3.5 4.0 4.0 3.6 3.2 2.8 2.4 2.0 1.6 1.2 0.8 0.4 0.0"
    },
    {
        "text": "5 visualizations and simple metrics 78 way to visualize that is with a heatmap, which color\u2010codes different regions of the plane by their relative density of points."
    },
    {
        "text": "in some applications (including pandas), those regions are small hexagons, and they are called \u201chexbin\u201d heatmaps."
    },
    {
        "text": "the code for generating a heatmap, along with the resulting figure, is plt.close() df.plot(kind=\"hexbin\", x=\"sepal width (cm)\", y=\"sepal length (cm)\") plt.show() 5.12 \u00adcorrelations if you hear the term \u201ccorrelation\u201d used casually, it is probably what\u2019s called \u201cpearson\u201d correlation."
    },
    {
        "text": "more generally, a correlation is a metric that measures how closely tied two variables x and y are, and there are two main types you\u2019ll see: 1) pearson correlation."
    },
    {
        "text": "this is the normal one, and it measures how accurate it is to say that y mx b = + a correlation near 1 means that, for some b and some m>0, this equation is a good approximation."
    },
    {
        "text": "if the correlation is near \u20131, then it means the same thing, except m is negative."
    },
    {
        "text": "note that assuming a linear relationship is very restrictive."
    },
    {
        "text": "if y = sqrt[x], they still move up/down together exactly in sync, but they will have a correlation less than 1."
    },
    {
        "text": "2) ordinal correlations."
    },
    {
        "text": "this makes no assumption about x and y having a linear relationship."
    },
    {
        "text": "it just models their relationship as being monotonic: if you sort your data points by their x value (say, somebody\u2019s height), is that more or less the same order you get from sorting them by their y value (say, somebody\u2019s weight)?"
    },
    {
        "text": "do the taller people tend to be the heavier ones?"
    },
    {
        "text": "there are two main types of ordinal correlation that you\u2019ll see: spearman and kendall."
    },
    {
        "text": "the correlations in pandas can be simply calculated as follows: >>> df[\"sepal width (cm)\"].corr( df[\"sepal length (cm)\"]) # pearson corr -0.10936924995064937 >>> df[\"sepal width (cm)\"].corr( df[\"sepal length (cm)\"], method=\"pearson\") -0.10936924995064937"
    },
    {
        "text": "5.12 correlations 79 >>> df[\"sepal width (cm)\"].corr( df[\"sepal length (cm)\"], method=\"spearman\") -0.15945651848582867 >>> df[\"sepal width (cm)\"].corr( df[\"sepal length (cm)\"], method=\"spearman\") -0.072111919839430924 none of the correlations measure how related two variables are."
    },
    {
        "text": "if y = sin(x), for example, and x covers a wide range, then in some cases they go up together and in other cases they go down, and will have a correlation near 0. the best way to try and correct for this is to plot their relationship."
    },
    {
        "text": "the two ordinal correlations are similar to each other, and usually, either will work."
    },
    {
        "text": "in general though, kendall correlation is more robust to aberrant data points, for example, if the tallest person in a room was also the least heavy."
    },
    {
        "text": "conversely, kendall is very sensitive to small changes in ordering that are often inconsequential for a real application: if the tallest person in the room is only the second heaviest, kendall will punish that deviation much more severely compared to spearman."
    },
    {
        "text": "if i have to choose, i usually use spearman."
    },
    {
        "text": "i periodically get asked about how strong a correlation needs to be to be \u201cstrong enough.\u201d the answer to this depends entirely on context, and i can\u2019t give you any absolute rules."
    },
    {
        "text": "personally, i start to care when the absolute value of the correlation gets above 0.4. around 0.7, we\u2019re starting to talk about using one variable to defensibly estimate the other \u2013 there is a rigorous sense in which \u201chalf of the variation\u201d of the one variable can be explained by the other at this level of correlation."
    },
    {
        "text": "anything over 0.95 and i figure that one variable is basically a synonym for the other, and there was probably some weirdness in the data that caused this."
    },
    {
        "text": "finally, you\u2019ve probably heard a lot about how \u201ccorrelation is not causation.\u201d the gold standard in science is controlled experiments, where we forcibly change one (and only one) experimental parameter and then see how other things change."
    },
    {
        "text": "reliable controlled experiments are, strictly speaking, the only safe way to conclude that one thing causes another."
    },
    {
        "text": "but especially in areas such as sociology or economics, this is usually impossible, and if all we know is that two things are correlated in the real world, we cannot rigorously conclude any- thing about causality."
    },
    {
        "text": "if thing a and thing b are highly correlated, then humans have an almost pathological need to say that a causes b (or vice versa \u2013 whichever one sounds more plausible)."
    },
    {
        "text": "usually, neither is really true, and i would love for my personal contribution to the lore of statistics to be the following: cady\u2019s rule of thumb: if a and b are correlated, then neither one is causing the other."
    },
    {
        "text": "instead, there is some factor c causing them both."
    },
    {
        "text": "5 visualizations and simple metrics 80 if thing a and thing b are correlated, then try to figure out what c might be."
    },
    {
        "text": "this is a great way to generate hypotheses when you\u2019re doing exploratory analysis."
    },
    {
        "text": "of course, sometimes there is still causation \u2013 we just need to be very careful about inferring it."
    },
    {
        "text": "a somewhat touchy example of this is smoking and lung can- cer."
    },
    {
        "text": "all we really know is that smoking is correlated with getting cancer down the road."
    },
    {
        "text": "from a purely statistical perspective, it\u2019s possible that some people have an underlying lung condition that makes them susceptible to cancer and also makes them prone to nicotine addiction."
    },
    {
        "text": "in this case, we must leverage our knowledge of biology and medicine, which gives compelling mechanisms for how a causal relationship would work."
    },
    {
        "text": "it\u2019s not rigorous statistical certainty, but we can still reach a scientific conclusion."
    },
    {
        "text": "this is one of my favorite examples of how rigorous math can dovetail with common sense and domain expertise."
    },
    {
        "text": "5.13 \u00adanscombe\u2019s quartet and the limits of numbers i\u2019ve mentioned a number of times the limits of summary metrics and the fact that important features of a dataset can be masked by them."
    },
    {
        "text": "there is a famous madeup dataset called anscombe\u2019s quartet that illustrates this fact."
    },
    {
        "text": "12 14 10 8 6 4 2 12 14 10 8 6 4 2 12 14 10 8 6 4 2 12 14 10 8 6 4 2 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20 anscombe\u2019s quartet 2 4 6 8 10 12 14 16 18 20 2 4 6 8 10 12 14 16 18 20"
    },
    {
        "text": "5.14 time series 81 the following plot shows anscombe\u2019s four datasets as scatterplots, along with their lines of best fit."
    },
    {
        "text": "in each of the four datasets, x and y have exactly the same average and standard deviation."
    },
    {
        "text": "furthermore, there is the same correla- tion between x and y, and the lines of best fit are identical: the first two plots show very different relationship between x and y: linear but noisy, and nonlinear by very clean."
    },
    {
        "text": "the other plots show the massive effect that outliers can have, either to throw off your best\u2010fit parameters or to suggest that there is a \u201cfit\u201d when there really isn\u2019t one."
    },
    {
        "text": "5.14 \u00adtime series time series data is one of the most important data types in the world, cap- turing everything from stock prices, to website traffic, to blood glucose levels."
    },
    {
        "text": "as sensors become more ubiquitous, this importance will only grow."
    },
    {
        "text": "so, it\u2019s perhaps surprising that time series analysis is one of things that data scientists tend to be bad at: there is a rich set of time series techniques that are common practice in engineering, but that (largely for historical reasons, i think) just haven\u2019t really percolated into the data science com- munity."
    },
    {
        "text": "i\u2019ll talk more about the analysis of time series data in a later chap- ter, but for now i just want to go over a few points about visualization techniques."
    },
    {
        "text": "the first and most important thing is just how critical visualization is."
    },
    {
        "text": "anscombe\u2019s quartet shows that relying on summary statistics can be danger- ous, but reasonable bell curves are common enough in the real world that you can often get away with it."
    },
    {
        "text": "with time series though, there is absolutely no substitute for plotting."
    },
    {
        "text": "the pertinent pattern might end up being a sharp spike followed by a gentle taper down."
    },
    {
        "text": "or, maybe there are weird plateaus."
    },
    {
        "text": "there could be noisy spikes that have to be filtered out."
    },
    {
        "text": "a good way to look at it is this: means and standard deviations are based on the nai\u0308ve assump- tion that data follows pretty bell curves, but there is no corresponding \u201cdefault\u201d assumption for time series data (at least, not one that works well with any frequency), so you always have to look at the data to get a sense of what\u2019s normal."
    },
    {
        "text": "a simple example of a time series plot in python is here."
    },
    {
        "text": "unfortunately, there isn\u2019t a good example of time series data built\u2010in to scikit\u2010learn, so i\u2019m pulling one in from another library called statsmodels, which describes measurements of atmospheric co2 levels over many years."
    },
    {
        "text": "import statsmodels.api as sm dta = sm.datasets.co2.load_pandas().data dta.plot() plt.title(\"co2 levels\")"
    },
    {
        "text": "5 visualizations and simple metrics 82 plt.ylabel(\"parts per million\") plt.show() 380 370 360 350 co2 levels co2 parts per million 340 330 320 310 1960 1965 1970 1975 1980 1985 1990 1995 2000 in this dataset, the dataframe\u2019s index is set to a datetime type."
    },
    {
        "text": "in that case, pandas is smart enough to do some very user\u2010friendly formatting on the x\u2010axis."
    },
    {
        "text": "we can see in the plot that co2 fluctuates on a yearly cycle and that it increases overall across time."
    },
    {
        "text": "but there\u2019s no way we would have known to expect that a priori, unless we are already familiar with the science of co2."
    },
    {
        "text": "image data is the same way; there is a huge amount of information and no a priori knowledge of how to extract features out of it, but the patterns are glar- ingly obvious to the human eye."
    },
    {
        "text": "along the lines of figuring out what patterns to expect, when you are explor- ing time series data, it is immensely useful to be able to zoom in and out."
    },
    {
        "text": "i have often zoomed in on a sharp spike only to find that it\u2019s actually a short\u2010lived plateau."
    },
    {
        "text": "or, what looked like an immediate step down turned into an exponen- tial decay when i looked closer."
    },
    {
        "text": "in some cases, you might want to plot not just the data itself but the log of the data (or, alternatively, plot it on a logarithmic scale)."
    },
    {
        "text": "this makes sense with something such as the behavior of the price of a stock over a long period of time: a 10% uptick in the price will look equally impressive \u2013 which is good, since it\u2019s equally relevant to investment returns \u2013 whether the starting price was $1 or $20."
    },
    {
        "text": "5.14 time series 83 a good example is this code, which plots the price of google\u2019s stock since 2000 on normal and logarithmic axes: import urllib import matplotlib.pyplot as plt import pandas as pd import numpy as np # get raw csv data from the web url = (\"http://ichart.finance.yahoo.com/\" + \"table.csv?s=goog&c=2000\") dat = urllib.urlopen(url).read() open('foo.csv','w').write(dat) # make dataframe, w timestamp as the index df = pd.read_csv('foo.csv') df.index = df['date'].astype('datetime64') df['logclose'] = np.log(df['close']) df['close'].plot() plt.title(\"normal axis\") plt.show() df['close'].plot(logy=true) plt.title(\"logarithmic axis\") plt.show() from the normal plot, it looks like google had a massive surge in both 2005 and 2013. indeed it did, in absolute dollar terms."
    },
    {
        "text": "but the logarithmic plot makes it clear that the surge in 2005 was much more significant, because it was a much larger increase proportionally."
    },
    {
        "text": "another common situation is that you have many different time series and you are looking for some kind of shared pattern."
    },
    {
        "text": "plotting them all on the same chart (possibly after normalizing them, so they\u2019re all on a similar scale) is a useful way to do this."
    },
    {
        "text": "if there are so many that this becomes cluttered, what you can do is plot the median and quantiles; this makes it hard to see what is nor- mal for an individual series but gives a good sense of how they move as a whole."
    },
    {
        "text": "alternatively, you can plot the quantiles and overlay them with the plots of a manageably small sample of real time series."
    },
    {
        "text": "also, in many applications, it makes sense to look not just at the time series data itself but at transformations of it into the frequency domain."
    },
    {
        "text": "if you\u2019re measuring heart rate, temperature across days, or anything else where there is a reasonable expectation of periodicity, it can be immensely helpful to take a fourier transform (see the later chapter if you\u2019re not familiar with these) and break the signal down into its component frequencies."
    },
    {
        "text": "also, you should still keep tools such as histograms in mind."
    },
    {
        "text": "especially in data that is so noisy, it\u2019s hard to make sense of visually, and a histogram of the"
    },
    {
        "text": "5 visualizations and simple metrics 84 values can give you a useful summary of a time period."
    },
    {
        "text": "bear in mind that for many applications, the visualizations are only a means to an end, an inspiration for how we can extract meaningful features to plug into a machine learning model."
    },
    {
        "text": "some such as the median value over the time period are very reasonable features."
    },
    {
        "text": "1400 normal axis date logarithmic axis date 2006 2008 2010 2012 2014 2016 2006 2008 2010 2012 2014 2016 1200 1000 800 600 400 104 103 102 200 0"
    },
    {
        "text": "5.16 glossary 85 5.15 \u00adfurther reading 1 janert, p, data analysis with open source tools, 2010, o\u2019reilly media, newton, ma."
    },
    {
        "text": "2 pandas: python data analysis library, viewed 7 august 2016, http://pandas."
    },
    {
        "text": "pydata.org/."
    },
    {
        "text": "3 matplotlib 1.5.1 documentation, viewed 7 august 2016, http://matplotlib.org/."
    },
    {
        "text": "5.16 \u00adglossary kendall correlation an ordinal correlation metric that is reasonably robust to gross outliers but highly sensitive to tiny variations in rank."
    },
    {
        "text": "logarithmic plot a plot where one or both axes are scaled to the logarithm of the value they portray."
    },
    {
        "text": "this makes it easier to visualize very small and very large numbers on a single axis."
    },
    {
        "text": "nonparametric correlation a correlation metric that doesn\u2019t tacitly assume a specific form for the relationship between two variables."
    },
    {
        "text": "kendall and spearman correlations are examples, since they only assume that the relationship is monotonic."
    },
    {
        "text": "pearson correlation the usual definition of correlation."
    },
    {
        "text": "technically, corr[x,y] = cov[x,y]/(std[x] * std[y])."
    },
    {
        "text": "quantile the xth quantile is the value v such that a fraction x of your data points are \u2264v, and a fraction (1 \u2212 x) are equal to v. spearman correlation an ordinal correlation metric that is robust to small changes in ordering but can get thrown off badly by outliers."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 87 6 in my mind, machine learning is technically a subset of statistics."
    },
    {
        "text": "however, that\u2019s not how it might look from the outside."
    },
    {
        "text": "for historical reasons, machine learning has evolved largely independently from statistics, in some cases rein- venting the same techniques and giving them a different name, and in other cases inventing whole new ideas without statisticians supposedly involved."
    },
    {
        "text": "classical statistics grew largely out of the needs of governments in processing census data and the agriculture industry."
    },
    {
        "text": "machine learning evolved later and largely as an outgrowth of computer science."
    },
    {
        "text": "early computer scientists, in turn, were drawn from the ranks of physicists and engineers."
    },
    {
        "text": "so the dna is quite different, and the tools have diverged a lot, but ultimately they\u2019re tackling the same problems."
    },
    {
        "text": "\u201cmachine learning\u201d has become a catchall term that covers a lot of different areas, ranging from classification to clustering."
    },
    {
        "text": "as such, i can\u2019t really give you a crisp definition of what it means."
    },
    {
        "text": "however, there are several commonalities that pretty much all machine learning algorithms seem to work with: \u25cf \u25cfit\u2019s all done using computers, leveraging them to do calculations that would be intractable by hand."
    },
    {
        "text": "\u25cf \u25cfit takes data as input."
    },
    {
        "text": "if you are simulating a system based on some idealized model, then you aren\u2019t doing machine learning."
    },
    {
        "text": "\u25cf \u25cfthe data points are thought of as being samples from some underlying \u201creal\u2010 world\u201d probability distribution."
    },
    {
        "text": "\u25cf \u25cfthe data is tabular (or at least you can think of it that way)."
    },
    {
        "text": "there is one row per data point and one column per feature."
    },
    {
        "text": "the features are all numerical, binary, or categorical."
    },
    {
        "text": "the last of these properties is the real kicker."
    },
    {
        "text": "most machine learning algo- rithms are designed to handle pretty much any tabular dataset, but they only handle tabular data."
    },
    {
        "text": "tabular data lends itself to all kinds of mathematical analysis, since the rows of a table with n rows and d columns can be viewed as locations in machine learning overview"
    },
    {
        "text": "6 machine learning overview 88 d\u2010dimensional space."
    },
    {
        "text": "this is why machine learning is easily the most mathe- matically sophisticated thing a data scientist is likely to do."
    },
    {
        "text": "in most machine learning applications, the data points are thought of as being drawn from some underlying distribution, and the goal is to find patterns in the samples that tell us something about that distribution as a whole or that will let us process other samples from it."
    },
    {
        "text": "the next chapter, and several chapters in the next book part, will discuss several of the main areas of machine learning."
    },
    {
        "text": "the purpose of this short chap- ter is to give you some background about machine learning as a whole and some of the techniques that touch all parts of it."
    },
    {
        "text": "6.1\u00ad historical context machine learning was partly born out of the initial failures of the artificial intelligence (ai) movement."
    },
    {
        "text": "for a long time, people were very focused on the idea that computers could be made to think, and it was widely expected that thinking machines were only a few years away."
    },
    {
        "text": "there is an anecdote that marvin minsky, one of the founders of ai, once assigned a grad student the task of working out computer vision over the course of a summer."
    },
    {
        "text": "people were thinking about the human brain as a big logic engine, and a lot of the focus was on getting computers to mimic the logical processing that humans do."
    },
    {
        "text": "ai failed (at least relative to the hype it had generated), and it\u2019s partly out of embarrassment on behalf of their discipline that the term \u201cartificial intel- ligence\u201d is rarely used in computer science circles (although it\u2019s coming back into favor, just without the over-hyping)."
    },
    {
        "text": "we are as far away from mimicking human intelligence as we have ever been, partly because the human brain is fantastically more complicated than a mere logic engine."
    },
    {
        "text": "the focus has shifted away from creating true intelligence and toward using computers to do tasks that historically a human has to do."
    },
    {
        "text": "this includes things such as recognizing whether there is a bird in a photograph, telling whether an e\u2010mail is spam, or identifying that an \u201cinteresting event\u201d has occurred in a time series."
    },
    {
        "text": "machine learning was built up on using computers as proxies for human judgment in specific, limited situations."
    },
    {
        "text": "of course, the techniques thus devel- oped can be applied to many areas, even ones where human judgment is never applied in practice, so machine learning has matured into a standard toolset for any data scientist."
    },
    {
        "text": "the kinds of tools being used shifted as well."
    },
    {
        "text": "ai traditionally took a rule\u2010 based approach that used logical inference to reach conclusions."
    },
    {
        "text": "machine learning is much more probabilistic in the way it makes models and inferences."
    },
    {
        "text": "as a final note, some people might criticize this book for not going into nearly enough depth, especially with regard to cutting\u2010edge developments in"
    },
    {
        "text": "6.3 training data, testing data, and the great boogeyman of overfitting 89 areas such as deep learning."
    },
    {
        "text": "the reason for this is simple: in my experience, data scientists rarely get that far into the weeds."
    },
    {
        "text": "machine learning experts spend a lot of time improving their classifiers with all the latest tricks, but data scientists tend to use off\u2010the\u2010shelf classifiers, instead pouring their effort into finding good features to plug into them."
    },
    {
        "text": "6.2 supervised versus unsupervised there are two main types of machine learning, called supervised and unsupervised."
    },
    {
        "text": "in supervised learning, your training data consists of some points and a label or target value associated with them."
    },
    {
        "text": "the goal of the algorithms is to figure out some way to estimate that target value."
    },
    {
        "text": "for example, we might have data on several medical patients saying what was in a drop of their blood and then whether they were later found to have cancer."
    },
    {
        "text": "if we want to use blood samples from future patients to assess their cancer risk, this is a supervised learning problem."
    },
    {
        "text": "in unsupervised learning, there is just raw data, without any particular thing that is supposed to be predicted."
    },
    {
        "text": "unsupervised algorithms are used for finding patterns in the data in general, teasing apart its underlying structure."
    },
    {
        "text": "clustering algorithms, which try to break a dataset down into \u201cnatural\u201d clusters, are a prototypical example of unsupervised learning."
    },
    {
        "text": "supervised learning is somewhat more common in real applications."
    },
    {
        "text": "business situations usually dictate a specific thing that you are trying to pre- dict, rather than a broad \u201csee what there is to see\u201d approach."
    },
    {
        "text": "however, unsu- pervised learning algorithms are often used as a preprocessing step for extracting meaningful features from a data point, with those features ultimately getting used for supervised learning."
    },
    {
        "text": "6.3 training data, testing data, and the great boogeyman of overfitting by far the greatest headache in machine learning is the problem of overfit- ting."
    },
    {
        "text": "this means that your results look great for the data you trained them on, but they don\u2019t generalize to other data in the future."
    },
    {
        "text": "as an extreme case, imagine that your dataset of medical patients included their names and your trained classifier just remembered the names of everybody who had cancer and made predictions based on that."
    },
    {
        "text": "it would give perfect predictions for everybody it was trained on but would be useless for assessing anybody else\u2019s cancer risk."
    },
    {
        "text": "6 machine learning overview 90 the solution is to train on some of your data and assess performance on other data."
    },
    {
        "text": "this can be done in a number of ways: \u25cf \u25cfmost basically, you randomly divide your data points between training and testing."
    },
    {
        "text": "randomness is critically important, so as to avoid unintentional sources of bias (such as taking the first half of your data file as training data, when those rows might have been collected earlier)."
    },
    {
        "text": "honestly, this crudely simple approach is often good enough in practice."
    },
    {
        "text": "\u25cf \u25cfa fancier method that works specifically for supervised learning is called k\u2010 fold cross\u2010validation."
    },
    {
        "text": "the goal here isn\u2019t to measure the performance of a particular, fitted classifier, but rather a family of classifiers."
    },
    {
        "text": "cross\u2010validation is done this way: \u2013 \u2013 divide the data randomly into k partitions."
    },
    {
        "text": "\u2013 \u2013 train a classifier on all but one partition, and test its performance on the partition that was left out."
    },
    {
        "text": "\u2013 \u2013 repeat, but choosing a different partition to leave out and test on."
    },
    {
        "text": "continue for all the partitions, so that you have k different trained classi- fiers and k performance metrics for them."
    },
    {
        "text": "\u2013 \u2013 take the average of all the metrics."
    },
    {
        "text": "this is the best estimate of the \u201ctrue\u201d performance of this family of classifiers when it is trained on this kind of data."
    },
    {
        "text": "\u25cf \u25cfif you\u2019re being very rigorous about your statistics, it is common to divide your data into a training set, a testing set, and a validation set."
    },
    {
        "text": "you only get to examine the validation set at the very end to test your hypotheses and the performance of your model."
    },
    {
        "text": "this is done to avoid a very subtle form of statistical bias."
    },
    {
        "text": "let\u2019s say you only had testing/train- ing data, and you had several machine learning models to choose from."
    },
    {
        "text": "in this case, you would pick the one that performed best when you trained it on the one dataset and tested it on the other."
    },
    {
        "text": "but this is a weak form of training on the test data, because the test data influences your choice of model."
    },
    {
        "text": "the validation data then lets you put your trained model to the real test."
    },
    {
        "text": "\u25cf \u25cfi don\u2019t know a name for it, but there is another approach i\u2019ve used that is great in many real applications."
    },
    {
        "text": "oftentimes, there\u2019s a situation where a model is retrained periodically, say every week, incorporating the new data acquired in the previous week."
    },
    {
        "text": "in these cases, it makes sense to train on all the data for week n and the previous weeks and then test it on all the data for week n + 1. the reasons why people click on ads might have changed a little bit over the course of that week, making the model slightly outdated, so testing on data from the same time period can artificially inflate your performance."
    },
    {
        "text": "6.5 glossary 91 6.4\u00ad further reading 1 bishop, c, pattern recognition and machine learning, 2007, springer, new york, ny."
    },
    {
        "text": "2 scikit\u2010learn 0.171.1 documentation, http://scikit\u2010learn.org/stable/index.html, viewed 7 august 2016, the python software foundation."
    },
    {
        "text": "6.5 glossary artificial intelligence trying to mimic human\u2010like reasoning and behavior in a computer program."
    },
    {
        "text": "artificial intelligence fell from grace when the fields failed to live up to its hype."
    },
    {
        "text": "machine learning solves a lot of similar problems, but it does so using statistical techniques rather than rule\u2010based ones, and it usually makes no pretense of mimicking the human brain."
    },
    {
        "text": "machine learning a catchall term for several techniques that operate on tabular data."
    },
    {
        "text": "overfitting a machine learning model becoming so specially tuned to its exact input data that it fails to generalize to other, similar data."
    },
    {
        "text": "supervised learning machine learning where there is a specific target variable you are trying to predict per data point."
    },
    {
        "text": "tabular data a dataset that is arranged in rows and numerical columns."
    },
    {
        "text": "each row is associated with some entity, and each column gives some feature about all the entities."
    },
    {
        "text": "testing data data that is used to assess how well a machine learning model performs."
    },
    {
        "text": "it should not have been involved in the creation of that model."
    },
    {
        "text": "training data data that is used for training a machine learning model."
    },
    {
        "text": "the performance of the model should generally not be tested on the training data."
    },
    {
        "text": "unsupervised learning machine learning where there is not a specific target variable you are trying to predict."
    },
    {
        "text": "clustering is an example."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 93 7 before we jump into specific machine learning technique, i want to come back to feature extraction."
    },
    {
        "text": "a machine learning analysis will be only as good as the features that you plug into it."
    },
    {
        "text": "the best features are the ones that carefully reflect the thing you are studying, so you\u2019re likely going to have to bring a lot of domain expertise to your problems."
    },
    {
        "text": "however, i can give some of the \u201cusual suspects\u201d: classical ways to extract features from data that apply in a wide range of contexts and are at the very least worth taking a look at."
    },
    {
        "text": "this interlude will go over several of them and lead to some discussion about applying them in real contexts."
    },
    {
        "text": "7.1\u00ad standard features here are several types of feature extraction that are real classics, along with some of the real\u2010world considerations of using them: \u25cf \u25cfis_null: one of the simplest, and surprisingly effective, features is just whether the original data entry is missing."
    },
    {
        "text": "this is often because the entry is null for an important reason."
    },
    {
        "text": "for example, maybe some data wasn\u2019t gathered for widgets produced by a particular factory."
    },
    {
        "text": "or, with humans, maybe demographic data is missing because some demographic groups are less likely to report it."
    },
    {
        "text": "\u25cf \u25cfdummy variables: a categorical variable is one that can take on a finite num- ber of values."
    },
    {
        "text": "a column for a us state, for example, has 50 possible values."
    },
    {
        "text": "a dummy variable is a binary variable that says whether the categorical column is a particular value."
    },
    {
        "text": "then you might have a binary column that says whether or not a state is washington, another column that says whether it is texas, and so on."
    },
    {
        "text": "this is also called one\u2010hot encoding, because every row in your dataset will have 1 in exactly one of the dummy variables for the states."
    },
    {
        "text": "there are two big issues to consider when using dummy variables: a) you might have a lot of categories, some of which are very rare."
    },
    {
        "text": "in this case, it\u2019s typical to pick some threshold and only have dummy variables interlude: feature extraction ideas"
    },
    {
        "text": "7 interlude: feature extraction ideas 94 for the more common values, then have another dummy variable that will be 1 for anything else."
    },
    {
        "text": "b) often, you only learn what the possible values are by looking at training data, and then you will have to extract the same dummy features from other data (maybe testing data) later on."
    },
    {
        "text": "in that case, you will have to have some protocol for dealing with entries that were not present in the train- ing data."
    },
    {
        "text": "\u25cf \u25cfranks: a blunt\u2010force way to correct for outliers in a column of data is to sort the values and instead use their ordinal ranks."
    },
    {
        "text": "there are two big problems with this: a) it\u2019s an expensive computation since the whole list must be sorted, and it cannot be done in parallel if your data is distributed across a cluster."
    },
    {
        "text": "b) ranks are a huge problem when it comes to testing/training data."
    },
    {
        "text": "if you rank all your points before dividing into training/testing, then informa- tion about the testing data will be implicit in the training data: a huge no\u2010no."
    },
    {
        "text": "a workaround is to give each testing data point the rank that it would have had in the training data, but this is computationally expensive."
    },
    {
        "text": "\u25cf \u25cfbinning: both of the problems associated with ranks can be addressed by choosing several histogram bins of roughly equal size that your data can be put into."
    },
    {
        "text": "you might have a bin for anything below the 25th percentile, another for the 25th to the 50th percentile, and so on."
    },
    {
        "text": "then rather than a percentile rank for your data points, just say which bin they fall into."
    },
    {
        "text": "the downside is that this takes away the fine resolution that you get with per- centile ranks."
    },
    {
        "text": "\u25cf \u25cflogarithms: it is common to take the logarithm of a raw number and use that as a feature."
    },
    {
        "text": "it dampens down large outliers and increases the prominence of small values."
    },
    {
        "text": "if your data contains any 0s, it\u2019s common to add 1 before taking the log."
    },
    {
        "text": "7.2 \u00adfeatures that involve grouping oftentimes, a dataset will include multiple rows for a single entity that we are describing."
    },
    {
        "text": "for example, our dataset might have one row per transaction and a column that says the customer we had the transaction with, but we are trying to extract features about the customers."
    },
    {
        "text": "in these cases, we have to aggregate the various rows for a given customer in some way."
    },
    {
        "text": "several brute\u2010force aggre- gate metrics you use could include the following: \u25cf \u25cfthe number of rows \u25cf \u25cfthe average, min, max, mean, median, and so on, for a particular column"
    },
    {
        "text": "95 7.4 defining the feature you want to predict \u25cf \u25cfif a column is nonnumerical, the number of distinct entries that it contains \u25cf \u25cfif a column is nonnumerical, the number of entries that were identical to the most common entry \u25cf \u25cfthe correlation between two different columns."
    },
    {
        "text": "7.3\u00ad preview of more sophisticated features many of the more advanced chapters in this book will talk about fancy meth- ods for feature extraction."
    },
    {
        "text": "here is a quick list of some of the very interesting ones: \u25cf \u25cfif your data point is an image, you can extract some measure of the degree to which it resembles some other image."
    },
    {
        "text": "the classical way to do this is called principal component analysis (pca)."
    },
    {
        "text": "it also works for numerical arrays of time series data or sensor measurements."
    },
    {
        "text": "\u25cf \u25cfyou can cluster your data and use the cluster of each point as a categorical feature."
    },
    {
        "text": "\u25cf \u25cfif the data is text, you can extract the frequency of each word."
    },
    {
        "text": "the problem with this is that it often gives you prohibitively many features, and some additional method may be required to condense them down."
    },
    {
        "text": "7.4\u00ad defining the feature you want to predict finally, it\u2019s worth noting that you may find yourself extracting the most impor- tant feature of all: the feature that you\u2019re using machine learning to try and predict."
    },
    {
        "text": "to show you how this might work, here are several examples from my own career: \u25cf \u25cfi had to predict human traffic to a collection of websites."
    },
    {
        "text": "however, the traffic logs were polluted by bots, and it became a separate problem to filter out the bot traffic and then estimate how well our cleaned traffic corresponded to flesh\u2010and\u2010blood humans."
    },
    {
        "text": "we did this by comparing our traffic estimate to those from google for a few select sites \u2013 sometimes we were over, and sometimes we were under, but we concluded that it was a good enough match to move forward with the project."
    },
    {
        "text": "\u25cf \u25cfi have studied customer \u201cchurn,\u201d that is, when customers take their busi- ness elsewhere."
    },
    {
        "text": "intuitively, the \u201cground truth\u201d is a feeling of loyalty that exists in the minds of customers, and you have to figure out how to gauge that based on their purchasing behavior."
    },
    {
        "text": "it\u2019s hard to distinguish between churn and the customer just not needing your services for a time period."
    },
    {
        "text": "7 interlude: feature extraction ideas 96 \u25cf \u25cfwhen you are trying to predict events based on time series data, you often have to predict whether or not an event is imminent at some point in time."
    },
    {
        "text": "this requires deciding how far in the future an event can be before it is con- sidered \u201cimminent.\u201d alternatively, you can have a continuous\u2010valued number that says how long until the next event, possibly having it top out at some maximum value so as to avoid outliers (or you could take the logarithm of the time until the next event \u2013 that would dampen outliers too)."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 97 8 machine learning classifiers are a critically important part of the data science toolkit."
    },
    {
        "text": "however, they are not nearly as important as they are made out to be."
    },
    {
        "text": "a large part of the mystique of data science comes from the idea that we can pour data into a magical black box that (through some mathematical voodoo that only data scientists are smart enough to understand) can learn everything about the data and solve business problems."
    },
    {
        "text": "the reality is a lot more mundane."
    },
    {
        "text": "as we\u2019ve discussed previously, it takes a lot of work to get the data into a form where it can be fed into the black box, a lot of savvy to point the black box at the right question, and additional work to make sense of the results."
    },
    {
        "text": "the machine learning black box itself is usually just a library that you call."
    },
    {
        "text": "sure, it\u2019s good to have some idea of how the classifiers work under the hood \u2013 you can pick better ones to use, avoid common pitfalls, make better sense of their output, and understand how to jury\u2010rig them as need be."
    },
    {
        "text": "but training a plain\u2010vanilla classifier is often construed as being rocket science, and it\u2019s not."
    },
    {
        "text": "this chapter comes in two sections."
    },
    {
        "text": "after some initial notes, the first will be a series of rapid\u2010fire tutorials about some of the most useful classifiers."
    },
    {
        "text": "the second section will discuss the various ways that we can grade their accuracy."
    },
    {
        "text": "8.1 \u00adwhat is a classifier, and what can you do with it?"
    },
    {
        "text": "a machine learning classifier is a computational object that has two stages: \u25cf \u25cfit gets \u201ctrained.\u201d it takes in its training data, which is a bunch of data points and the correct label associated with them, and tries to learn some pattern for how the points map to the labels."
    },
    {
        "text": "\u25cf \u25cfonce it has been trained, the classifier acts as a function that takes in addi- tional data points and outputs predicted classifications for them."
    },
    {
        "text": "sometimes, machine learning classification"
    },
    {
        "text": "8 machine learning classification 98 the prediction will be a specific label; other times, it will give a continuous\u2010 valued number that can be seen as a confidence score for a particular label."
    },
    {
        "text": "there are two big use cases for classifiers."
    },
    {
        "text": "the first is the obvious one; we have things that need to be classified."
    },
    {
        "text": "this happens all the time in production code situations, where a computer has to decide, say, which ad to show a user."
    },
    {
        "text": "it also happens when computers aren\u2019t making decisions autonomously, but instead flagging things for a flesh\u2010and\u2010blood human to look at: flagging poten- tial instances of credit card fraud, for example."
    },
    {
        "text": "the other use for classifiers is to give insights about the underlying data."
    },
    {
        "text": "in my own career, this has actually been the more common use case."
    },
    {
        "text": "my clients have not been so interested in predicting, say, that a particular machine will fail."
    },
    {
        "text": "what they really want to know is the patterns in the data that predict fail- ures, because those patterns can help them diagnose and fix something that\u2019s going wrong on their assembly line."
    },
    {
        "text": "in cases such as this, we want to dissect our classifiers after the fact, extracting business insights."
    },
    {
        "text": "this becomes an interesting balancing act for a data scientist; sometimes, the most accurate classifiers are the hardest to make real\u2010world sense of."
    },
    {
        "text": "8.2 \u00ada few practical concerns the whole notion of machine learning classification is premised on the idea of having correctly labeled training data and in sufficient quantities to train our classifier."
    },
    {
        "text": "however, this is a luxury that the real world often doesn\u2019t afford."
    },
    {
        "text": "for example, in fraud detection, you will probably have a modest\u2010size set of hand\u2010 labeled fraud cases and a huge mass of unlabeled data."
    },
    {
        "text": "you just presume that those unlabeled points are nonfraudulent, which means that an unknown frac- tion of your training data is mislabeled."
    },
    {
        "text": "it\u2019s not like the hand\u2010labeled fraud cases are a nice random sample from all fraud cases; they represent whatever kind of fraud people have been looking for so far."
    },
    {
        "text": "there could easily be whole new categories of fraud, every instance of which is labeled as nonfraud in your training data."
    },
    {
        "text": "what even is \u201cfraud\u201d anyway?"
    },
    {
        "text": "in many cases, you have to come up with the training data yourself, and it\u2019s not a priori clear how things should be labeled."
    },
    {
        "text": "an e\u2010mail about nigerian princes is almost certainly fraud, but what about somebody selling \u201cdiscont vi@gra?\u201d in cases where i\u2019m looking for cool patterns, rather than the classifiers them- selves, what i will often do is remove edge cases from my training data."
    },
    {
        "text": "for example, i once had a client who was trying to understand \u201ccustomer loyalty,\u201d and i was writing a classifier for whether or not they would lose a customer within the next year."
    },
    {
        "text": "the problem is that customers don\u2019t announce they\u2019re leaving \u2013 they just stop using my client\u2019s services, which most customers don\u2019t"
    },
    {
        "text": "8.4 example script 99 use all that often anyway."
    },
    {
        "text": "what i did was to formulate criteria for customers being \u201cdefinitely still loyal,\u201d and another for them being \"definitely not loyal.\""
    },
    {
        "text": "every \"gray area\" customer, who didn\u2019t fall into one of those categories, was discarded before training \u2013 they made up around a third of customers."
    },
    {
        "text": "the resulting classifier worked surprisingly well."
    },
    {
        "text": "but the really exciting part was that when we applied it to the gray area customers, those that were flagged as higher\u2010risk did indeed come closer to satisfying the \"not loyal\" criteria com- pared to the loyal criteria."
    },
    {
        "text": "this suggested that (not surprisingly, but reassuring to know) the gray area customers were more\u2010 or less\u2010extreme versions of what the other customers were doing, rather than some fundamentally new category of people."
    },
    {
        "text": "this question of defining ground truth is beside the perennial problem of data science: feature extraction."
    },
    {
        "text": "contrary to popular myth, any machine learn- ing classifier will suck if you give it features that don\u2019t contain signal or features for which the signal is deeply buried in idiosyncrasies and convoluted depend- encies."
    },
    {
        "text": "a large portion of data science boils down to understanding the dataset and the domain of application well enough that you can extract meaningful features."
    },
    {
        "text": "8.3 \u00adbinary versus multiclass most classification problems have a binary classification: 1 or 0, yes or no."
    },
    {
        "text": "however, oftentimes, the label is a categorical variable, capable of taking on several values."
    },
    {
        "text": "there are some classifier algorithms that handle this situation natively, but many others are strictly binary."
    },
    {
        "text": "when you are using a classifier that is binary but solving a problem that has k possibly labels, the standard solution is to actually train k different classifiers: one for each label x, classify- ing points as being x or something else."
    },
    {
        "text": "for the most part, these distinctions are wrapped up within machine learn- ing libraries and invisible to data scientists who use the libraries."
    },
    {
        "text": "explanations in this chapter will freely assume that classifiers are all binary."
    },
    {
        "text": "8.4 \u00adexample script the following script demonstrates many of the topics we will cover in this chapter in a realistic setting, using the sample iris dataset from the last chapter."
    },
    {
        "text": "it takes several important classifiers, trains each one to distinguish iris virgi- nica from the other species, and then plots the results on an roc curve (i\u2019ll explain these shortly \u2013 they\u2019re a tool for visualizing how well a classifier works)."
    },
    {
        "text": "each of these classifiers, and the metrics we use to evaluate them, will be explained later in the chapter."
    },
    {
        "text": "8 machine learning classification 100 from matplotlib import pyplot as plt import sklearn from sklearn.metrics import roc_curve, auc from sklearn.cross_validation import train_test_split from sklearn.linear_model import logisticregression from sklearn.tree import decisiontreeclassifier from sklearn.ensemble import randomforestclassifier from sklearn.naive_bayes import gaussiannb # name -> (line format, classifier) class_map = { 'logisticregression': ('-', logisticregression()), 'naive bayes': ('--', gaussiannb()), 'decision tree': ('.-', decisiontreeclassifier(max_depth=5)), 'random forest': (':', randomforestclassifier( max_depth=5, n_estimators=10, max_features=1)), } # divide cols by independent/dependent, rows by test/ train x, y = df[df.columns[:3]], (df['species']=='virginica') x_train, x_test, y_train, y_test = \\ train_test_split(x, y, test_size=.8) for name, (line_fmt, model) in class_map.items(): model.fit(x_train, y_train) # array w one col per label preds = model.predict_proba(x_test) pred = pd.series(preds[:,1]) fpr, tpr, thresholds = roc_curve(y_test, pred) auc_score = auc(fpr, tpr) label='%s: auc=%f' % (name, auc_score) plt.plot(fpr, tpr, line_fmt, linewidth=5, label=label) plt.legend(loc=\"lower right\") plt.title('comparing classifiers')"
    },
    {
        "text": "8.5 specific classifiers 101 plt.plot([0, 1], [0, 1], 'k--') #x=y line."
    },
    {
        "text": "visual aid plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('false positive rate') plt.ylabel('true positive rate') plt.show() 1.0 0.8 0.6 0.4 true positive rate comparing classifers 0.2 0.0 0.0 0.2 0.4 0.6 false positive rate logistic regression: auc = 0.988837 naive bayes: auc = 0.896886 random forest: auc = 0.901146 decision tree: auc = 0.830200 0.8 1.0 8.5 \u00adspecific classifiers the world is full of different classification algorithms."
    },
    {
        "text": "this section will go over some of the most useful and important ones."
    },
    {
        "text": "8.5.1 decision trees a decision tree is conceptually one of the simplest classifiers available."
    },
    {
        "text": "using a decision tree to classify a data point is the equivalent of following a basic flow- chart."
    },
    {
        "text": "it consists of a tree structure such as the following:"
    },
    {
        "text": "8 machine learning classification 102 age > 18?"
    },
    {
        "text": "height < 4?"
    },
    {
        "text": "no no yes female yes gender male scores: scores: scores: scores: 0: 0.3 1: 0.7 0: 0.5 1: 0.5 0: 0.9 1: 0.1 0: 0.8 1: 0.2 every node in the tree asks a question about one feature of a data point."
    },
    {
        "text": "if the feature is numerical, the node asks whether it is above or below a threshold, and there are child nodes for \"yes\" and \"no.\""
    },
    {
        "text": "if the feature is categorical, typically there will be a different child node for every value it can take."
    },
    {
        "text": "a leaf node in the tree will be the score that is assigned to the point being classified (or several scores, one for each possible thing the point could be flagged as)."
    },
    {
        "text": "it doesn\u2019t get much simpler than that."
    },
    {
        "text": "using a decision tree is conceptually quite straightforward, but training one is another matter entirely."
    },
    {
        "text": "in general, finding the optimal decision for your training data is computationally intractable, so in practice, you train the tree with a series of heuristics and hope that the result is close to the best tree pos- sible."
    },
    {
        "text": "generally, the algorithm is something along these lines: 1) given your training data x, find the single feature (and cutoff for that fea- ture, if it\u2019s numerical) that best partitions your data into classes."
    },
    {
        "text": "2) there are a variety of ways to quantify how good a partition is."
    },
    {
        "text": "the most common ones are the \"information gain\" and the \"gini impurity.\""
    },
    {
        "text": "i won\u2019t delve into their precise meanings here."
    },
    {
        "text": "3) this single best feature/cutoff becomes the root of your decision tree."
    },
    {
        "text": "partition x up according to this node."
    },
    {
        "text": "4) recursively train each of the child nodes on its partition of the data."
    },
    {
        "text": "5) the recursion stops when either all of the data points in your partition have the same label or the recursion has gone to a predetermined maximum depth."
    },
    {
        "text": "at that point, the scores stored in this node will just be the break- downs of the labels in the partition."
    },
    {
        "text": "you will probably never need to worry about the details of how to train a deci- sion tree, but knowing the basic process helps to understand one of the biggest problems with this classifier: overfitting."
    },
    {
        "text": "if you set the maximum depth too far, for example, every leaf node will end up with a partition that contains only a few points, all of which have the same label."
    },
    {
        "text": "this will result in the decision tree"
    },
    {
        "text": "8.5 specific classifiers 103 consistently giving out extremely confident scores that, realistically, are just an accident of small numbers."
    },
    {
        "text": "you can set parameters on your decision tree that force it to terminate when, say, the best partitions on a node are too small."
    },
    {
        "text": "but in many libraries, the default settings will let a decision tree drastically overfit itself, so you should be aware of this and tune your trees accordingly."
    },
    {
        "text": "decision trees are very easy to understand, so it\u2019s perhaps a bit surprising that they are often difficult to tease real\u2010world insights out of."
    },
    {
        "text": "looking at the top few layers is certainly interesting and suggests what some of the more important features are."
    },
    {
        "text": "but it\u2019s not always clear what the features and their cutoffs mean to the real world, unless you want to wade into the deep waters of dissecting the gini impurities from the training stage."
    },
    {
        "text": "even if you do this, there is still a very real risk that the same feature will weigh toward hits at one node of the tree and toward nonhits at another node."
    },
    {
        "text": "what the heck does that mean?"
    },
    {
        "text": "personally, i don\u2019t use decision trees much for serious work."
    },
    {
        "text": "however, they are extremely useful for their human readability \u2013 this is especially handy if you\u2019re working with people who don\u2019t know machine learning and are wary of black boxes \u2013 and the rapidity with which they can do classifications."
    },
    {
        "text": "above all, decision trees are useful as a building block for constructing random forest classifiers, which i\u2019ll discuss in the next section."
    },
    {
        "text": "the following code shows how to train and use a decision tree in python: from sklearn.tree import decisiontreeclassifier clf = decisiontreeclassifier(max_depth=5) clf.fit(train[indep_cols], train.breed) predictions = clf.predict(test[indep_cols]) 8.5.2 random forests if i were stuck on a desert island and could only take one classifier with me, it would be the random forest."
    },
    {
        "text": "they are consistently one of the most accurate, robust classifiers out there, legendary for taking in datasets with a dizzying number of features, none of which are very informative and none of which have been cleaned, and somehow churning out results that beat the pants off of anything else."
    },
    {
        "text": "the basic idea is almost too simple."
    },
    {
        "text": "a random forest is a collection of deci- sion trees, each of which is trained on a random subset of the training data and only allowed to use some random subset of the features."
    },
    {
        "text": "there is no coordina- tion in the randomization \u2013 a particular data point or feature could randomly get plugged into all the trees, none of the trees, or anything in between."
    },
    {
        "text": "the final classification score for a point is the average of the scores from all the trees (or sometimes, you treat the decision trees as binary classifiers and report the fraction of all of them that votes a certain way)."
    },
    {
        "text": "8 machine learning classification 104 the hope is that the different trees will pick up on different salient patterns, and each one will only give confident guesses when its pattern is present."
    },
    {
        "text": "that way, when it comes time to classify a point, several of the trees will classify it correctly and strongly while the other trees give answers that are on the fence, meaning that the overall classifier slouches toward the right answer."
    },
    {
        "text": "the individual trees in a random forest are subject to overfitting, but they tend to be randomly overfitted in different ways."
    },
    {
        "text": "these largely cancel each other out, yielding a robust classifier."
    },
    {
        "text": "the problem with random forests is that they\u2019re impossible to make real business sense of."
    },
    {
        "text": "the whole point of a classifier such as this is that it is too complex for human comprehension, and its performance is an averaged\u2010out thing."
    },
    {
        "text": "the one thing that you can do with a random forest is to get a \"feature importance\" score for any feature in the dataset."
    },
    {
        "text": "these scores are opaque and impossible to ascribe a specific real\u2010world meaning to."
    },
    {
        "text": "the importance of the kth feature is calculated by randomly swapping the kth feature around between the points in the training data and then looking at how much randomizing this feature hurts performance (there is a little bit of extra logic to make sure that no randomized data point is fed into a tree that was trained on its nonrand- omized version)."
    },
    {
        "text": "in practice, you can often take this list of features and, with a little bit of old\u2010fashioned data analysis, figure out compelling real\u2010world interpretations of what they mean."
    },
    {
        "text": "but the random forest itself tells you nothing."
    },
    {
        "text": "the following code shows how to train and use a random forest in python: from sklearn.tree import randomforestclassifier clf = randomforestclassifier( max_depth=5, n_estimators=10, max_features=1)) clf.fit(train[indep_cols], train.breed) predictions = clf.predict(test[indep_cols]) 8.5.3 ensemble classifiers random forests are the best\u2010known example of what are called \"ensemble clas- sifiers,\" where a wide range of classifiers (decision trees, in this case) are trained under randomly different conditions (in our case, random selections of data points and features) and their results are aggregated."
    },
    {
        "text": "intuitively, the idea is that if every classifier is at least marginally good, and the different classifiers are not very correlated with each other, then the ensemble as a whole will very reliably slouch toward the right classification."
    },
    {
        "text": "basically, it\u2019s using raw computational power in lieu of domain knowledge or mathematical sophistication, relying on the power of the law of large numbers."
    },
    {
        "text": "8.5 specific classifiers 105 8.5.4 support vector machines i\u2019ll be honest: i personally hate support vector machines (svms)."
    },
    {
        "text": "they\u2019re one of the most famous machine learning classifiers out there, so it\u2019s important that you be familiar with them; but i have several gripes."
    },
    {
        "text": "first off, they make a very strong assumption about the data called linear separability."
    },
    {
        "text": "oftentimes that assumption is wrong, and occasionally it\u2019s right in mathematically perverse ways."
    },
    {
        "text": "there are sometimes hacks that work around this assumption, but there\u2019s no principle behind them and no a priori way of knowing which (if any) hack will work in a particular situation."
    },
    {
        "text": "svms are also one of the few classifiers that are fundamentally binary; they don\u2019t give continuous\u2010valued \"scores\" that can be used to assess how confident the classifier is."
    },
    {
        "text": "this makes them annoying if you\u2019re looking for business insights and unusable if you need to have the notion of a \"gray area.\""
    },
    {
        "text": "that said, they\u2019re popular for a reason."
    },
    {
        "text": "they are intuitively simple, mathe- matically elegant, and trivial to use."
    },
    {
        "text": "plus, those unprincipled hacks i mentioned earlier can be incredibly powerful if you pick the right one."
    },
    {
        "text": "the key idea of an svm is illustrated in the following figure: margin hyperplane essentially, you view every data point as a point in d\u2010dimensional space and then look for a hyperplane that separates the two classes."
    },
    {
        "text": "the assumption that there actually is such a hyperplane is called linear separability."
    },
    {
        "text": "training the svm involves finding the hyperplane that (1) separates the datasets and (2) is \"in the middle\" of the gap between the two classes."
    },
    {
        "text": "specifically, the \"margin\" of a hyperplane is min(its distance to the nearest point in class a, its distance to the nearest point in class b), and you pick the hyperplane that maximizes the margin."
    },
    {
        "text": "mathematically, the hyperplane is specified by the equation"
    },
    {
        "text": "8 machine learning classification 106 f x w x b ( ) = \u22c5 + = 0 where w is a vector perpendicular to the hyperplane and b measures how far offset it is from the origin."
    },
    {
        "text": "to classify a point x, simply calculate f(x) and see whether it is positive or negative."
    },
    {
        "text": "training the classifier consists of finding the w and b that separates the dataset while having the largest margin."
    },
    {
        "text": "this version is called \u201chard margin\u201d svm."
    },
    {
        "text": "however, in practice, there often is no hyperplane that completely separates the two classes in the training data."
    },
    {
        "text": "intuitively, what we want to do is find the best hyperplane that almost sepa- rates the data, by penalizing any points that are on the wrong side of hyper- plane."
    },
    {
        "text": "this is done using \u201csoft margin\u201d svm."
    },
    {
        "text": "the other killer problem with svm is if you have as many features as data points."
    },
    {
        "text": "in this case, there is guaranteed to be a separating hyperplane, regard- less of how the points are labeled."
    },
    {
        "text": "this is one of the curses of working in high\u2010dimensional space."
    },
    {
        "text": "you can do dimensionality reduction (which i will discuss in a later chapter) as a preprocessing step, but if you just plug high\u2010 dimensional data into an svm, it is almost guaranteed to be grotesquely overfitted."
    },
    {
        "text": "the most notorious problem with a plain svm is the linear separability assumption."
    },
    {
        "text": "an svm will fail utterly on a dataset as the following: y x \u03b8 there is no line between the two classes of points."
    },
    {
        "text": "the pattern is clear if you just look at it \u2013 one class is near the origin, and the other is far from it \u2013 but an svm can\u2019t tell."
    },
    {
        "text": "the solution to this problem is a very powerful generalization of svm called \u201ckernel svm.\u201d the idea of kernel svm is to first map our points into some other space in which the decision boundary is linear, and then"
    },
    {
        "text": "8.5 specific classifiers 107 construct a support vector machine that operates in that space."
    },
    {
        "text": "for the previ- ous figure, if we plot the distance from the origin on the x\u2010axis and the angle \u03b8 on the y\u2010axis, we get the following figure: angle \u03b8 radius the data here is linearly separable."
    },
    {
        "text": "in general, kernel svm requires finding some function \u03c6 that maps our points in d\u2010dimensional space to points in some n\u2010dimensional space."
    },
    {
        "text": "in the example i gave, n and d were both 2, but in practice, we usually want n to be larger than d, to increase the chances of linear separability."
    },
    {
        "text": "if you can find \u03c6, then you\u2019re golden."
    },
    {
        "text": "now here\u2019s the key point computationally: you never need to find \u03c6 itself."
    },
    {
        "text": "when you crank through the math, it turns out that whenever you calculate \u03c6(x), it is always part of a larger expression called the kernel function: k x y x y ( ) ( ) ( ) , = \u22c5 \u03c6 \u03c6 the kernel function takes two points in the original space and gives their dot product in the mapped space."
    },
    {
        "text": "this means that the mapping function \u03c6 is just an abstraction \u2013 we never need to calculate it directly and can instead just focus on k. in many cases, it turns out that calculating k directly is much, much easier than calculating any \u03c6(x) intermediates."
    },
    {
        "text": "it is often the case that \u03c6 is an intricate mapping into a massively high\u2010dimensional space, or even an infinite\u2010dimensional space, but the expression for k reduces to some simple, tractable function that is nonlinear."
    },
    {
        "text": "using only the kernel function in this way is called the \"kernel trick,\" and it ends up applying to areas outside of svms."
    },
    {
        "text": "8 machine learning classification 108 not every function that takes in two vectors is a valid kernel, but an awful lot of them are."
    },
    {
        "text": "some of the most popular, which are typically built into libraries, are as follows: \u25cf \u25cfpolynomial kernel: k x y x y c n ( ) ( ) , = \u22c5 + \u25cf \u25cfgaussian kernel: k x y x y ( ) exp[ | | ] , = \u2212 \u2212 \u03b3 2 \u25cf \u25cfsigmoid: k x y x y r , ( ) = \u22c5 + ( ) tanh ."
    },
    {
        "text": "most kernel svm frameworks will let users define their own functions as well."
    },
    {
        "text": "if you take this route, you should be aware that it\u2019s a bit technical to make sure that k is a valid kernel function, that is, that it has a corresponding map- ping \u03c6. most simply k has to be symmetric: k(x,y) = k( y,x) for any x and y. the major constraint though is that it be \"positive definite.\""
    },
    {
        "text": "this is a highly techni- cal constraint that i won\u2019t get into here."
    },
    {
        "text": "8.5.5 logistic regression logistic regression is a great general\u2010purpose classifier, striking an excellent balance between accurate classifications and real\u2010world interpretability."
    },
    {
        "text": "i think of it as kind of a nonbinary version of svm, one that scores points with prob- abilities based on how far they are from the hyperplane, rather than using that hyperplane as a definitive cutoff."
    },
    {
        "text": "if the training data is almost linearly separa- ble, then all points that aren\u2019t near the hyperplane will get a confident predic- tion near 0 or 1. but if the two classes bleed over the hyperplane a lot, the predictions will be more muted, and only points far from the hyperplane will get confident scores."
    },
    {
        "text": "in logistic regression, the score for a point will be p x w x b ( ) = + \u22c5 + [ ] 1 1 exp note that exp[ ] w x b \u22c5 + is the same f(x) we saw in svm, where w is a vector that gives weights to each feature and b is a real\u2010valued offset."
    },
    {
        "text": "with svm we look at whether f(x) is positive or negative, but in this case we plug it into the so\u2010called \"sigmoid function\": \u03c3( ) exp[ ] z z = + 1 1 as with svm, we have a dividing hyperplane defined by w x b \u22c5 + = 0 . in svm, that hyperplane is the binary decision boundary, but in this case, it is the hyperplane along which p x ( ) = 1 2 . the sigmoid function shows up a few places in machine learning, so it makes sense to dwell on it a bit."
    },
    {
        "text": "if you plot out \u03c3(x), it appears as follows:"
    },
    {
        "text": "8.5 specific classifiers 109 1.0 0.8 0.6 0.4 0.2 0.0 \u22126 \u22124 \u22122 0 2 4 6 sigmoid function you can see that \u03c3(0) is 0.5. as the argument blows up to infinity, it approaches 1.0, and as it goes to negative infinity, it goes 0.0. intuitively, this makes it a great way to take \u201cconfidence weights\u201d and cram them down into the interval (0, 1.0) where they can be treated as probabilities."
    },
    {
        "text": "the sigmoid function also has a lot of convenient mathematical properties that make it easy to work with."
    },
    {
        "text": "we will see it again in the section on neural networks."
    },
    {
        "text": "pulling real\u2010world meaning out of a trained logistic regression model is easy: \u25cf \u25cfif the kth component of w is large and positive, then the kth feature being big suggests that the correct label is 1."
    },
    {
        "text": "\u25cf \u25cfif the kth component of w is large and negative, then the kth feature being big suggests that the correct label is 0."
    },
    {
        "text": "\u25cf \u25cfthe larger the elements of w are in general, the tighter our decision bound- ary and the more closely we approach an svm."
    },
    {
        "text": "note though that in order for this to be meaningful, you must make sure that your data is all set to the same scale before training; if the most important fea- ture also happens to be the largest number, then its coefficient would be mis- leadingly small."
    },
    {
        "text": "another perk of logistic regression is that it\u2019s extremely efficient to store and use."
    },
    {
        "text": "the entire model consists of just d + 1 floating point numbers, for the d components of the weight vector and the offset b. performing a classification"
    },
    {
        "text": "8 machine learning classification 110 requires just d multiplication operations, d addition operations, and one com- putation of a sigmoid function."
    },
    {
        "text": "this code will train and use a logistic regression model: from sklearn import linear_model clf = linear_model.logisticregression() clf.fit(train_data, train_labels) predictions = clf.predict(test_data) 8.5.6 lasso regression lasso regression is a variant of logistic regression."
    },
    {
        "text": "one of the problems with logistic regression is that you can have many different features all with modest weights, instead of a few clearly meaningful features with large weights."
    },
    {
        "text": "this makes it harder to extract real\u2010world meaning from the model."
    },
    {
        "text": "it is also an insidi- ous form of overfitting, which is begging to have the model generalize poorly."
    },
    {
        "text": "in lasso regression, p(x) has the same functional form of \u03c3( ) w x b \u22c5 + ."
    },
    {
        "text": "however, we train it in a way that punishes modest\u2010sized weights."
    },
    {
        "text": "the numeri- cal algorithm that finds the optimal weights generally doesn\u2019t use heuristics or anything; it\u2019s just cold numerical trudging."
    },
    {
        "text": "however, as an aid to human intui- tion, i like to think of some examples of heuristics that the solver might, in effect, employ: \u25cf \u25cfif features i and j have large weights, but they usually cancel each other out when classifying a point, set both their weights to 0."
    },
    {
        "text": "\u25cf \u25cfif features i and j are highly correlated, you can reduce the weight for one while increasing the weight for the other and keeping predictions more or less the same."
    },
    {
        "text": "the end result of all this tends to be having most of the feature weights go to 0 while only a few of the most significant features have nonzero weights."
    },
    {
        "text": "8.5.7 naive bayes bayesian statistics is one of the biggest, most interesting, and most mathemati- cally sophisticated areas of machine learning."
    },
    {
        "text": "however, most of that is in the context of bayesian networks, which are a deep, highly sophisticated family of models that you typically don\u2019t see in normal data science (although i will dis- cuss them a little bit in a later chapter)."
    },
    {
        "text": "data scientists are more likely to use a drastically simplified version called naive bayes."
    },
    {
        "text": "i talk in more detail about bayesian statistics in the chapter on statistics."
    },
    {
        "text": "briefly though, a bayesian classifier operates on the following intuition: you start off with some initial confidence in the labels 0 and 1 (assume that it\u2019s a binary classification problem)."
    },
    {
        "text": "when new information becomes available, you adjust your confidence levels depending on how likely that information is"
    },
    {
        "text": "8.5 specific classifiers 111 conditioned on each label."
    },
    {
        "text": "when you\u2019ve gone through all available informa- tion, your final confidence levels are the probabilities of the labels 0 and 1. ok, now let\u2019s get more technical."
    },
    {
        "text": "during the training phase, a naive bayesian classifier learns two things from the training data: \u25cf \u25cfhow common every label is in the whole training data \u25cf \u25cffor every feature xi, its probability distribution when the label is 0 \u25cf \u25cffor every feature xi, its probability distribution when the label is 1. the last two are called the conditional probabilities, and they are written as pr | pr | x x y x x y i i i i = = ( ) = = ( ) 0 1 when it comes time to classify a point x = (x1, x2,..., xd), the classifier starts off with confidences pr y 0 =fraction of the training data with y=0 pr y 0 =fraction of = = ( ) ( ) the training data with y=1 then for each feature xi in the data, let xi be the value it actually had."
    },
    {
        "text": "we then update our confidences to pr pr *pr | * pr pr *pr | y y x x y y y x x y i i i i = ( ) \u2190 = ( ) = = ( ) = ( ) \u2190 = ( ) = = 0 0 0 1 1 1 \u03b3 ( )* \u03b3 where we set \u03b3 so that the confidences add up to 1. there are a lot of things here that need fleshing out if you\u2019re implementing a naive bayes classifier."
    },
    {
        "text": "for example, we need to assume some functional form for pr( | ) x x y i i = = 0 , such as a normal distribution or something, in order to fit it during the training stage."
    },
    {
        "text": "we also need to be equipped to deal with overfitting there."
    },
    {
        "text": "but the biggest problem is that we are treating xi as being independent of every other xj."
    },
    {
        "text": "for example, our data might be such that x5 is just a copy of x4."
    },
    {
        "text": "in that case, we really shouldn\u2019t adjust our confidences when we get to x5, since x4 had already accounted for it."
    },
    {
        "text": "naive bayes classifiers completely ignore this possibility, so it\u2019s perhaps surprising that they tend to be very powerful classifiers."
    },
    {
        "text": "the way i think of it is this: imagine the situation i described, where x4 and x5 are identical, so we essentially double\u2010count x4."
    },
    {
        "text": "if x4 is a powerful predictive variable, then this might make us overconfident, but it usually doesn\u2019t make us wrong."
    },
    {
        "text": "the following code uses scikit\u2010learn to train and use a gaussian naive bayes classifier, where pr( ) xi/\u03b3 is assumed to be a normal distribution: from sklearn.naive_bayes import gaussiannb clf = gaussiannb()"
    },
    {
        "text": "8 machine learning classification 112 clf.fit(train[indep_cols], train.breed) predictions = clf.predict(test[indep_cols]) 8.5.8 neural nets neural nets used to be the black sheep of classifiers, but they have enjoyed a renaissance in recent years, especially the sophisticated variants collectively known as \"deep learning.\""
    },
    {
        "text": "neural nets are as massive an area as bayesian net- works, and many people make careers out of them."
    },
    {
        "text": "however, basic neural nets are standard tools that you see in machine learning."
    },
    {
        "text": "they are simple to use, fairly effective as classifiers, and useful for teasing interesting features out of a dataset."
    },
    {
        "text": "neural nets were inspired by the workings of the human brain, but now that we know more about how biological circuits work, it\u2019s clear that that analogy is bunk."
    },
    {
        "text": "really sophisticated deep learning is at the point where it can be com- pared to some parts of real brains (or maybe we just don\u2019t know enough about the brain yet to see how much they fall short), but anything short of that should be thought of as just another classifier."
    },
    {
        "text": "the simplest neural network is the perceptron."
    },
    {
        "text": "a perceptron is a network of \"neurons,\" each of which takes in multiple inputs and produces a single output."
    },
    {
        "text": "an example is shown in the following figure: x2 x3 x1 y2 y3 y1 the labeled nodes correspond to either the input variables to a classification or a range of output variables."
    },
    {
        "text": "the other nodes are neurons."
    },
    {
        "text": "the neurons in the first layer take all of the raw features as inputs."
    },
    {
        "text": "their outputs are fed as inputs to the second layer, and so on."
    },
    {
        "text": "ultimately, the outputs of the final layer constitute the output of your program."
    },
    {
        "text": "all layers of neurons before the last one"
    },
    {
        "text": "8.5 specific classifiers 113 are called \"hidden\" layers (there is one in this figure the way i\u2019ve drawn it)."
    },
    {
        "text": "unlike other classifiers, neural networks very organically produce an arbitrary number of different outputs, one for each neuron in the final layer."
    },
    {
        "text": "in this case, there are three outputs."
    },
    {
        "text": "in general, you can use neural nets for tasks other than classification and treat the outputs as a general\u2010purpose numerical vector."
    },
    {
        "text": "in classification tasks though, we typically look at the ith output as the score for the ith category that a point can be classified as."
    },
    {
        "text": "the key part of a neural net is how each neuron determines its output from its various inputs."
    },
    {
        "text": "this is called the \"activation function,\" and there are a num- ber of options you can pick from."
    },
    {
        "text": "the one i\u2019ve seen the most is our old friend, the sigmoid function."
    },
    {
        "text": "if we let i indicate some particular neuron, and j ranges over its inputs, then: activation input i i j ij j b w = + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u2211 \u03c3 * in effect, each neuron in the system is its own little logistic regression func- tion, operating on the inputs to that neuron."
    },
    {
        "text": "a neural network with no hidden layers is, in fact, just a collection of logistic regressors."
    },
    {
        "text": "neural networks are trained using an iterative algorithm called backpropa- gation."
    },
    {
        "text": "without getting into too much detail, you feed it sequences of input vectors along with the corresponding correct output vectors (in classification, each correct output vector would be all 0s except for a single one)."
    },
    {
        "text": "as you do this, the parameters in the final layer get tweaked to accommodate the new information."
    },
    {
        "text": "those changes are sent back to the previous layer, whose param- eters are tweaked, and so on."
    },
    {
        "text": "part of the beauty of the sigmoid function is that it makes backpropagation more mathematically tractable."
    },
    {
        "text": "many classifiers are useful for identifying single features that are especially informative."
    },
    {
        "text": "neural networks, on the other hand, are known for identifying aggregate features, which are often more interesting than any of the raw inputs."
    },
    {
        "text": "let\u2019s say you have a neural network with one hidden layer."
    },
    {
        "text": "a neuron in the hidden layer works by taking a weighted combination of all the raw inputs and plugging that combination in a sigmoid function."
    },
    {
        "text": "well that weighted combina- tion is an aggregate feature of your data and often an extremely useful one."
    },
    {
        "text": "if you take a bunch of images of handwritten letters and train a neural net on them, the input layer will be raw pixels and the 26 outputs will correspond to letters of the alphabet."
    },
    {
        "text": "but neurons in the middle will emit signals that corre- spond to things such as straight line segments, curves, and other key compo- nents of letters."
    },
    {
        "text": "going back and making real\u2010world sense of a neural network\u2019s internal features can be very illuminating."
    },
    {
        "text": "for myself, neural networks are not a tool i use a lot."
    },
    {
        "text": "simple ones such as the perceptron don\u2019t perform particularly well, and using the more complicated ones is an extremely specialized area."
    },
    {
        "text": "i\u2019m more of an ensemble classifier guy,"
    },
    {
        "text": "8 machine learning classification 114 trusting in the law of large numbers rather than the voodoo of deep learning."
    },
    {
        "text": "but that\u2019s just me."
    },
    {
        "text": "neural nets are a hot area, and they are solving some very impressive problems."
    },
    {
        "text": "there\u2019s a good chance they will become a much larger, more standard tool in the data science toolkit."
    },
    {
        "text": "8.6 \u00adevaluating classifiers in most business applications of classification, you are looking for one class more so than the other."
    },
    {
        "text": "for example, you are looking for promising stocks in a large pool of potential duds."
    },
    {
        "text": "or, you are looking for patients who have cancer."
    },
    {
        "text": "the job of a classifier is to flag the interesting ones."
    },
    {
        "text": "there are two aspects of how well a classifier performs: you want to flag the things you\u2019re looking for, but you also want to not flag the things you aren\u2019t looking for."
    },
    {
        "text": "flag aggressively, and you\u2019ll get a lot of false positives \u2013 potentially very dangerous if you\u2019re looking for promising stocks to invest in."
    },
    {
        "text": "flag con- servatively, and you\u2019ll be leaving out many things that should have been flagged \u2013 terrible if you\u2019re screening people for cancer."
    },
    {
        "text": "how to strike the bal- ance between false positives and false negatives is a business question that can\u2019t be answered analytically."
    },
    {
        "text": "in this chapter, we will focus on two performance metrics that, together, give the full picture of how well a classifier performs: \u25cf \u25cftrue positive rate (tpr)."
    },
    {
        "text": "of all things that should be flagged by our classifier, this is the fraction that actually gets flagged."
    },
    {
        "text": "we want it to be high: 1.0 is perfect."
    },
    {
        "text": "\u25cf \u25cffalse positive rate (fpr)."
    },
    {
        "text": "of all things that should not be flagged, this is the fraction that still ends up getting flagged."
    },
    {
        "text": "we want it low: 0.0 is perfect."
    },
    {
        "text": "i will give you a nice graphical way to think of tpr and fpr, that is, the main way i think about classifiers in my own work."
    },
    {
        "text": "but you could pick other metrics too \u2013 they\u2019re all equivalent."
    },
    {
        "text": "the other options you\u2019re most likely to see are \"precision\" and \"recall.\""
    },
    {
        "text": "precision is the same thing as the true positive rate \u2013 the fraction of all flagged results that actually should have been flagged."
    },
    {
        "text": "recall measures your classifier\u2019s cover- age \u2013 out of all the things that should be flagged, it is the fraction that actually gets flagged."
    },
    {
        "text": "8.6.1 confusion matrices a common way to display performance metrics for a binary classifier is with a \u201cconfusion matrix.\u201d it is a 2 \u00d7 2 matrix displaying how many points in your test- ing data were placed in which category versus which category they should have been placed in."
    },
    {
        "text": "for example,"
    },
    {
        "text": "8.6 evaluating classifiers 115 correct label predicted = 0 predicted = 1 0 35 4 1 1 10 in the given confusion matrix, the true positive rate would then be 10/ (10 + 1) = 0.91, and the false positive rate would be 4/(4 + 35) = 0.10."
    },
    {
        "text": "8.6.2 roc curves if you treat the false positive rate as an x\u2010coordinate and the true positive rate as the y\u2010coordinate, then you can visualize a classifier\u2019s performance as a loca- tion in a two\u2010dimensional box such as the following: a happy medium!"
    },
    {
        "text": "many false positives any classifer in this triangle is worse than useless many false negatives false positive rate true positive rate 1.0 1.0 the upper\u2010left corner (0.0, 1.0) corresponds to a perfect classifier, flagging every relevant item with no false positives."
    },
    {
        "text": "the lower\u2010left corner means flag- ging nothing, and the upper\u2010right means flagging everything."
    },
    {
        "text": "if your classifier is below the y = x line, then it\u2019s worse than useless; an irrelevant item is more likely to be flagged than one that\u2019s actually relevant."
    },
    {
        "text": "my discussion so far has been about classifiers in a binary way; they will label something as (say) fraud or nonfraud."
    },
    {
        "text": "but very few classifiers are truly binary; most of them output some sort of a score, and it\u2019s up to data scientists to pick a cutoff for what counts as a hit."
    },
    {
        "text": "this means that a single classifier is really a whole family of classifiers, corresponding to where we pick the cutoff."
    },
    {
        "text": "each of these cutoffs corresponds to a different location in our 2d box, and together they trace out what\u2019s called an roc curve, similar to the ones that we gener- ated at the beginning of this chapter:"
    },
    {
        "text": "8 machine learning classification 116 1.0 0.8 0.6 0.4 true positive rate comparing classifers 0.2 0.0 0.0 0.2 0.4 0.6 false positive rate logistic regression: auc = 0.988837 naive bayes: auc = 0.896886 random forest: auc = 0.901146 decision tree: auc = 0.830200 0.8 1.0 imagine that you start with an insanely high classification threshold, so high that nothing actually gets flagged."
    },
    {
        "text": "this means that you are starting in the lower\u2010left of the box, at (0, 0)."
    },
    {
        "text": "as you loosen your criteria and start flagging a few results your location shifts."
    },
    {
        "text": "hopefully, the first results you start flagging are all dead ringers and you get very few false positives, that is, your curve slopes sharply up from the origin."
    },
    {
        "text": "when you get to the knee of the curve, you have flagged all the low\u2010hanging fruit and hopefully not incurred many false positives."
    },
    {
        "text": "if continue to loosen your criteria, you will start to correctly flag the stragglers, but you will also flag a lot of false positives."
    },
    {
        "text": "eventually, everything will get flagged, and you will be at (1, 1)."
    },
    {
        "text": "if we are trying to understand the quality of the underlying score\u2010based clas- sifier, then it\u2019s not fair to judge it by a single threshold."
    },
    {
        "text": "you want to judge it by the entire roc curve \u2013 a sharp \"knee\" jutting into the upper\u2010left corner is the signature of a strong classifier."
    },
    {
        "text": "8.6.3 area under the roc curve this holistic, look\u2010at\u2010the\u2010whole\u2010roc\u2010curve viewpoint doesn\u2019t absolve us from sometimes having to boil the performance down into a single number."
    },
    {
        "text": "sometimes, you\u2019ll need a numerical criterion for, say, declaring that one con- figuration for your classifier is better than another."
    },
    {
        "text": "the standard way to score an entire roc curve is to calculate the area under the curve (auc); a good classifier will mostly fill the square and have an auc"
    },
    {
        "text": "8.7 selecting classification cutoffs 117 near 1.0, but a poor one will be close to 0.5. the auc is a good way to score the underlying classifier, since it makes no reference to where we would draw a classification cutoff."
    },
    {
        "text": "when you run the numbers, there is a very clear, real\u2010 world meaning to the auc: it is the probability that a randomly selected hit will have a higher prediction compared to a randomly selected nonhit."
    },
    {
        "text": "in my own work, i will use the auc to decide which of several underlying classifiers and configurations i want to use."
    },
    {
        "text": "if random forest has an auc of 0.95, but logistic regression only has 0.85, it\u2019s clear where i\u2019m going to focus my efforts."
    },
    {
        "text": "in the script at the beginning of this chapter, we showed code that computes taking in our prediction scores and the correct labels and uses them to com- pute the fpr, tpr, classification thresholds, and auc."
    },
    {
        "text": "the relevant lines are as follows: from sklearn.metrics import roc_curve, auc fpr, tpr, thresholds = roc_curve(y_test, pred) auc_score = auc(fpr, tpr 8.7 \u00adselecting classification cutoffs intuitively, we want to set our thresholds so that our classifier is near the \"knee\" of the curve."
    },
    {
        "text": "maybe business considerations will nudge us to one part or another of the knee, depending on how we value precision versus recall, but there are also mathematically elegant ways to do it."
    },
    {
        "text": "i\u2019ll discuss two of the most common ones in this section."
    },
    {
        "text": "the first is to look at where the roc curve intersects the line y = 1\u2013x."
    },
    {
        "text": "this means that the fraction of all hits that end up getting flagged is equal to the fraction of all nonhits that don\u2019t get flagged: we have the same accuracy on the hits and the nonhits."
    },
    {
        "text": "a different cutoff would make me do better on hits but worse on nonhits, or vice versa."
    },
    {
        "text": "all other things being equal, this is the cutoff that i use, partly because i can truthfully answer the question of \u201chow accurate is your classifier?\u201d with a single number."
    },
    {
        "text": "the second approach is to look at where the roc curve has a 90% slope, that is, where it runs parallel to the line y = x. this is sort of an \"inflection point\": below this threshold, relaxing your classifier boosts the flagging probability of a hit more so than a nonhit."
    },
    {
        "text": "above this threshold, relaxing your classifier will boost the flagging probability for nonhits more so than for hits."
    },
    {
        "text": "it\u2019s effectively like saying that an epsilon increase in tpr is worth the cost of an epsilon increase in fpr, but no more than that."
    },
    {
        "text": "this second approach is also useful because it generalizes."
    },
    {
        "text": "you could instead decide that a tiny increase in tpr is worth three times the increase in fpr, because it\u2019s that much more important to you to find extra hits."
    },
    {
        "text": "personally, i\u2019ve"
    },
    {
        "text": "8 machine learning classification 118 never had occasion to go that far into the weeds, but you should be aware it\u2019s possible in case the need arises."
    },
    {
        "text": "8.7.1 other performance metrics the auc is the right metric to use when you\u2019re trying to gauge the holistic performance of a classifier that gives out continuous scores."
    },
    {
        "text": "but if you\u2019re using the classifier as a basis for making decisions, the underlying classifier score is only worth as much as the best single classifier you can make out of it."
    },
    {
        "text": "so, what you might do here is to pick a \"reasonable\" threshold (by the definition of your choice) for your classifier and then evaluate the performance of that truly binary classifier."
    },
    {
        "text": "if you set your classification threshold to the point where the roc curve intersects the line y = 1\u2013x, then your classifier has the same accuracy in classi- fying hits that it does with nonhits."
    },
    {
        "text": "in that case, you can use this single number as the accuracy of the classifier."
    },
    {
        "text": "this has the advantage of simplicity and of being easier to explain to clients who aren\u2019t used to the idea of using two num- bers to evaluate a classifier."
    },
    {
        "text": "the classical way to judge a binary classifier is called the f1 score."
    },
    {
        "text": "it is the harmonic mean of the classifier\u2019s precision with its recall, defined by f1 1 1 1 2 2 = + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = precision recall precision recall precision / * * + recall the f1 score will be 1.0 for a perfect classifier and 0.0 in the worst case."
    },
    {
        "text": "it\u2019s worth noting though that there is nothing magical about using the harmonic mean of precision and recall, and sometimes, you will see the geometric mean used to compute the g\u2010score: g = precision recall * technically, you could even use the arithmetic mean (precision + recall)/2, but that would have the unpleasant effect that flagging everything (or not flag- ging anything at all) would give a score better than 0."
    },
    {
        "text": "8.7.2 lift\u2013reach curves some people prefer what\u2019s called a lift\u2013reach curve, rather than an roc curve."
    },
    {
        "text": "it captures equivalent information, namely how the performance of a classifier varies as you adjust the classification threshold, but displays it in a different way."
    },
    {
        "text": "the lift\u2013reach curve is based on the following notions: \u25cf \u25cfreach is the fraction of all points that get flagged."
    },
    {
        "text": "8.9 glossary 119 \u25cf \u25cflift is the fraction of all points you flag that are hits, divided by the fraction of hits in the overall population."
    },
    {
        "text": "a lift of 1 means that you are just flagging randomly, and anything above that is positive performance."
    },
    {
        "text": "the reach is plotted along the x\u2010axis, and the lift along the y\u2010axis."
    },
    {
        "text": "typically, the lift will start off high, and then it decays to 1.0 as reach approaches 1."
    },
    {
        "text": "8.8 \u00adfurther reading 1 bishop, c, pattern recognition and machine learning, 2007, springer, new york, ny."
    },
    {
        "text": "2 janert, p, data analysis with open source tools, 2010, o\u2019reilly media, newton, ma."
    },
    {
        "text": "8.9 \u00adglossary area under the roc curve the area under an roc curve measures how well a classifier works that is independent of where the classification threshold is set."
    },
    {
        "text": "confusion matrix a 2 \u00d7 2 table giving the true positives, false positives, true negatives, and false negatives for a classifier."
    },
    {
        "text": "decision tree a machine learning model that works as a flowchart considering one input feature at a time."
    },
    {
        "text": "ensemble classifier a machine learning classifier that works by training multiple classifiers on random subsets of the rows/columns of the training data."
    },
    {
        "text": "when classifying a point, it averages the results of the different classifiers."
    },
    {
        "text": "false positive rate the fraction of all nonhits that are errantly classified as hits."
    },
    {
        "text": "f1 score a measure of classifier performance."
    },
    {
        "text": "it is the harmonic mean of the precision and the recall."
    },
    {
        "text": "logistic regression a machine learning classifier that can be thought of as a nonbinary version of support vector machines."
    },
    {
        "text": "neural net a type of machine learning model inspired by human neurons."
    },
    {
        "text": "precision of all flagged results flagged by a classifier, this is the fraction that are actually hits."
    },
    {
        "text": "random forest an ensemble of decision trees in a single classifier."
    },
    {
        "text": "recall the fraction of all hits that get flagged by a classifier."
    },
    {
        "text": "roc curve a graph that measures fpr on the x\u2010axis and tpr on the y\u2010axis, across all possible classification cutoffs for a classifier that outputs continuous scores."
    },
    {
        "text": "8 machine learning classification 120 sigmoid function a common activation function for neural nets, whose output is always between 0 and 1. support vector machine a machine learning classifier that works by drawing a hyperplane in d\u2010dimensional space and classifying points by which side they fall on."
    },
    {
        "text": "svm short for \u201csupport vector machine.\u201d true positive rate the fraction of all hits that are correctly classified as hits."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 121 9 i debated whether to include this chapter in the book."
    },
    {
        "text": "in the first place, it ven- tures into \u201ctouchy feely\u201d areas that i generally try to avoid."
    },
    {
        "text": "this is mostly a technical, brass\u2010tacks type of book."
    },
    {
        "text": "the second problem is that i don\u2019t feel that i myself am that great at technical communication."
    },
    {
        "text": "i\u2019m certainly good enough to get my own work done (and given that i\u2019m in consulting, that bar is higher for me than for most data scientists), but beyond that i don\u2019t claim to have any special expertise."
    },
    {
        "text": "however, my own lack of natural talent is part of why i felt this chapter is necessary."
    },
    {
        "text": "i\u2019ve seen that first\u2010rate technical work can be tragically undervalued if people fail to communicate it in an effective way."
    },
    {
        "text": "i\u2019ve also seen that just a few basic, easy\u2010to\u2010learn principles can make a world of difference between incom- prehensibility and a stunning presentation."
    },
    {
        "text": "internalizing a few guiding princi- ples has made for career advancement for myself, follow\u2010up engagements for my company, and early identification of mismatches between the technical work and business objectives."
    },
    {
        "text": "data scientists are in a uniquely communication\u2010intensive niche."
    },
    {
        "text": "software engineers mostly talk with other software engineers, business analysts with business analysts, and so on."
    },
    {
        "text": "it is the job of a data scientist to bridge the gaps between the worlds of business, analytics, and software."
    },
    {
        "text": "so, it\u2019s a crying shame that, frankly, most of us aren\u2019t that good at it."
    },
    {
        "text": "ultimately, everything in this chapter is about one goal: conveying ideas to your audience in a way that (1) they will actually understand and (2) they have an easy time understanding."
    },
    {
        "text": "in this chapter i will start with a few general principles that i think underlie most good technical communication."
    },
    {
        "text": "i will then move on to specific tips for slide decks, written reports, and spoken presentations."
    },
    {
        "text": "i will also include a sec- tion on source code, which is sometimes the communication medium of last resort."
    },
    {
        "text": "technical communication and documentation"
    },
    {
        "text": "9 technical communication and documentation 122 9.1 \u00adseveral guiding principles 9.1.1 know your audience this is one of the most basic principles of technical communication, but it\u2019s also one of the hardest to master."
    },
    {
        "text": "as a data scientist, you will talk to the follow- ing people: \u25cf \u25cfdomain experts who understand what you\u2019re studying better than you do but don\u2019t know much about software and analytics."
    },
    {
        "text": "you will often have to hold their hand a little bit in explaining what you did."
    },
    {
        "text": "the various real\u2010 world scenarios that got left out of your analysis are likely to be a particular focus of conversation."
    },
    {
        "text": "in my experience, these people are extremely useful in uncovering shortcomings in your work or making sense of issues in the data."
    },
    {
        "text": "\u25cf \u25cfanalytical people, who are very interested in the nitty\u2010gritty of what you\u2019ve done."
    },
    {
        "text": "you can expect to spend a lot of time here discussing (and possibly justifying) your statistical methodology and modeling choices."
    },
    {
        "text": "\u25cf \u25cfsoftware engineers, who will often want to treat your code as a black box that magically spits out answers, but they will care a lot about its perfor- mance in real\u2010world situations."
    },
    {
        "text": "software engineers run the gamut from barely knowing how to calculate an average to being extremely mathemati- cally savvy."
    },
    {
        "text": "\u25cf \u25cfbusiness people, a diverse group, ranging from former engineers who will grill you about the details to nontechnical managers who want everything translated into business speak."
    },
    {
        "text": "the one thing they will have in common is a keen curiosity about what your work means for the different parts of the company."
    },
    {
        "text": "another important part of knowing your audience is knowing how much detail to include and how much of the story of your work."
    },
    {
        "text": "a high\u2010level execu- tive might want to know just a few key take\u2010home points."
    },
    {
        "text": "your peers might want more details about your methodology, especially if your findings are especially important or surprising or if you change direction mid\u2010project because of preliminary findings."
    },
    {
        "text": "sometimes, it might be important to go into several things you tried that didn\u2019t work, because you didn\u2019t get strong results and you need to show that it was the data\u2019s fault and not yours."
    },
    {
        "text": "in other cases, that is a waste of people\u2019s time."
    },
    {
        "text": "9.1.2 show why it matters always make sure to frame an analytics in the context of something people already care about, usually a business problem, in order to make it compelling."
    },
    {
        "text": "depending on your audience, you may also need to clearly explain how the analytics relates back to the problem and can impact the bottom line."
    },
    {
        "text": "you"
    },
    {
        "text": "9.1 several guiding principles 123 typically don\u2019t need to belabor the point, but you should give people a reason to care about what you\u2019re saying."
    },
    {
        "text": "many analytics problems are clearly related to a company\u2019s business and probably don\u2019t need any motivation."
    },
    {
        "text": "getting people to click on ads, for exam- ple, is obviously core to many business models."
    },
    {
        "text": "in other cases though, the con- nection is more tenuous."
    },
    {
        "text": "do we really need customer segmentation, if we aren\u2019t currently planning to target ads?"
    },
    {
        "text": "especially in large corporations, disa- greements between high\u2010level people come up over the value of analytics pro- jects, and chances are your boss will have to lean on you to explain why the project is worthwhile."
    },
    {
        "text": "as i write this, i\u2019m on a project where i\u2019m working to convince a factory manager that it\u2019s worthwhile to use analytics to study how we can reduce the bottleneck at their test bench."
    },
    {
        "text": "this is not just about communicating with other people; it\u2019s also an excellent exercise for you."
    },
    {
        "text": "if you can\u2019t explain in simple terms why an analytics project is worthwhile, then maybe you should be working on a different problem."
    },
    {
        "text": "9.1.3 make it concrete the human brain doesn\u2019t do very well with abstract concepts."
    },
    {
        "text": "i don\u2019t just mean nontechnical people; even if somebody has the background to follow a purely abstract discussion, their understanding will be immeasurably helped if you give their brain a few concrete mental hooks."
    },
    {
        "text": "often, the business case at hand provides all the concrete examples you need."
    },
    {
        "text": "other times though, the business case is too convoluted to illustrate things clearly, and you will want a simple toy problem."
    },
    {
        "text": "9.1.4 a picture is worth a thousand words one of the best pieces of advice i ever got for writing papers or giving technical talks was this: the heart of your presentation is one or a few key figures."
    },
    {
        "text": "the rest of the paper is just an extended caption describing how you generated those figures and how to interpret them."
    },
    {
        "text": "it seems like every year i decide that visuals are more important than i had previously realized, and every year i\u2019m right."
    },
    {
        "text": "whether it\u2019s diagrams to illustrate a concept, plots to display data, or just a stick figure scrawled on a whiteboard, this is the best way to convey ideas to another person and make them compelling."
    },
    {
        "text": "in my opinion, the lack of pictures in some papers and presentations is often a sign of laziness (certainly, it sometimes is with mine)."
    },
    {
        "text": "it takes some planning to decide what figures would work best."
    },
    {
        "text": "then there\u2019s a lot of legwork in gener- ating those figures, whether it\u2019s manipulating a diagram in powerpoint or mak- ing sure that the axes are set correctly on a plot."
    },
    {
        "text": "it\u2019s a lot easier to just sit at the keyboard and churn out slides and pages of text (at least, for me it is), but that\u2019s the wrong way to go about it."
    },
    {
        "text": "9 technical communication and documentation 124 9.1.5 don\u2019t be arrogant about your tech knowledge this should go without saying, but i feel compelled to bring it up because i have seen it way too often: data scientists being jerks toward people who don\u2019t know as much math as they do."
    },
    {
        "text": "obviously, this is horrible for repeat business, and it puts up a massive barrier to clear communication."
    },
    {
        "text": "but for my two cents, i\u2019d like to say that it\u2019s also wrong."
    },
    {
        "text": "i\u2019ve seen the same data scientists who were being so arrogant go on to screw up projects, because they knew the mathe- matical equations but were unable to think critically about the concepts behind them."
    },
    {
        "text": "i had a lot of fun on a consulting project once, where the manager on the client\u2019s side had no mathematical background to speak of."
    },
    {
        "text": "but whenever she asked me about the tech work, she would immediately ask all the right ques- tions."
    },
    {
        "text": "it was almost as if i had sat down with a list of all the statistics concepts that were important to the problem, translated each one into normal english, and then put a question mark at the end."
    },
    {
        "text": "she had no training in statistics; she was just smart and level\u2010headed enough that she zeroed in on all the right points, even better than most data scientists i\u2019ve worked with."
    },
    {
        "text": "it was a fun reminder to me that math is not synonymous with clear thinking: it\u2019s just a way of reducing that clear thinking to calculations so that you can get a number out."
    },
    {
        "text": "9.1.6 make it look decent i used to think that aesthetics was peripheral to clear communication."
    },
    {
        "text": "i felt like people should judge my work based on its technical merits, rather than how much i agonized over which shade of peach to use."
    },
    {
        "text": "so, it came as quite a shock when i first read about graphic design."
    },
    {
        "text": "i discovered that it isn\u2019t an attempt to shoehorn artistic sentiments into technical work; it is a pragmatic way to make sure that communication is clear and compelling."
    },
    {
        "text": "you should use good design principles on a slide for the same reason you should use logarithmic axes when graphing some data: it helps to get the point across."
    },
    {
        "text": "9.2 \u00adslide decks slide decks are the most common medium for communicating data science results."
    },
    {
        "text": "they\u2019re also the medium where you can have the most impact, because people are more likely to listen to your talk or walk through a slide deck than they are to read a written report."
    },
    {
        "text": "the biggest pitfall that i see with slide decks is people treating them as writ- ten reports."
    },
    {
        "text": "in some cases, they will go so far as to just copy and paste text from a write\u2010up they did and call it a slide deck."
    },
    {
        "text": "this is the wrong way to do it!"
    },
    {
        "text": "slide decks are a fundamentally different communication medium and governed by"
    },
    {
        "text": "9.2 slide decks 125 a different set of rules."
    },
    {
        "text": "on the one hand, you have complete freedom to control what the slide looks like."
    },
    {
        "text": "on the other, people will expect to be able to step through a slide deck much more quickly than a report, so it is critical to keep the deck crisp, compelling, and to the point."
    },
    {
        "text": "frequently, this means replacing textual content with graphical content wherever possible."
    },
    {
        "text": "the second most frequent problem i see is people going overboard with what you can do in a slide deck."
    },
    {
        "text": "crazy shapes, goofy moving images, and pic- tures of cats should be used very sparingly."
    },
    {
        "text": "ultimately, you will have to think about how best to structure your ideas around a graphical representation."
    },
    {
        "text": "this is a totally different skill set from writ- ing or speaking in clear prose."
    },
    {
        "text": "in my experience, good graphical presentations are harder to put together compared to good prose but much easier for your audience to follow."
    },
    {
        "text": "there is no magic bullet to making good presentations: it will require hard work on your end and making a habit of asking whether something could be explained a little more clearly."
    },
    {
        "text": "to get you started though, a good rule of thumb is that 75% of your slides should consist of an image (a data graphic, a flow- chart, etc.)"
    },
    {
        "text": "and at most two phrases/sentences that give the take\u2010home mes- sage for that image."
    },
    {
        "text": "9.2.1 c.r.a.p."
    },
    {
        "text": "design a lot of the principles of good design are captured in the acronym c.r.a.p, which stands for contrast, repetition, alignment, and proximity: \u25cf \u25cfcontrast: things that are different should look different."
    },
    {
        "text": "this makes it seam- less for people to notice and internalize the differences."
    },
    {
        "text": "for example: \u2013 \u2013 use different fonts for code, text, and figure captions."
    },
    {
        "text": "\u2013 \u2013 use different colors for different customer segments that you\u2019ve identified."
    },
    {
        "text": "\u2013 \u2013 within reason, use different font sizes on slides to emphasize different points."
    },
    {
        "text": "\u25cf \u25cfrepetition: key points or design motifs should be repeated throughout your work."
    },
    {
        "text": "people are likely to miss them the first time, so you want to repeat them in a way that is obvious enough to have an impact but subtle enough that they\u2019re not annoying if people already got the point."
    },
    {
        "text": "repetition is partly the dual of contrast: if things are similar or related, or if they are different, those patterns should be carried through consistently."
    },
    {
        "text": "\u25cf \u25cfalignment: of all the principles, this comes the closest to being a purely aesthetic thing."
    },
    {
        "text": "make sure that the different parts of your visual field line up with each other in a natural way."
    },
    {
        "text": "or, if they shouldn\u2019t line up (maybe because you\u2019re trying to contrast them), then make sure they are obviously not aligned."
    },
    {
        "text": "the last thing you want is for somebody to be distracted dur- ing a talk by wondering whether two blocks of text are actually slightly out of line."
    },
    {
        "text": "9 technical communication and documentation 126 \u25cf \u25cfproximity: use distance between things to indicate their relationships."
    },
    {
        "text": "more generally, use the layout of the visual field to your advantage."
    },
    {
        "text": "to show you just how bad it can get when the basic principles of design are brazenly flaunted, i humbly present one of my own slides that i dug up from back in the day: overview purpose numpy scipy numerical and mathematical computing for python make it fast core extension to python support for n-dimensional arrays mathematical operations on arrays extensive libraries for technical computation operates on numpy arrays among the problems in this slide, which i somehow managed to stand up in front of people and present, are the following: \u25cf \u25cfalmost everything is in the same font."
    },
    {
        "text": "\u25cf \u25cfthe purpose of the talk looks like it\u2019s just another topic, on par with scipy and numpy."
    },
    {
        "text": "\u25cf \u25cfthere is an unsightly amount of white space."
    },
    {
        "text": "\u25cf \u25cfeverything is aligned with everything else."
    },
    {
        "text": "if i could take back that presentation, i would replace the slide with some- thing like the one on the next : the changes i wish i could make include the following: \u25cf \u25cf[contrast] the slide title, goal, topic text all look different."
    },
    {
        "text": "\u25cf \u25cf[repetition] those text differences are marked consistently."
    },
    {
        "text": "\u25cf \u25cf[alignment] the numpy and scipy sections line up with each other, as do their supporting pictures."
    },
    {
        "text": "\u25cf \u25cf[alignment] numpy and scipy, the two subjects of the talk, both have their own half of the space."
    },
    {
        "text": "the stuff that applies to the whole slide is in the middle."
    },
    {
        "text": "\u25cf \u25cf[proximity] the word \u201cnumpy,\u201d the description of numpy, and the sugges- tive pictures are all next to each other."
    },
    {
        "text": "similarly for scipy."
    },
    {
        "text": "the goal line is next to the slide title."
    },
    {
        "text": "9.2 slide decks 127 \u25cf \u25cfi added some suggestive pictures."
    },
    {
        "text": "realistically, many people aren\u2019t going to read my descriptions of numpy and scipy."
    },
    {
        "text": "but even so, they\u2019ll still get the idea that numpy does basic operations on arrays, while scipy does fancier mathematical operations."
    },
    {
        "text": "the content of the slides is the same."
    },
    {
        "text": "graphic design is there to grease the wheels of communication."
    },
    {
        "text": "it\u2019s not there to supplement my content: it\u2019s there to make you notice it."
    },
    {
        "text": "9.2.2 a few tips and rules of thumb making good slides is a skill that only develops with practice."
    },
    {
        "text": "but to help you avoid a few common pitfalls, here are several helpful hints or rules of thumb that have served me well: \u25cf \u25cfavoid long sentences or phrases."
    },
    {
        "text": "you can often replace a complete sentence with a shorter phrase."
    },
    {
        "text": "\u25cf \u25cfhave no more than four bullet points in any given section."
    },
    {
        "text": "\u25cf \u25cfif you feel like there\u2019s too much you want to fit into a slide, it\u2019s fine to break it out into two slides."
    },
    {
        "text": "\u25cf \u25cfif you can replace a bulleted list with something else, such as callouts with arrows pointing at different parts of a figure, you should probably do it."
    },
    {
        "text": "\u25cf \u25cfif the same image or section occurs in the same place on two consecutive slides, make absolutely sure that it is in exactly the same place."
    },
    {
        "text": "nothing is more visually grating than having something shift over by a few pixels when you go from one slide to the next."
    },
    {
        "text": "overview numpy core numerical library extensive libraries for technical computation n-dimensional arrays math operations on arrays operates on numpy arrays scipy 2.3 0.2 2.2 4.7 0.0 1.3 \u22122.3 \u20133.1 2.3 0.2 2.2 4.7 0.0 1.3 \u22122.3 \u20133.1 \u22127\u22126\u22125\u22124\u22123\u22122\u22121 1 2 3 4 5 6 7 ... ... ... ... 1"
    },
    {
        "text": "9 technical communication and documentation 128 \u25cf \u25cfavoid the (in)famous comic sans ms font altogether unless you\u2019re sure about your audience."
    },
    {
        "text": "personally, i think comic sans is useful when employed judiciously, but a lot of people feel strongly that it looks unprofessional and should just never be used."
    },
    {
        "text": "know your audience."
    },
    {
        "text": "\u25cf \u25cfpng files are your friend, because they can have transparent backgrounds."
    },
    {
        "text": "this lets you put an image in your slide without having its white background mar other images or the slide\u2019s backdrop."
    },
    {
        "text": "personally, i keep a slide deck con- sisting of only images with transparent backdrops (logos for different pro- gramming languages, etc.)"
    },
    {
        "text": "to save myself the trouble of looking for such images on the internet each time i need them."
    },
    {
        "text": "\u25cf \u25cfmake sure the axes are labeled on all of your figures."
    },
    {
        "text": "i made this mistake once... not good."
    },
    {
        "text": "\u25cf \u25cfhave a slide background."
    },
    {
        "text": "this is easy to do and makes slides look so much better."
    },
    {
        "text": "keep it simple, so that it doesn\u2019t distract people from your content."
    },
    {
        "text": "\u25cf \u25cfmake sure that your figures include some color if possible."
    },
    {
        "text": "\u25cf \u25cfif you want two different colors in a color scheme, don\u2019t use red and green."
    },
    {
        "text": "many people are partly colorblind and can\u2019t see the difference."
    },
    {
        "text": "\u25cf \u25cfit\u2019s fine to be more casual in a slide deck than you would in a written report."
    },
    {
        "text": "but don\u2019t overdo it."
    },
    {
        "text": "\u25cf \u25cfwhen planning your presentation schedule, the rule of thumb that people use is that one slide will take about 2 minutes to cover."
    },
    {
        "text": "\u25cf \u25cfin some cases, you might want to include slide numbers, dates, or a company logo or something on every slide."
    },
    {
        "text": "it\u2019s good to give people concrete reference points."
    },
    {
        "text": "9.3 \u00adwritten reports first off, please don\u2019t use latex unless you are planning to publish your work in a scientific journal or you are deliberately trying to look academic so as to make an impression on somebody."
    },
    {
        "text": "this is one of my pet peeves, which i see distressingly often since so many data scientists come from an academic back- ground where latex is standard."
    },
    {
        "text": "if you haven\u2019t heard of it, latex is a markup language that can be compiled into beautifully formatted documents, and it is very popular for publishing scientific papers."
    },
    {
        "text": "the downside though is that you need to know the idiosyncrasies of latex\u2019s syntax in order to edit the docu- ment, which bars most people from editing it collaboratively."
    },
    {
        "text": "it\u2019s true that latex gives you very fine\u2010grained control over what a document looks like, but that power is rarely necessary and often abused."
    },
    {
        "text": "i generally recommend that you use microsoft word, google docs, or some other wysiwyg editor."
    },
    {
        "text": "i\u2019m writing this book in word."
    },
    {
        "text": "now that that\u2019s out of the way, let\u2019s talk about content and presentation."
    },
    {
        "text": "the structure of a written report will vary depending on your intended audience,"
    },
    {
        "text": "9.3 written reports 129 who you are in relation to them (team member, outside consultant, member of another team, etc."
    },
    {
        "text": "), and the problem you\u2019re addressing."
    },
    {
        "text": "however, most techni- cal reports will have some subset of the following sections: \u25cf \u25cfan executive summary."
    },
    {
        "text": "this is up to one that summarizes what prob- lem you were addressing and why, what you did, and what can be done with it."
    },
    {
        "text": "the emphasis should be on the takeaway points from a business perspec- tive and how your work fits into the larger context of a company."
    },
    {
        "text": "\u25cf \u25cfbackground and motivation."
    },
    {
        "text": "clearly, frame how this work fits into a larger context for your likely audience."
    },
    {
        "text": "depending on who you\u2019re writing for, it might be a description of how this fits into the company\u2019s business, the role it plays in software, or existing knowledge that it builds on."
    },
    {
        "text": "\u25cf \u25cfdatasets used."
    },
    {
        "text": "describe in brief which datasets are being used, where you got them from, and what they\u2019re describing."
    },
    {
        "text": "plus maybe a little bit about which features you extract from them and any limitations of the data that should be pointed out."
    },
    {
        "text": "this section should be short and sweet; if there are a lot of gory details, put them in an appendix."
    },
    {
        "text": "\u25cf \u25cfanalytical overview."
    },
    {
        "text": "describe at a high level the analysis you performed or the algorithm you are studying."
    },
    {
        "text": "focus on the mathematical model in the abstract, rather than how it is implemented in software (unless some key aspects of it were driven by software requirements, such as wanting it to be massively parallel)."
    },
    {
        "text": "this section should probably have a diagram or two that illustrate what you\u2019re talking about."
    },
    {
        "text": "\u25cf \u25cfresults."
    },
    {
        "text": "describe any results you got from your analysis, and present them in graphical form."
    },
    {
        "text": "this is often the most important part of your report, so keep it crisp and compelling, and make sure to tie it back to the context of how these results are relevant."
    },
    {
        "text": "if you have a lot of results to report that contain similar information (such as results for each feature), then include only the most interesting ones in this section."
    },
    {
        "text": "put the rest in an appendix."
    },
    {
        "text": "\u25cf \u25cfsoftware overview."
    },
    {
        "text": "this section often doesn\u2019t need to be there and should be short if you do include it."
    },
    {
        "text": "it\u2019s mostly relevant if your code is being plugged into somebody else\u2019s code or some production system or if your code might be regularly rerun in the future as datasets are updated."
    },
    {
        "text": "describe how to run the code (this should be at most a handful of lines \u2013 if it\u2019s not, then you should refactor it and maybe combine it into a master script) if it\u2019s a stand\u2010alone analysis or how it plugs into other software if that\u2019s how it works."
    },
    {
        "text": "include a high\u2010level architecture of the code as a diagram, and describe which languages it is written in and what tools it uses."
    },
    {
        "text": "\u25cf \u25cffuture work."
    },
    {
        "text": "discuss natural next steps."
    },
    {
        "text": "this section often reads as boiler- plate in practice, and sometimes, it\u2019s ok if it\u2019s extremely brief or even omit- ted entirely."
    },
    {
        "text": "however, it can also be an opportunity to point the way to"
    },
    {
        "text": "9 technical communication and documentation 130 significant new projects and to suggest others that should not be pursued."
    },
    {
        "text": "data science is often used to \u201ctest the waters\u201d and see whether something is worth pursuing as a larger project or clarify the scope that such a project should have."
    },
    {
        "text": "\u25cf \u25cfconclusions."
    },
    {
        "text": "\u25cf \u25cfappendices with technical details."
    },
    {
        "text": "for me, personally, up to half my report is liable to be appendices."
    },
    {
        "text": "9.4 \u00adspeaking: what has worked for me i have never benefitted from the classic technique of imagining your audience naked."
    },
    {
        "text": "more seriously though, different people have very different styles of presen- tation."
    },
    {
        "text": "some people script out their presentations and practice them down to the word, making sure that every inflection is carefully chosen but still feels natural."
    },
    {
        "text": "personally, i\u2019ve never had much success with that approach."
    },
    {
        "text": "what works best for me is to: \u25cf \u25cfmake sure i\u2019ve got a great slide deck."
    },
    {
        "text": "\u25cf \u25cfpractice a lot, discussing each slide in the deck and the key concepts it\u2019s dis- cussing."
    },
    {
        "text": "not scripting them out, but giving clear explanations out loud."
    },
    {
        "text": "\u25cf \u25cfwhen it comes time for the actual presentation, throw all my practice out the window and wing it."
    },
    {
        "text": "i keep half an eye on time, but otherwise just explain my slides in whatever way feels most natural."
    },
    {
        "text": "this is what works for me."
    },
    {
        "text": "i tend to speak very well spontaneously, and many of my best talks are given at the spur of a moment, on topics that i know well but wasn\u2019t planning to discuss."
    },
    {
        "text": "but when i have an actual plan for what i\u2019m going to say, i immediately become mumbly and boring."
    },
    {
        "text": "the gold standard of presentations is to make it feel like a conversation to your audience (with a well\u2010composed, clearly\u2010thought\u2010out person who is good at explaining things) rather than any kind of formal speech."
    },
    {
        "text": "there is an awful lot that goes into making it look natural: tone of voice, cadence, facial expres- sions, and moving your body, all of which are independent of the actual words you are saying."
    },
    {
        "text": "this is too much to keep track of while you are focusing on your actual talk, so the only solution is to practice so that these things become sec- ond nature."
    },
    {
        "text": "i recommend training yourself to have a \u201cspeaking personality,\u201d a game face that you can put on at will."
    },
    {
        "text": "you should regularly practice giving off\u2010the\u2010cuff explanations of things that are in your line of work."
    },
    {
        "text": "this might be explaining how a cache works, describing the goals and status of a project you\u2019re working on, or giving an elevator spiel about your grad school research."
    },
    {
        "text": "it can be done in front of a mirror, in front of friends, or even just in your own head as an"
    },
    {
        "text": "9.5 code documentation 131 internal monologue."
    },
    {
        "text": "during your explanations, focus on your content but also keep an eye toward the following guidelines: \u25cf \u25cfbe clear and enunciate."
    },
    {
        "text": "\u25cf \u25cfspeak a little more slowly than you would in normal conversation."
    },
    {
        "text": "people will be paying attention to your slides as well as your words, and you won\u2019t be able to repeat something if a single person misses it."
    },
    {
        "text": "so, give everybody a little more time to absorb what you\u2019re saying."
    },
    {
        "text": "plus, this helps if (like me) you tend to speak faster when you\u2019re nervous."
    },
    {
        "text": "\u25cf \u25cfthrowing in a short personal anecdote, joke, or opinion can make your pres- entation more relatable and interesting."
    },
    {
        "text": "don\u2019t go overboard, but this can add a nice humanizing touch to technical material."
    },
    {
        "text": "\u25cf \u25cfa short pause is always better than \u201cum.\u201d \u25cf \u25cflet yourself be animated, so that it\u2019s clear you\u2019re excited about what you\u2019re discussing."
    },
    {
        "text": "add personality!"
    },
    {
        "text": "\u25cf \u25cfadopt a natural, at\u2010ease cadence."
    },
    {
        "text": "even if somebody isn\u2019t paying attention to your talk, they will immediately notice anxiety or nervousness in your voice."
    },
    {
        "text": "\u25cf \u25cftry to have good posture: stand up straight, hold your head high, and gently pull your shoulders back and down."
    },
    {
        "text": "this little bit of polish doesn\u2019t just make you look more confident; research has shown that adopting confident body language makes you more confident in reality."
    },
    {
        "text": "\u25cf \u25cfkeep an eye toward your hand movements."
    },
    {
        "text": "some people move their hands nervously in a way that is very distracting."
    },
    {
        "text": "but moderate use of hand gestures can add emphasis and personality."
    },
    {
        "text": "with enough practice, these finer points will become second nature."
    },
    {
        "text": "you will be able to easily shift in and out of \u201ctalk mode\u201d similarly to an actor going in and out of character."
    },
    {
        "text": "at that point, it won\u2019t just feel natural; it will now be sec- ond nature for you."
    },
    {
        "text": "many people have a deep\u2010seated anxiety about public speaking."
    },
    {
        "text": "if you are in that situation, then chances are this section won\u2019t give you everything you need."
    },
    {
        "text": "i encourage you to look into toast masters or a similar group that helps people practice public speaking and learn to feel at ease with it."
    },
    {
        "text": "9.5 \u00adcode documentation whenever you provide a significant piece of code as a deliverable, it\u2019s impor- tant to provide some kind of documentation of what it does and how to use it."
    },
    {
        "text": "depending on the context that can take a variety of forms, including the following: \u25cf \u25cfa long comment at the top of a file."
    },
    {
        "text": "\u25cf \u25cfa separate runbook or user manual."
    },
    {
        "text": "this is more common with extremely large pieces of software or if you\u2019re giving it to a client or another team."
    },
    {
        "text": "9 technical communication and documentation 132 \u25cf \u25cfpages on a company wiki."
    },
    {
        "text": "\u25cf \u25cfunit tests that can be run against the code."
    },
    {
        "text": "no matter what form the documentation is in though, the most important thing about the documentation is to explain how somebody can run the code and reproduce its functionality."
    },
    {
        "text": "this lets them use the code themselves and verify that it works the way that it\u2019s supposed to."
    },
    {
        "text": "explaining how it operates under the hood is secondary, and going into too much detail can be counterproductive."
    },
    {
        "text": "if you are delivering your source code to somebody, it is generally reasonable to expect them to be able to read and understand your code, and restating it all in english is superfluous."
    },
    {
        "text": "telling them how to run the software tells them where to start looking in the source code, and they can follow the thread from there."
    },
    {
        "text": "it is good to give a brief archi- tectural overview, pointing out which modules do what, but i wouldn\u2019t go beyond that."
    },
    {
        "text": "the one thing that is very nice to include though is a troubleshooting sec- tion."
    },
    {
        "text": "most pieces of software have some weird ways they can break down, or parts that are known to be especially fragile, that are highly specific to your software."
    },
    {
        "text": "if this is the case, save your users potentially hours of time debugging by telling them what they probably did wrong."
    },
    {
        "text": "9.6 \u00adfurther reading kolko, j, exposing the magic of design, 2011, oxford university press, new york, ny matplotlib 1.5.1 documentation, viewed 7 august 2016, http://matplotlib.org/ 9.7 \u00adglossary comic sans a font that is notorious for being very casual, some would argue overly so."
    },
    {
        "text": "c.r.a.p design the idea that you should use contrast, repetition, alignment, and proximity as underlying principles in graphic design."
    },
    {
        "text": "latex a markup language that is a very popular tool for publishing scientific papers."
    },
    {
        "text": "it can be compiled into beautifully formatter papers, but it is finicky to use and has a steep learning curve."
    },
    {
        "text": "runbook a document explaining what a piece of code does, how to use it, and how to troubleshoot it."
    },
    {
        "text": "133 the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. part ii stuff you still need to know the second section of this book will cover a variety of topics that are liable not to show up in a given data science project."
    },
    {
        "text": "this doesn\u2019t mean that knowing them is optional for a professional data scientist, but it does mean that you might not be put on the spot about them until somewhat later in your career."
    },
    {
        "text": "my goal is to fill those holes ahead of time."
    },
    {
        "text": "the topics here cover a wide range."
    },
    {
        "text": "there are very general\u2010purpose analytics tools that almost made it into the first part, such as clustering."
    },
    {
        "text": "most software engineering concepts, beyond basic scripting, fit into this chapter was well."
    },
    {
        "text": "finally, there are very specialized areas such as natural language processing, which some data scientists never use at all."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 135 10 this chapter is about techniques for studying the latent structure of your data, in situations where we don\u2019t know a priori what it should look like."
    },
    {
        "text": "they are often called \u201cunsupervised\u201d learning because, unlike classification and regres- sion, the \u201cright answers\u201d are not known going in."
    },
    {
        "text": "there are two primary ways of studying a dataset\u2019s structure: clustering and dimensionality reduction."
    },
    {
        "text": "clustering is an attempt to group the data points into distinct \u201cclusters.\u201d typically, this is done in the hopes that the different clusters correspond to different underlying phenomena."
    },
    {
        "text": "for example, if you plotted people\u2019s height on the x\u2010axis and their weight on the y\u2010axis, you would see two more\u2010or\u2010less clear blobs, corresponding to men and women."
    },
    {
        "text": "an alien who knew nothing else about human biology might hypothesize that we come in two distinct types."
    },
    {
        "text": "in dimensionality reduction, the goal isn\u2019t to look for distinct categories in the data."
    },
    {
        "text": "instead, the idea is that the different fields are largely redundant, and we want to extract the real, underlying variability in the data."
    },
    {
        "text": "the idea is that your data is d\u2010dimensional, but all of the points actually only lie on a k\u2010dimen- sional subset of the space (with k < d), plus some d\u2010dimensional noise."
    },
    {
        "text": "for example, in 3d data, your points could line mostly just along a single line or perhaps in a curved circle."
    },
    {
        "text": "real situations of course are usually not so clean cut."
    },
    {
        "text": "it\u2019s more useful to think of k dimensions as capturing \u201cmost\u201d of the variability in the data, and you can make k larger or smaller depending on how much of the information you want to reproduce."
    },
    {
        "text": "a key practical difference between clustering and dimensionality reduction is that clustering is generally done in order to reveal the structure of the data, but dimensionality reduction is often motivated mostly by computational con- cerns."
    },
    {
        "text": "for example, if you\u2019re processing sound, image, or video files, d is likely to be tens of thousands."
    },
    {
        "text": "processing your data then becomes a massive compu- tational task, and there are fundamental problems that come with having more dimensions compared to data points (the \u201ccurse of dimensionality,\u201d which i\u2019ll unsupervised learning: clustering and dimensionality reduction"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 136 discuss shortly)."
    },
    {
        "text": "in these cases, dimensionality reduction is a prerequisite for almost any analytics you might want to do, regardless of whether you\u2019re actu- ally interested in the data\u2019s latent structure."
    },
    {
        "text": "10.1 \u00adthe curse of dimensionality geometry in high\u2010dimensional spaces is weird."
    },
    {
        "text": "this is important, because a machine learning algorithm with d features operates on feature vectors that live in d\u2010dimensional spaces."
    },
    {
        "text": "d can be quite large if your features are, say, all of the pixel values in an image!"
    },
    {
        "text": "in these cases, the performance of these algorithms often starts to break down, and this decay is best understood as a pathology of high\u2010dimensional geometry, the so\u2010called curse of dimensionality."
    },
    {
        "text": "the practical punch line to all of this is that if you want your algorithms to work well, you will usually need some way to cram your data down into a lower\u2010dimensional space."
    },
    {
        "text": "there\u2019s no need to dwell too much on the curse of dimensionality \u2013 thinking about it can hurt the heads up us three\u2010dimensional beings."
    },
    {
        "text": "but if you\u2019re interested, i would like to give you at least a tiny taste of what goes on in high dimensions."
    },
    {
        "text": "basically, the problem is that in high dimensions, different points get very far away from each other."
    },
    {
        "text": "to illustrate, the following code lets us set d as a parameter."
    },
    {
        "text": "it then generates a thousand random points in the unit cube, calculates the distance from every point to every other point, and shows a histogram of those distances for d = 2 and d = 500. import numpy, scipy d = 500 data = numpy.random.uniform( size=d*1000).reshape((1000,d)) distances = scipy.spatial.distance.cdist(data, data) pd.series(distances.reshape(1000000)).hist(bins=50) plt.title(\"dist."
    },
    {
        "text": "between points in r%i\" % d) plt.show() you can see that for d = 500, two points in the cube are almost always about the same distance from each other."
    },
    {
        "text": "if you did a similar simulation with spheres, you would see that almost all the mass of a high\u2010dimensional sphere is in its crust."
    },
    {
        "text": "sounds weird?"
    },
    {
        "text": "well yes, it is."
    },
    {
        "text": "that\u2019s why we reduce dimensions."
    },
    {
        "text": "10.1 the curse of dimensionality 137 40,000 distance between points in r2 distance between points in r500 35,000 30,000 25,000 20,000 15,000 10,000 100,000 150,000 200,000 250,000 300,000 350,000 50,000 0 5000 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0 2 4 6 8 10 12 0"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 138 10.2 \u00adexample: eigenfaces for dimensionality reduction the following script will dive right into a lot of the material that we will cover in this chapter."
    },
    {
        "text": "it loads in a sample data of images of 64 \u00d7 64 pixel faces, with 10 pictures each of 40 different people."
    },
    {
        "text": "that\u2019s d = 64*64 = 4096 dimensions."
    },
    {
        "text": "it then clusters the images, prints out a measure of how distinct the clusters are from each other, and then prints out a measure of how closely the identified clusters line up with the identities of the humans pictured."
    },
    {
        "text": "then the script uses a technique called \u201cprincipal component analysis\u201d (which we will go over in this chapter) to reduce the 4096\u2010dimensional images down to a saner 25 dimensions and redoes the analysis."
    },
    {
        "text": "it finds that the identi- fied clusters match up slightly better with the humans being pictured."
    },
    {
        "text": "import sklearn import sklearn.datasets as datasets from sklearn.decomposition import pca from sklearn.cluster import kmeans from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn import metrics # get data and format it faces_data = datasets.fetch_olivetti_faces() person_ids, image_array = faces_data['target'], faces_ data.images # unroll each 64x64 image -> (64*64) element vector x = image_array.reshape((len(person_ids), 64*64)) # cluster raw data and compare print \"** results from raw data\" model = kmeans(n_clusters=40) model.fit(x) print \"cluster goodness: \", silhouette_score(x, model."
    },
    {
        "text": "labels_) print \"match to faces: \", metrics.adjusted_rand_score( model.labels_, person_ids) # 0.15338 # use pca to print \"** now using pca\" pca = pca(25) # pass in number of components to fit pca.fit(x) x_reduced = pca.transform(x) model_reduced = kmeans(n_clusters=40) model_reduced.fit(x_reduced)"
    },
    {
        "text": "10.2 example: eigenfaces for dimensionality reduction 139 labels_reduced = model_reduced.labels_ print \"cluster goodness: \", \\ silhouette_score(x_reduced, model_reduced.labels_) print \"match to faces: \", metrics.adjusted_rand_score( model_reduced.labels_, person_ids) when i run this script, my output is ** results from raw data cluster goodness: 0.148591 match to faces: 0.454254676789 ** now using pca cluster goodness: 0.230444 match to faces: 0.467292493785 in the interest of visualizing the data itself, you can continue the analysis with the following script in order to get better insight into the pca process."
    },
    {
        "text": "it shows a picture of one of the raw images and then displays the first two so\u2010 called eigenfaces that were identified."
    },
    {
        "text": "pca tries to model every picture in the dataset as a mixture of the most important eigenfaces, so visualizing them can give us an idea of how the faces vary across the dataset."
    },
    {
        "text": "finally, it shows a graph called a \u201cskree plot,\u201d which plots out the importance of the different eigenfaces."
    },
    {
        "text": "# display a random face, to get a feel for the data sample_face = image_array[0,:,:] plt.imshow(sample_face) plt.title(\"sample face\") plt.show() # show eigenface 0 eigenface0 = pca.components_[0,:].reshape((64,64)) plt.imshow(eigenface0) plt.title(\"eigenface 0\") plt.show() eigenface1 = pca.components_[1,:].reshape((64,64)) plt.imshow(eigenface1) plt.title(\"eigenface 1\") plt.show() # skree plot pd.series( pca.explained_variance_ratio_).plot() plt.title(\"skree plot of eigenface importance\") plt.show()"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 140 0 10 20 30 40 50 60 0 10 20 30 40 sample face 50 60 0.25 skree plot of eigenface importance 0.20 0.15 0.10 0.05 0.00 0 5 10 15 eigenface 1 eigenface 0 0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60 0 10 20 30 40 50 60 20 the script generates the following figures: 10.3 \u00adprincipal component analysis and factor analysis the granddaddy of dimensionality reduction algorithms is, without question, principal component analysis or pca."
    },
    {
        "text": "geometrically, pca assumes that your data in d\u2010dimensional space is \u201cfoot- ball shaped\u201d \u2013 an ellipsoidal blob that is stretched out along some axes, narrow in others, and generally free of any massive outliers."
    },
    {
        "text": "take the following image, for example: intuitively, the data is \u201creally\u201d one\u2010dimensional, lying on the line x = y, but there is some random noise that slightly perturbs each point."
    },
    {
        "text": "rather than"
    },
    {
        "text": "10.3 principal component analysis and factor analysis 141 giving the two features x and y for each point, you can get a good approxima- tion with just the single feature x + y. there are two ways to look at this: \u25cf \u25cfintuitively, it seems that x + y might be the real\u2010world feature underlying the data, while x and y are just what we measured."
    },
    {
        "text": "by using x + y, we are extracting a feature that is more meaningful than any of our actual raw features."
    },
    {
        "text": "\u25cf \u25cftechnically, it is more computationally efficient to process one number rather than two."
    },
    {
        "text": "if you wanted, in this case, you could estimate x and y pretty accurately if you only knew x + y. numerically, using one number rather than two lets us shy away from the curse of dimensionality."
    },
    {
        "text": "pca is a way of (1) identifying the \u201ccorrect\u201d features such as x + y that capture most of the structure of the dataset and (2) extracting these features from the raw data points."
    },
    {
        "text": "to be a little more technical, pca takes in a collection of d\u2010dimensional vec- tors and finds a collection of d \u201cprincipal component\u201d vectors of length 1, called p1, p2, ... and pd."
    },
    {
        "text": "a point x in the data can be expressed as x a p a p a p d d = + + + 1 1 2 2 ... however, the pi are chosen so that generally a1 is much larger than the other ai, a2 is larger than a3 and above, etc."
    },
    {
        "text": "so realistically, the first few pi capture most of the variation in the dataset, and x is a linear combination of the first few pi and some small correction terms."
    },
    {
        "text": "the ideal case for pca is something where large swaths of features tend to be highly correlated, such as a photo- graph where adjacent pixels are likely to have similar brightness."
    },
    {
        "text": "so, our exam- ple script was an excellent candidate."
    },
    {
        "text": "the code in our script that performed the pca analysis and reduced the dataset\u2019s dimension is y x"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 142 pca = pca(25) pca.fit(x) x_reduced = pca.transform(x) note that in this case, we have passed the number of components we want to extract as a parameter in pca."
    },
    {
        "text": "under the hood, it\u2019s much more computation- ally efficient to only extract the first few components, so that\u2019s good to do if you don\u2019t need entire pca decomposition."
    },
    {
        "text": "10.4 \u00adskree plots and understanding dimensionality in our motivation for pca, we suggested that the dataset was \u201creally\u201d one\u2010 dimensional and that one of the goals of pca is to extract that \u201creal\u201d dimen- sionality of a dataset by seeing how many components it took to capture most of the dataset\u2019s structure."
    },
    {
        "text": "in reality, it\u2019s rarely that clear."
    },
    {
        "text": "what you get instead is that the first few components are the most important, and then there is a gentle taper into uselessness as you go further out."
    },
    {
        "text": "it is common to plot out the importance of the different principal compo- nents (technically, the importance is measured as the fraction of a dataset\u2019s variance that the component accounts for) in what\u2019s called a \u201cskree plot,\u201d which we generated earlier: 0.25 0.20 0.15 0.10 0.05 0.00 0 5 10 skree plot of eigenface importance 15 20 eyeballing this plot, i would venture that the face dataset is more or less 15 dimensions \u2013 still a lot, but much less than the 4096 dimensions in the raw data."
    },
    {
        "text": "10.6 limitations of pca 143 10.5 \u00adfactor analysis i should note that pca is related to the statistical technique of factor analysis."
    },
    {
        "text": "mathematically, they\u2019re the same thing: you find a change of coordinates where most of the variance in your data exists in the first new coordinate, the second most in the second coordinate, and so on."
    },
    {
        "text": "the divergent terminology is mostly an accident of the fact that they were developed independently and applied in very different ways to different fields."
    },
    {
        "text": "in pca, the idea is generally dimensionality reduction; you find how many of these new coordinates are needed to capture most of your data\u2019s variance, and then you reduce your data points to just those few coordinates."
    },
    {
        "text": "a prototypical application of pca is analyzing pictures of faces: there is a staggering number of dimensions in the data, which are almost entirely redundant, and examining the principal components themselves gives insights into the behavior of the system."
    },
    {
        "text": "factor analysis, on the other hand, is more about identifying causal \u201cfactors\u201d that give rise to the observed data."
    },
    {
        "text": "a major historical example is in the study of intelligence."
    },
    {
        "text": "tests of intelligence in many different areas (math, language, etc.)"
    },
    {
        "text": "were seen to be correlated, suggesting that the same underlying factors could affect intelligence in many different areas."
    },
    {
        "text": "researchers found that a single so\u2010 called g\u2010factor accounts for about half of the variance between different intel- ligence tests."
    },
    {
        "text": "this book generally looks at things from a pca perspective, but you should be aware that both viewpoints are useful."
    },
    {
        "text": "10.6 \u00adlimitations of pca there are three big gotchas when using pca: \u25cf \u25cfyour dimensions all need to be scaled to have comparable standard devia- tions."
    },
    {
        "text": "if you arbitrarily multiplied one of your features by a thousand (maybe by measuring a distance in millimeters rather than in meters, which in prin- ciple does not change the actual content of your data), then pca will con- sider that feature to contribute much more to the dataset\u2019s variance."
    },
    {
        "text": "if you are applying pca to image data, then there is a good chance that this isn\u2019t a big deal, since all pixels are usually scaled the same."
    },
    {
        "text": "but if you are trying to perform pca on demographic data, for example, you have to have some- body\u2019s income and their height measured to the same scale."
    },
    {
        "text": "along with this limitation is the fact that pca is very sensitive to outliers."
    },
    {
        "text": "\u25cf \u25cfpca assumes that your data is linear."
    },
    {
        "text": "if the \u201creal\u201d shape of your dataset is that it\u2019s bent into an arc in high\u2010dimensional space, it will get blurred into several principal components."
    },
    {
        "text": "pca will still be useful for dimensionality reduction, but the components themselves are likely not to be very meaningful."
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 144 \u25cf \u25cfif you are using pca on images of faces or something similar, the key parts of the pictures need to be aligned with each other."
    },
    {
        "text": "pca will be of no use if, for example, the eyes are covered by different pixels."
    },
    {
        "text": "if your pictures are not aligned, doing automatic alignment is outside the skill set of most data scientists."
    },
    {
        "text": "10.7 \u00adclustering clustering is a bit of a dicier issue than using pca."
    },
    {
        "text": "there are several reasons for this, but many of them boil down to the fact that it\u2019s clear what pca is sup- posed to do, but we\u2019re usually not quite sure what we want out of clustering."
    },
    {
        "text": "there is no crisp analytical definition of \u201cgood\u201d clusters; every candidate you might suggest has a variety of very reasonable objections you could raise."
    },
    {
        "text": "the only real metric is whether the cluster reflects some underlying natural seg- mentation, and that\u2019s very hard to assess: if you already know the natural seg- ments, then why are you clustering?"
    },
    {
        "text": "to give you an idea of what we\u2019re up against, here are some of the questions to keep in the back of your mind \u25cf \u25cfwhat if our points fall on a continuum?"
    },
    {
        "text": "this fundamentally baffles the notion of a cluster."
    },
    {
        "text": "how do i want to deal with that?"
    },
    {
        "text": "\u25cf \u25cfshould clusters be able to overlap?"
    },
    {
        "text": "\u25cf \u25cfdo i want my clusters to be nice little compact balls?"
    },
    {
        "text": "or, would i allow some- thing such as a doughnut?"
    },
    {
        "text": "10.7.1 real\u2010world assessment of clusters i will talk later about some analytical methods for assessing the quality of the clusters you found."
    },
    {
        "text": "one of the most useful of those is the rand index, which allows you to compare your clusters against some known ground truth for what the clusters ought to be."
    },
    {
        "text": "that\u2019s what i used in the aforementioned sample script."
    },
    {
        "text": "usually though, you don\u2019t have any ground truth available, and the question of what exactly constitutes a good cluster is completely open\u2010ended."
    },
    {
        "text": "in this case, i recommend a battery of sanity checks."
    },
    {
        "text": "some of my favorite include the following: \u25cf \u25cffor each cluster, calculate some summary statistics of it based on features that were not used as input to the clustering."
    },
    {
        "text": "if your clusters really corre- spond to distinct things in the real world, then they should differ in ways that they were not clustered on."
    },
    {
        "text": "\u25cf \u25cftake some random samples from the different clusters and examine them by hand."
    },
    {
        "text": "do the samples from different clusters seem plausibly different?"
    },
    {
        "text": "10.7 clustering 145 \u25cf \u25cfif your data is high\u2010dimensional, use pca to project it down to just two dimensions and do a scatterplot."
    },
    {
        "text": "do the clusters look distinct?"
    },
    {
        "text": "this is espe- cially useful if you were able to give a real\u2010world interpretation to the princi- pal components."
    },
    {
        "text": "\u25cf \u25cfscrew pca."
    },
    {
        "text": "pick two features from the data that you care about and do a scatterplot on just those two dimensions."
    },
    {
        "text": "do the clusters look reasonable?"
    },
    {
        "text": "\u25cf \u25cftry a different clustering algorithm."
    },
    {
        "text": "do you get similar clusters?"
    },
    {
        "text": "\u25cf \u25cfredo the clustering on a random subset of your data."
    },
    {
        "text": "do you still get similar clusters?"
    },
    {
        "text": "another big thing to keep in mind is whether it is important to be able, in the future, to assign new points to one of the clusters we have found."
    },
    {
        "text": "in some algorithms, there are crisp criteria for which cluster a point is in, so it\u2019s ease to label new points that we didn\u2019t train on."
    },
    {
        "text": "in other algorithms though, a cluster is defined by the points contained in it, and assigning a new point to a cluster requires reclustering the entire dataset (or doing some kind of a clever hack that\u2019s tantamount to that)."
    },
    {
        "text": "10.7.2 k\u2010means clustering the k\u2010means algorithm is one of the simplest techniques to understand, imple- ment, and use."
    },
    {
        "text": "it starts with vectors in d\u2010dimensional space, and the idea is to partition them into compact, nonoverlapping clusters."
    },
    {
        "text": "i say again: the pre- sumed clusters are compact (not loops, not super elongated, etc.)"
    },
    {
        "text": "and not overlapping."
    },
    {
        "text": "the classical algorithm to compute the clusters is quite simple."
    },
    {
        "text": "you start off with k\u2010cluster centers, then iteratively assign each data point to its closest clus- ter center, and recompute the new cluster centers."
    },
    {
        "text": "here is the pseudocode: 1. start off with k initial cluster centers."
    },
    {
        "text": "2. assign each point to the cluster center that it\u2019s closest to."
    },
    {
        "text": "3. for each cluster, recompute its center as the average of all its assigned points."
    },
    {
        "text": "4. repeat 2 and 3 until some stopping criterion is met."
    },
    {
        "text": "there are clever ways to initialize the clusters if they are not present at the beginning and to establish when they have become stable enough to stop, but otherwise the algorithm is very straightforward."
    },
    {
        "text": "in scikit\u2010learn, the code to perform the clustering looks as the following: from sklearn.cluster import kmeans model = kmeans(n_clusters=k) model.fit(my_data)"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 146 labels = model.labels_ cluster_centers = model.cluster_centers_ k\u2010means clustering has an interesting property (sometimes a benefit) that when k is larger than the \u201cactual\u201d number of clusters in the data, you will split a large \u201creal\u201d cluster into several computed ones."
    },
    {
        "text": "in this case, k\u2010means cluster- ing is less of a way to identify clusters and more of a way to partition your dataset into \u201cnatural\u201d regions, as shown in this figure where we set k = 3, but there were really two clusters: situations such as this can often be found using the silhouette score to find clusters that are not very distinct."
    },
    {
        "text": "the results of k\u2010means clustering are extremely easy to apply to new data; you simply compare a new data point to each of the cluster centers and assign it to the one that it\u2019s closest to."
    },
    {
        "text": "an important caveat about k\u2010means is that there is no guarantee about finding optimal clusters in any sense."
    },
    {
        "text": "for this reason, it is common to restart it several times with different, random initial cluster centers."
    },
    {
        "text": "scikit\u2010learn does this by default."
    },
    {
        "text": "10.7.3 gaussian mixture models a key feature of most clustering algorithms is that every point is assigned to a single cluster."
    },
    {
        "text": "but realistically, many datasets contain a large gray area, and mixture models are a way to capture that."
    },
    {
        "text": "you can think of gaussian mixture models (gmms) as a version of k\u2010means that captures the notion of a gray area and gives confidence levels whenever it assigns a point to a particular cluster."
    },
    {
        "text": "each cluster is modeled as a multivariate gaussian distribution, and the model is specified by giving the following: 1) the number of clusters"
    },
    {
        "text": "10.7 clustering 147 2) the fraction of all data points that are in each cluster 3) each cluster\u2019s mean and its d\u2010by\u2010d covariance matrix."
    },
    {
        "text": "when training a gmm, the computer keeps a running confidence level of how likely each point is to be in each cluster, and it never decides them defini- tively: the mean and standard deviation for a cluster will be influenced by every point in the training data, but in proportion to how likely they are to be in that cluster."
    },
    {
        "text": "when it comes time to cluster a new point, you get out a confidence level for every cluster in your model."
    },
    {
        "text": "mixture models have many of the same blessings and curses of k\u2010means."
    },
    {
        "text": "they are simple to understand and implement."
    },
    {
        "text": "the computational costs are very light and can be done in a distributed way."
    },
    {
        "text": "they provide clear, under- standable output that can be easily used to cluster additional points in the future."
    },
    {
        "text": "on the other hand, they both assume more\u2010or\u2010less convex clusters, and they are both liable to fall into local minimums when training."
    },
    {
        "text": "in scikit\u2010learn, the code to fit a gmm looks such as the following: from sklearn import mixture model = mixture.gmm(n_components=5) model.fit(my_data) cluster_means = model.means_ # array giving the weight of each datapoint for each of the clusters labels = model.predict_proba(my_date) i should also note that gmms are the most popular instance of a large family of mixture models."
    },
    {
        "text": "you could equally well have used something other than gaussians as the models for your underlying clusters or even had some clusters be gaussians and others something else."
    },
    {
        "text": "most mixture model librar- ies use gaussian, but under the hood, they are all trained with something called the em algorithm, and it is agnostic to the distribution being modeled."
    },
    {
        "text": "10.7.4 agglomerative clustering hierarchical clustering is a general class of algorithms sharing a common structure."
    },
    {
        "text": "we start off with a large number of small clusters, typically with each point being its own cluster."
    },
    {
        "text": "we then successively merge clusters together until they form a single giant cluster."
    },
    {
        "text": "so, the output isn\u2019t a single clustering of the data, but rather a hierarchy of potential clusterings."
    },
    {
        "text": "how you choose which clusters to merge and how we find the \u201chappy medium\u201d clustering in the hier- archy determine the specifics of the algorithm."
    },
    {
        "text": "an advantage of hierarchical clustering over k\u2010means is that (depending on how you choose to merge your clusters) your clusters can be of any size or"
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 148 shape."
    },
    {
        "text": "a disadvantage though is that there\u2019s no natural way to assign a new point to an existing cluster."
    },
    {
        "text": "the following python code will perform an agglomerative clustering of the data until five clusters remain, merging, and assigning each point to one of the clusters."
    },
    {
        "text": "\u201cward\u201d linkage means that we pick which clusters to merge by looking for the two clusters with the lowest variance between them."
    },
    {
        "text": "from sklearn.cluster import agglomerativeclustering clst = agglomerativeclustering( n_clusters=5, linkage=\u2019ward\u2019) cluster_labels = clst.fit_predict(my_data) 10.7.5 evaluating cluster quality algorithmic methods to evaluate the outcome of clustering come in two major varieties."
    },
    {
        "text": "first, there are the supervised ones, where we have some ground\u2010 truth knowledge about what the \u201cright\u201d clusters are, and we see how closely the clusters we found match up to them."
    },
    {
        "text": "then, there are the unsupervised ones, where we think of the points as vectors in d\u2010dimensional space and look at how geometrically distinct the clusters are from each other."
    },
    {
        "text": "10.7.6 siihouette score silhouette scores are the most common unsupervised method you\u2019ll see, and they are ideal for scoring the output of k\u2010means clustering."
    },
    {
        "text": "it is based on the intuition that clusters should be dense and widely separated from each other, so similar to k\u2010means, it works best with dense, compact clusters that are all of comparable size."
    },
    {
        "text": "silhouette scores aren\u2019t applicable to things such as a dough- nut\u2010shaped cluster with a compact one in the middle."
    },
    {
        "text": "specifically, every point is given a \u201csilhouette coefficient\u201d defined in terms of \u25cf \u25cfa = the average distance between the point and all other points in the same cluster \u25cf \u25cfb = the average distance between the point and all other points in the next\u2010 closest cluster."
    },
    {
        "text": "the silhouette coefficient is then defined as s b a a b = \u2212 ( ) max , the coefficient is always between \u20131 and 1. if it is near 1, this means that b is much larger than a, that is, the point is on average much closer to points in its"
    },
    {
        "text": "10.7 clustering 149 own cluster."
    },
    {
        "text": "a score near 0 suggests that the point is equidistant from the two clusters, that is, they overlap in space."
    },
    {
        "text": "a negative score suggests that the point is wrongly clustered."
    },
    {
        "text": "the silhouette score for a whole cluster is the average coefficient over all points in the cluster."
    },
    {
        "text": "the silhouette score is far from perfect."
    },
    {
        "text": "for example, imagine that one clus- ter is much larger than another and that they are very close, as in the following figure."
    },
    {
        "text": "the point indicated is clearly in the right cluster, but because its cluster is so large, it is far away from most of its clustermates."
    },
    {
        "text": "this will give it a poor silhouette score because it is closer, on average, to the points in the nearby cluster than its own cluster."
    },
    {
        "text": "however, the silhouette score is straightforward to compute and easy to understand, and you should consider using it if your cluster scheme rests on similar assumptions."
    },
    {
        "text": "the silhouette score is built into scikit\u2010learn and can be called in the follow- ing way: from sklearn.metrics import silhouette_score from sklearn.metrics import silhouette_samples coeffs_for_each_point = silhouette_samples(mydata, labels) avg_coeff = silhouette_score(mydata, labels) 10.7.7 rand index and adjusted rand index the rand index is useful when we have knowledge of the correct clusters for at least some of the points."
    },
    {
        "text": "it doesn\u2019t try to match up the clusters we found to the right ones we know."
    },
    {
        "text": "instead, it is based on the idea of whether two points that should be in the same cluster are, indeed, in the same cluster."
    },
    {
        "text": "10 unsupervised learning: clustering and dimensionality reduction 150 a pair of points (x, y) is said to be \u201ccorrectly clustered\u201d if we put x and y in the same cluster and our ground truth also has them in the same cluster."
    },
    {
        "text": "the pair is also correctly clustered if we put x and y in different clusters and our ground truth has them in different clusters."
    },
    {
        "text": "if there are n points for which we know the right cluster, there are n(n \u2212 1)/2 different pairs of such points."
    },
    {
        "text": "the rand index is the fraction of all such points that are correctly clustered."
    },
    {
        "text": "it ranges from 0 to 1, with 1 meaning that every single pair of points is incor- rectly clustered."
    },
    {
        "text": "the problem with the rand index is that even if we cluster points randomly, we will get some pairs correct by chance and have a score greater than 0. in fact, the average score will depend on the relative sizes of the correct clusters; if there are many clusters and they\u2019re all small, then most pairs of points will, by dumb luck, be correctly assigned to different clusters."
    },
    {
        "text": "the \u201cadjusted rand index\u201d solves this problem by looking at the sizes of the identified clusters and the size of the ground\u2010truth clusters."
    },
    {
        "text": "it then looks at the range of rand indices possible given those sizes and scales the rand index so that it is 0 on average if the cluster assignments are random and still maxes out at 1 if the match is perfect."
    },
    {
        "text": "code for the rand index is as follows: from sklearn import metrics labels_true = [0, 0, 0, 1, 1, 1] labels_pred = [0, 0, 1, 1, 2, 2] metrics.adjusted_rand_score(labels_true, labels_pred) 10.7.8 mutual information another supervised cluster quality metric is mutual information."
    },
    {
        "text": "mutual information is a concept from information theory and is similar to correlation, except that it applies to categorical variables instead of numerical ones."
    },
    {
        "text": "in the context of clustering, the idea is that if you pick a random data point from your training data, you get two random variables: the ground\u2010truth cluster that the point should be in and the identified cluster that it was assigned to."
    },
    {
        "text": "the ques- tion is how good a guess you can make about one of these variables if you only know the other one."
    },
    {
        "text": "if these probability distributions are independent, then the mutual information will 0, and if either can be perfectly inferred from the other, then you get the entropy of the distribution."
    },
    {
        "text": "the mutual information score is available in scikit\u2010learn as the following: from sklearn import metrics labels_true = [0, 0, 0, 1, 1, 1] labels_pred = [0, 0, 1, 1, 2, 2] metrics.mutual_info_score(labels_true, labels_pred)"
    },
    {
        "text": "10.9 glossary 151 10.8 \u00adfurther reading 1 bishop, c, pattern recognition and machine learning, 2007, springer, new york, ny."
    },
    {
        "text": "2 scikit\u2010learn 0.171.1 documentation, http://scikit\u2010learn.org/stable/index.html, viewed 7 august 2016, the python software foundation."
    },
    {
        "text": "10.9 \u00adglossary adjusted rand index a variant of the rand index that is 0 when the that is 0 when the rand index is no better than chance."
    },
    {
        "text": "agglomerative clustering a clustering method where you look for two clusters (which could be just single points) that are close to each other and then merge them into a single cluster."
    },
    {
        "text": "you do this until some stopping criterion is reached."
    },
    {
        "text": "eigenface when using pca on images of faces, the eigenfaces are the principal components."
    },
    {
        "text": "when viewed as images, they are usually ghostly pseudo\u2010faces."
    },
    {
        "text": "gaussian mixture model a mixture model where all clusters are modeled as gaussian distributions."
    },
    {
        "text": "k\u2010means clustering probably, the most popular clustering method."
    },
    {
        "text": "it assumes compact, nonoverlapping clusters."
    },
    {
        "text": "it works well in practice, is efficient to train, and can easily be used to clustering other points in the future."
    },
    {
        "text": "mixture model a probability model used in clustering."
    },
    {
        "text": "every cluster is modeled as a probability distribution that its points are drawn from."
    },
    {
        "text": "this makes it plausible for a point in your dataset to be in the gray area between two clusters, where it could plausibly have come from either."
    },
    {
        "text": "mutual information a measure of \u201ccorrelation\u201d between two probability distributions that can be used as a measure of how well a set of clusters corresponds to known ground\u2010truth clusters principal component analysis a dimensionality reduction technique where you express your input data points, to a good approximation, as linear combinations of several \u201cprincipal component\u201d vectors."
    },
    {
        "text": "examining the components themselves can give insights into the dataset."
    },
    {
        "text": "pca popular shorthand for principal component analysis rand index a measure of how well a set of clusters corresponds to known ground\u2010truth clusters."
    },
    {
        "text": "silhouette score a measure of how well compact and distinct from each other a collection of clusters is."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 153 11 regression is similar to classification: you have a number of input features, and you want to predict an output feature."
    },
    {
        "text": "in classification, this output feature is either binary or categorical."
    },
    {
        "text": "with regression, it is a real\u2010valued number."
    },
    {
        "text": "typically, regression algorithms fall into two categories: \u25cf \u25cfmodeling the output as a linear combination of the inputs."
    },
    {
        "text": "there is a ton of elegant math here and principled ways to handle data pathologies."
    },
    {
        "text": "\u25cf \u25cfugly hacks to deal with anything nonlinear."
    },
    {
        "text": "this chapter will review several of the more popular regression techniques in machine learning, along with some techniques for assessing how well they performed."
    },
    {
        "text": "i have made the unconventional decision to include fitting a line (or other curves) to two\u2010dimensional data within the chapter on regression."
    },
    {
        "text": "you usually don\u2019t see curve fitting in the context of machine learning regression, but they\u2019re really the same thing mathematically: you assume some functional form for the output as a function of the inputs (such as y = m1x1 + m2x2, where xi are inputs and mi are parameters that you set to whatever you want), and then you choose the parameters to line up as well as possible (however, you define \u201cas well as possible\u201d) with your training data."
    },
    {
        "text": "the distinction between them is a historical accident; fitting a curve to data was developed long before machine learning and even before computers."
    },
    {
        "text": "11.1 \u00adexample: predicting diabetes progression the following script uses a dataset describing physiological measurements taken from 442 diabetes patients, with the target variable being an indicator of the progression of their disease."
    },
    {
        "text": "after the script, comes the images it generates."
    },
    {
        "text": "import sklearn.datasets import pandas as pd regression"
    },
    {
        "text": "11 regression 154 from matplotlib import pyplot as plt from sklearn.cross_validation import train_test_split from sklearn.linear_model import\\ linearregression, lasso from sklearn.preprocessing import normalize from sklearn.metrics import r2_score diabetes = sklearn.datasets.load_diabetes() x, y = normalize(diabetes['data']), diabetes['target'] x_train, x_test, y_train, y_test = \\ train_test_split(x, y, test_size=.8) linear = linearregression() linear.fit(x_train, y_train) preds_linear = linear.predict(x_test) corr_linear = round(pd.series(preds_linear).corr( pd.series(y_test)), 3) rsquared_linear = r2_score(y_test, preds_linear) print(\"linear coefficients:\") print(linear.coef_) plt.scatter(preds_linear, y_test) plt.title(\"lin."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr=%f rsq=%f\" % (corr_linear, rsquared_linear)) plt.xlabel(\"predicted\") plt.ylabel(\"actual\") # add x=y line for comparison plt.plot(y_test, y_test, \u2018k--\u2019) plt.show() lasso = lasso() lasso.fit(x_train, y_train) preds_lasso = lasso.predict(x_test) corr_lasso = round(pd.series(preds_lasso).corr( pd.series(y_test)), 3) rsquared_lasso = round( r2_score(y_test, preds_lasso), 3) print(\"lasso coefficients:\") print(lasso.coef_) plt.scatter(preds_lasso, y_test) plt.title(\"lasso."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr=%f rsq=%f\" % (corr_lasso, rsquared_lasso)) plt.xlabel(\"predicted\") plt.ylabel(\"actual\") # add x=y line for comparison plt.plot(y_test, y_test, \u2018k--\u2019) plt.show()"
    },
    {
        "text": "11.1 example: predicting diabetes progression 155 400 350 300 250 200 150 100 50 0 400 350 300 250 200 150 100 50 0 0 50 100 150 200 predicted actual actual lasso."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr = 0.677000 rsq = 0.445000 lin."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr = 0.660000 rsq = 0.396544 250 300 350 400 0 50 100 150 200 predicted 250 300 350 400"
    },
    {
        "text": "11 regression 156 11.2 \u00adleast squares the simplest example of regression is one that you probably saw in high school: fitting a line to data."
    },
    {
        "text": "you have a collection of x/y pairs, and you try to fit a line to them of the form y mx b = + i remember back in school being encouraged to plot the points out, fit a line to them by eye, trace the line with a ruler, and use that to pull out m and b. on some level, i still think that\u2019s the best way to do it, because the human eye can account for outliers and instantly notices data pathologies."
    },
    {
        "text": "recall anscombe\u2019s quartet, where each of the four datasets has the same line of best fit and the same quality of fit, at least using the standard methods: 14 12 10 8 6 4 2 2 4 6 8 10 12 14 16 18 20 14 anscombe\u2019s quartet 12 10 8 6 4 22 4 6 8 10 12 14 16 18 20 14 12 10 8 6 4 2 2 4 6 8 10 12 14 16 18 20 14 12 10 8 6 4 2 2 4 6 8 10 12 14 16 18 20 however, we also need an objective way to pull out a number and one that can be done by a computer without human intervention."
    },
    {
        "text": "the standard way to fit a line is called least squares."
    },
    {
        "text": "in python, it can be fit using the linear regression class in the example scripts, and the fit coefficients can be found in the following way: >>> import numpy as np >>> x = np.array([[0.0],[1.0],[2.0]])"
    },
    {
        "text": "11.3 fitting nonlinear curves 157 >>> y = np.array([1.0,2.0,2.9]) >>> lm = linearregression().fit(x, y) >>> lm.coef_ # m array([ 0.95]) >>> lm.intercept_ # b 1.0166666666666671 least squares works by picking the values of m and b that minimize the \u201cpen- alty function,\u201d which adds up an error term across all of the points: l y mx b i i i = \u2212 + ( ) ( ) \u2211 2 the key thing to understand here is that this penalty function makes least\u2010 squares regression extremely sensitive to outliers in the data: three deviations of size 5 will give a penalty of 75, but just a single larger deviation of size 10 will give the larger penalty of size 100. linear regression will bend the parameters so as to avoid large deviations of even a single point, which makes it unsuitable in situations where a handful of large deviations are to be expected."
    },
    {
        "text": "an alternative approach that is more suitable to data with outliers is to use the penalty function l y mx b i i i = \u2212 + ( ) \u2211 where we just take the absolute values of the different error terms and add them."
    },
    {
        "text": "this is called \u201cl1 regression,\u201d among other names."
    },
    {
        "text": "outliers will still have an impact, but it is not as egregious as with least squares."
    },
    {
        "text": "on the other hand, l1 regression penalizes small deviations from expectation more harshly com- pared to least squares, and it is significantly more complicated to implement computationally."
    },
    {
        "text": "11.3 \u00adfitting nonlinear curves fitting a curve to data is a ubiquitous problem not just in data science but in engineering and the sciences in general."
    },
    {
        "text": "often, there are good a priori reasons that we expect a certain functional form, and extracting the best\u2010fit parameters will tell us something very meaningful about the system we are studying."
    },
    {
        "text": "a few examples that i\u2019ve seen include the following: \u25cf \u25cfexponential decay to some baseline."
    },
    {
        "text": "this is useful for modeling many pro- cesses where a system starts in some kind of agitated state and decays to a baseline y ae c bx = + \u2212"
    },
    {
        "text": "11 regression 158 \u25cf \u25cfexponential growth y aebx = \u25cf \u25cflogistic growth, which is useful in biology for modeling the population den- sity of organisms growing in a constrained environment that can only sup- port so many individuals: y a e c e bx bx = + \u25cf \u25cfpolynomials of various degrees y a a x a x = + + 0 1 2 2 least squares is the typical approach in all of these cases, where we pick the parameters to as to minimize the penalty function l y f x i i i = \u2212( ) ( ) \u2211 2 in python, the way to do general least\u2010squares fitting is with the curve_fit function, shown in the following code."
    },
    {
        "text": "it takes as its first argument a user\u2010 defined function, which takes in x and some number of additional parameters (two parameters in this code), uses those parameters to calculate some func- tion of x, and returns the value."
    },
    {
        "text": "the next arguments are the x values and y values of the data we have."
    },
    {
        "text": "then curve_fit, through a process of trial\u2010and\u2010error called optimization (which i\u2019ll talk about later in the book) tries to find the values of the additional parameters that will minimize the error term for the given x and y values."
    },
    {
        "text": "it returns a tuple of two things: the best\u2010fitted parameters and a matrix that estimates how much they vary."
    },
    {
        "text": "the following script creates some data of the form y = 2 + 3x2, adds some noise to it, and then uses curve_fit to fit a curve of the form y = a + bx2 to the data."
    },
    {
        "text": "from scipy.optimize import curve_fit xs = np.array([1.0, 2.0, 3.0, 4.0]) ys = 2.0 + 3.0 *xs*xs + 0.2*np.random.uniform(3) def calc(x, a, b): return a + b*x*x cf = curve_fit(calc, xs, ys) best_fit_params = cf[0] when i ran it on my computer, it found a = 2.33677376 and b = 3 \u2212 a pretty good match."
    },
    {
        "text": "11.4 goodness of fit: r2 and correlation 159 i should note that, computationally, doing nonlinear fits such as this is extremely slow, and the numerical algorithms can sometimes go horribly awry and give incorrect results."
    },
    {
        "text": "the best way to address this, if you can, is to trans- form your problem into a linear one by fitting a line to some function of your data (such as the log)."
    },
    {
        "text": "if that is not possible, you can often improve the perfor- mance by inputting an initial guess as an optional parameter: in curve_fit, that optional argument is called p0."
    },
    {
        "text": "11.4 \u00adgoodness of fit: r2 and correlation when you are assessing the quality of a fitted curve, there are two questions we want to answer: \u25cf \u25cfhow accurately can we predict values?"
    },
    {
        "text": "\u25cf \u25cfwe assumed that the data followed some functional form."
    },
    {
        "text": "was that even a good assumption?"
    },
    {
        "text": "the standard way to answer the first of the questions is called r2, pronounced \u201cr squared.\u201d r2 is often described as the fraction of the variance that is accounted for by the model."
    },
    {
        "text": "a value of 1.0 means a perfect match, and a value of 0 means you didn\u2019t capture any of the variation."
    },
    {
        "text": "in some cases (there are a few different definitions of r2 floating around), it can even take on negative values."
    },
    {
        "text": "if you want to get a bit more detailed, the calculation of r2 is based on two concepts: \u25cf \u25cfthe total variation: tv = \u2212 \u2211 i iy y 2 where y is the average of all the y values in your data."
    },
    {
        "text": "\u25cf \u25cfthe residual variation: rv = \u2212( ) \u2211 i i i y f x 2 these allow us to say, in a precise sense, that your fitted model accounts for a certain percentage of the variation in the data."
    },
    {
        "text": "the definition of r2 is then r2 = \u2212( ) 1 rv tv and you can see it as the fraction of all variation that is captured by the model."
    },
    {
        "text": "of course, taking the squares of the residuals isn\u2019t necessarily the \u201cright\u201d way to quantify variation, but it is the most standard option."
    },
    {
        "text": "11 regression 160 despite looking like a square, technically r2 can be negative if your model is truly abysmal."
    },
    {
        "text": "having r2 = 0 is what you would see if you just defined your fit- ted function to return the average of y as a constant value: f x y ( ) = you can think of this as the crudest way to fit a function to data."
    },
    {
        "text": "do any worse than this, and your r2 score will go negative."
    },
    {
        "text": "in the example script at the beginning of the chapter, the relevant lines for r squared were from sklearn.metrics import r2_score rsquared_linear = r2_score(y_test, preds_linear) another way to quantify your goodness\u2010of\u2010fit is to simply take the correla- tion between your predicted values and the known values in the test data."
    },
    {
        "text": "this has the advantage that you can use pearson, spearman, or kendall correlation, depending on how you want to deal with outliers."
    },
    {
        "text": "on the other hand though, correlation just measures whether your predictions and target values are related; it doesn\u2019t measure whether they actually match up."
    },
    {
        "text": "11.5 \u00adcorrelation of residuals the main ways to measure goodness\u2010of\u2010fit in regression situations are r2 and correlation between predictions and targets."
    },
    {
        "text": "you typically don\u2019t see people ask- ing about whether the functional form being assumed was actually the \u201ccor- rect\u201d form."
    },
    {
        "text": "if you are fitting two\u2010dimensional data though, this question can be addressed as well."
    },
    {
        "text": "the simplest way to assess the quality of our model form is to plot the known data against a curve of the predicted values."
    },
    {
        "text": "do they match up?"
    },
    {
        "text": "in anscombe\u2019s quartet, for example, it is visually clear that a linear model is the correct way to approach the first dataset, but the wrong way to approach the second one."
    },
    {
        "text": "a way to quantify this relationship is the so\u2010called correlation of residuals."
    },
    {
        "text": "intuitively, if our model form is correct, the observed data should be our best\u2010 fit formula plus some random noise."
    },
    {
        "text": "in that case, the actual data would be randomly above or below our curve."
    },
    {
        "text": "on the other hand, if our model form was bad, we would expect long stretches where the data was systematically higher or lower than our curve."
    },
    {
        "text": "this suggests that we look across our data points, sorted by x, and calculate the correlation between the consecutive residuals."
    },
    {
        "text": "a correlation near 0 suggests that our model form was good, and any failure of its predictive power comes from true noise in the data, rather than a failure to pick the right functional form."
    },
    {
        "text": "11.6 linear regression 161 11.6 \u00adlinear regression now let\u2019s move on from fitting a curve and into topics that fit more firmly under the \u201cmachine learning\u201d umbrella."
    },
    {
        "text": "first up: linear regression."
    },
    {
        "text": "linear regression is the same process as fitting a line to data, except that we say y b m x m x m x d d = + + + + 1 1 2 2 where d is the number of input features we have."
    },
    {
        "text": "most of the previous sections carry over directly to this more general case: we fit the data using least squares, we quantify performance using r2, and we can also use correlation between predicted and actual values."
    },
    {
        "text": "the first big difference is that it\u2019s no longer practical to plot the predicted curve against the actual data points."
    },
    {
        "text": "what you can do instead is to make a scatterplot between the known test values and the values predicted for those test data points."
    },
    {
        "text": "this allows us to gauge whether our model performs better for larger or smaller values and whether it suffers from major outliers."
    },
    {
        "text": "to illus- trate, the aforementioned example script will generate this figure for the linear regression model: 400 350 300 250 200 150 100 50 0 0 50 100 150 200 predicted actual lin."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr = 0.660000 rsq = 0.396544 250 300 350 400"
    },
    {
        "text": "11 regression 162 we can see that there is a clear correlation between the predicted and actual numbers, but it is fairly tenuous."
    },
    {
        "text": "in particular, we can see that there are a num- ber of data points where the actual value was substantially below our predic- tions, that is, their diabetes was significantly less damaging than we would have guessed based on the other measurements."
    },
    {
        "text": "in fact, the fit line as a whole looks slightly more shallow than the data itself."
    },
    {
        "text": "together, these suggest that there are a number of anomalously low data points, which are pulling our overall predic- tions lower than perhaps they should be."
    },
    {
        "text": "the other thing that we can do with linear regression is use it to identify features in the data that are particularly interesting."
    },
    {
        "text": "in the example script, we used the normalize() function to scale all the features so that they had mean 0 and standard deviation 1. this means that, by looking at the relative size of their weights in the linear model, we can get a sense of how related they are to the progression of diabetes."
    },
    {
        "text": "in the example script, i print out the coefficients as the following: >>> print(linear.coef_) [-28.12698694 -33.32069944 85.46294936 70.47966698 -37.66512686 20.59488356 -14.6726611 33.10813747 43.68434357 -5.50529361] this suggests that the third and fourth features are particularly interesting, if we want to zero in on and examine their relationship to diabetes more closely."
    },
    {
        "text": "11.7 \u00adlasso regression and feature selection look at the coefficients in the linear regression again."
    },
    {
        "text": "we are able to identify several features as being more promising than the other as targets of further investigation, but the painful truth is that all of the coefficients except that last one are pretty big."
    },
    {
        "text": "there are two problems with this: \u25cf \u25cfit makes it harder to pinpoint exactly which features are the most interesting."
    },
    {
        "text": "\u25cf \u25cfthere is a very good chance that the data are overfitted."
    },
    {
        "text": "many of the moder- ate\u2010sized coefficients could be set so that they balance each other out, yield- ing a slightly better fit on the training data itself but generalizing very poorly."
    },
    {
        "text": "the idea of lasso regression is that we still fit a linear model to the data, but we want to penalize nonzero weights."
    },
    {
        "text": "a lasso regression model takes in a parameter called alpha, which indi- cates how severely nonzero weights should be punished."
    },
    {
        "text": "setting alpha to 0"
    },
    {
        "text": "11.7 lasso regression and feature selection 163 will reduce to linear regression."
    },
    {
        "text": "the default value, which was used in the script, is 1.0. the sample script produces the same scatterplot and performance metrics that were created for linear regression."
    },
    {
        "text": "we can see that the predicted/actual scatterplot hugs the middle line a little more closely, suggesting a better fit."
    },
    {
        "text": "this eyeballing is borne out by the higher r2 value and correlation."
    },
    {
        "text": "the linear model was indeed overfitting the data."
    },
    {
        "text": "400 350 300 250 200 150 100 50 0 actual lasso."
    },
    {
        "text": "reg."
    },
    {
        "text": "corr = 0.677000 rsq = 0.445000 0 50 100 150 200 predicted 250 300 350 400 the different between linear and lasso jumps out when we look at the fitted coefficients: >>> print(lasso.coef_) [ -0."
    },
    {
        "text": "-11.49747021 73.20707164 37.75257628 0."
    },
    {
        "text": "0."
    },
    {
        "text": "-10.36895667 3.70576596 24.17976499 0. ]"
    },
    {
        "text": "four of the six features have weights of precisely 0. of the remaining fea- tures, it is clear that the third is the most relevant to diabetes progression, fol- lowed by the fourth and the ninth."
    },
    {
        "text": "11 regression 164 11.8 \u00adfurther reading bishop, c, pattern recognition and machine learning, 2007, springer, new york, ny."
    },
    {
        "text": "scikit\u2010learn 0.171.1 documentation, http://scikit\u2010learn.org/stable/index.html, viewed 7 august 2016, the python software foundation."
    },
    {
        "text": "11.9 \u00adglossary l1 penalty a regression method where we tune our model parameters so as to minimize the sum of the absolute values of the residuals."
    },
    {
        "text": "this method is more robust to outliers than least squares."
    },
    {
        "text": "least squares a regression method where we tune our model parameters so as to minimize the sum of the squares of the residuals."
    },
    {
        "text": "this is the most standard best\u2010fit method."
    },
    {
        "text": "r squared a measure of how well a regression model fits the data."
    },
    {
        "text": "it is the fraction of all the test data\u2019s variance that is accounted for by the model."
    },
    {
        "text": "residual the difference between the value predicted for a data point and the actual observed value."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 165 12 coming from a background of academic physics, my first years in data science were one big exercise in discovering new data formats that i probably should have already known about."
    },
    {
        "text": "it was a bit demoralizing at the time, so let me make something clear upfront: people are always dreaming up new data types and formats, and you will forever be playing catch\u2010up on them."
    },
    {
        "text": "however, there are several formats that are common enough you should know them."
    },
    {
        "text": "it seems that every new format that comes out is easily understood as a variation of a previ- ous format, so you\u2019ll be on good footing going forward."
    },
    {
        "text": "there are also some broad principles that underlie all formats, and i hope to give you a flavor of them."
    },
    {
        "text": "first, i will talk about specific file formats that you are likely to encounter as a data scientist."
    },
    {
        "text": "this will include sample code for parsing them, discussions about when they are useful, and some thoughts about the future of data formats."
    },
    {
        "text": "for the second half of the chapter, i will switch gears to a discussion of how data is laid out in the physical memory of a computer."
    },
    {
        "text": "this will involve peaking under the hood of the computer to look at performance considerations and give you a deeper understanding of the file formats we just discussed."
    },
    {
        "text": "this section will come in handy when you are dealing with particularly gnarly data pathologies or writing code that aims for speed when you are chugging through a dataset."
    },
    {
        "text": "12.1 \u00adtypical file format categories there are many, many different specific file formats out there."
    },
    {
        "text": "however, they fall under several broad categories."
    },
    {
        "text": "this section will go over the most important ones for a data scientist."
    },
    {
        "text": "the list is not exhaustive, and neither are the categories mutually exclusive, but this should give you a broad lay of the land."
    },
    {
        "text": "data encodings and file formats"
    },
    {
        "text": "12 data encodings and file formats 166 12.1.1 text files most raw data files seen by data scientists are, at least in my experience, text files."
    },
    {
        "text": "this is the most common format for csv files, json, xml and web pages."
    },
    {
        "text": "pulls from databases, data from the web, and log files generated by machines are all typically text."
    },
    {
        "text": "the advantage of a text file is that it is readable by a human being, meaning that it is very easy to write scripts that generate it or parse it."
    },
    {
        "text": "text files work best for data with a relatively simple format."
    },
    {
        "text": "there are limitations though."
    },
    {
        "text": "in particular, text is a notoriously inefficient way to store numbers."
    },
    {
        "text": "the string \"938238234232425123\" takes up 18 bytes, but the number it represents would be stored in memory as 8 bytes."
    },
    {
        "text": "not only is this a price to pay for storage, but the number must be converted from text to a new format before a machine can operate on it."
    },
    {
        "text": "12.1.2 dense numerical arrays if you are storing large arrays of numbers, it is much more space\u2010 and perfor- mance\u2010efficient to store them in something such as the native format that computers use for processing numbers."
    },
    {
        "text": "most image files or sound files consist mostly of dense arrays of numbers, packed adjacent to each other in memory."
    },
    {
        "text": "many scientific datasets fall into this category too."
    },
    {
        "text": "in my experience, you don\u2019t see these datasets as often in data science, but they do come up."
    },
    {
        "text": "12.1.3 program\u2010specific data formats many computer programs have their own specialized file format."
    },
    {
        "text": "this category would include things such as excel files, db files, and similar formats."
    },
    {
        "text": "typically, you will need to look up a tool to open one of these files."
    },
    {
        "text": "in my experience, opening them often takes a while computationally, since there are often a lot of bells and whistles built into the program that may or may not be present in this particular dataset."
    },
    {
        "text": "this makes it a pain to reparse them every time you rerun your analysis scripts \u2013 often, it takes much longer than the actual analysis does."
    },
    {
        "text": "what i typically do is make csv versions of them right upfront and use those as the input to my analyses."
    },
    {
        "text": "12.1.4 compressed or archived data many data files, when stored in a particular format, take up a lot more space compared to the file in question logically needs; for example, if most lines in a large text file are exactly the same or a dense numerical array consists mostly of 0s."
    },
    {
        "text": "in these cases, we want to compress the large file into a smaller one, so that it can be stored and transferred more easily."
    },
    {
        "text": "a related problem is when we have a large collection of files that we want to condense into a single file for easier management, often called data archiving."
    },
    {
        "text": "there are a variety of ways that we can encode the raw data into these more manageable forms."
    },
    {
        "text": "12.2 csv files 167 there is a lot more to data compression than just reducing the size."
    },
    {
        "text": "a \u201cper- fect\u201d algorithm would have the following properties: \u25cf \u25cfit generally reduces the size of the data, easing storage requirements."
    },
    {
        "text": "\u25cf \u25cfif it can\u2019t compress the data much (or at all), then at least it doesn\u2019t balloon it to take up much more space."
    },
    {
        "text": "\u25cf \u25cfyou can decompress it quickly."
    },
    {
        "text": "if you do this really well, it might take you less time to load the compressed data compared to the raw data itself, even with the decompression step."
    },
    {
        "text": "this is because decompression in ram can be fairly quick, but it takes a long time to pull extra data off the disk."
    },
    {
        "text": "\u25cf \u25cfyou can decompress it \u201cone line at a time,\u201d rather than loading the entire file."
    },
    {
        "text": "this helps you deal with corrupt data and typically makes decompression go faster since you\u2019re operating on less data at a time."
    },
    {
        "text": "\u25cf \u25cfyou can recompress it quickly."
    },
    {
        "text": "in the real world, there is a wide range of compression algorithms available, which balance these interests in a lot of different ways."
    },
    {
        "text": "compression becomes especially important in big data settings, where datasets are typically large and reloaded from disk every time the code runs."
    },
    {
        "text": "12.2 \u00adcsv files csv files are the workhorse data format for data science."
    },
    {
        "text": "\u201ccsv\u201d usually stands for \u201ccomma\u2010separated value,\u201d but it really should be \u201ccharacter\u2010separated value\u201d since characters other than commas do get used."
    },
    {
        "text": "sometimes, you will see \u201c.tsv\u201d if tabs are used or \u201c.psv\u201d if pipes (the \u201c|\u201d character) are used."
    },
    {
        "text": "more often though, in my experience, everything gets called csv regardless of the delimiter."
    },
    {
        "text": "csv files are pretty straightforward conceptually \u2013 just a table with rows and columns."
    },
    {
        "text": "there are a few complications you should be aware of though: \u25cf \u25cfheaders."
    },
    {
        "text": "sometimes, the first line gives names for all the columns, and sometimes, it gets right into the data."
    },
    {
        "text": "\u25cf \u25cfquotes."
    },
    {
        "text": "in many files, the data elements are surrounded in quotes or another character."
    },
    {
        "text": "this is done largely so that commas (or whatever the delimiting character is) can be included in the data fields."
    },
    {
        "text": "\u25cf \u25cfnondata rows."
    },
    {
        "text": "in many file formats, the data itself is csv, but there are a certain number of nondata lines at the beginning of the file."
    },
    {
        "text": "typically, these encode metadata about the file and need to be stripped out when the file is loaded into a table."
    },
    {
        "text": "\u25cf \u25cfcomments."
    },
    {
        "text": "many csv files will contain human\u2010readable comments, as source code does."
    },
    {
        "text": "typically, these are denoted by a single character, such as the # in python."
    },
    {
        "text": "12 data encodings and file formats 168 \u25cf \u25cfblank lines."
    },
    {
        "text": "they happen."
    },
    {
        "text": "\u25cf \u25cflines with the wrong number of columns."
    },
    {
        "text": "these happen too."
    },
    {
        "text": "the following python code shows how to read a basic csv file into a data frame using pandas: import pandas df = pandas.read_csv(\"myfile.csv\") ) if your csv file has weird complexities associated with it, then read_csv has a number of optional arguments that let you deal with them."
    },
    {
        "text": "here is a more complicated call to read_csv: import pandas df = pandas.read_csv(\"myfile.csv\", sep = \"|\", # the delimiter."
    },
    {
        "text": "default is the comma header = false, quotechar = \u2019\"\u2019, compression = \"gzip\", comment = \u2019#\u2019 ) in my work, the optional arguments i use most are sep and header."
    },
    {
        "text": "12.3 \u00adjson files json is probably my single favorite data format, for its dirt simplicity and flex- ibility."
    },
    {
        "text": "it is a way to take hierarchical data structures and serialize them into a plain text format."
    },
    {
        "text": "every json data structure is either of the following: \u25cf \u25cfan atomic type, such as a number, a string, or a boolean."
    },
    {
        "text": "\u25cf \u25cfa jsonobject, which is just a map from strings to json data structures."
    },
    {
        "text": "this is similar to python dictionaries, except that there are keys in the jsonobject."
    },
    {
        "text": "\u25cf \u25cfan array of json data structures."
    },
    {
        "text": "this is similar to a python list."
    },
    {
        "text": "here is an example of some valid json, which encodes a jsonobject map with a lot of substructures: { \"firstname\": \"john\", \"lastname\": \"smith\", \"isalive\": true,"
    },
    {
        "text": "12.3 json files 169 \"age\": 25, \"address\": { \"streetaddress\": \"21 2nd street\", \"city\": \"new york\", \"state\": \"ny\", \"postalcode\": \"10021\u20103100\" }, \"children\": [\"alice\", \"john\", {\"name\": \"alice\", \"birth_order\": 2}], \"spouse\": null } note a few things about this example: \u25cf \u25cfthe fact that i\u2019ve made it all pretty with the newlines and indentations is purely to make it easier to read."
    },
    {
        "text": "this could have all been on one long line and any json parser would parse it equally well."
    },
    {
        "text": "a lot of programs for viewing json will automatically format it in this more legible way."
    },
    {
        "text": "\u25cf \u25cfthe overall object is conceptually similar to a python dictionary, where the keys are all strings and the values are json objects."
    },
    {
        "text": "the overall object could have been an array too though."
    },
    {
        "text": "\u25cf \u25cfa difference between json objects and python dictionaries is that all the field names have to be strings."
    },
    {
        "text": "in python, the keys can be any hashable type."
    },
    {
        "text": "\u25cf \u25cfthe fields in the object can be ordered arrays, such as \u201cchildren.\u201d these arrays are analogous to python lists."
    },
    {
        "text": "\u25cf \u25cfyou can mix and match types in the object, just as in python."
    },
    {
        "text": "\u25cf \u25cfyou can have boolean types."
    },
    {
        "text": "note though that they are declared in lower case."
    },
    {
        "text": "\u25cf \u25cfthere are also numerical types."
    },
    {
        "text": "\u25cf \u25cfthere is a null supported \u25cf \u25cfyou can nest the object arbitrarily deeply."
    },
    {
        "text": "parsing json is a cinch in python."
    },
    {
        "text": "you can either \u201cload\u201d a json string into a python object (a dictionary at the highest level, with json arrays mapping to python lists, etc.)"
    },
    {
        "text": "or \u201cdump\u201d a python dictionary into a json string."
    },
    {
        "text": "the json string can either be a python string or be stored in a file, in which case you write from/to a file object."
    },
    {
        "text": "the code looks as follows: >>> import json >>> json_str = \"\"\"{\"name\": \"field\", \"height\":6.0}\"\"\" >>> my_obj = json.loads(json_str) >>> my_obj {u'name': u'field', u'height': 6.0} >>> str_again = json.dumps(my_obj)"
    },
    {
        "text": "12 data encodings and file formats 170 historically, json was invented as a way to serialize objects from the javascript language."
    },
    {
        "text": "think of the keys in a jsonobject as the names of the members in an object."
    },
    {
        "text": "however, json does not support notions such as pointers, classes, and functions."
    },
    {
        "text": "12.4 \u00adxml files xml is similar to json: a text\u2010based format that lets you store hierarchical data in a format that can be read by both humans and machines."
    },
    {
        "text": "however, it\u2019s significantly more complicated than json \u2013 part of the reason that json has been eclipsing it as a data transfer standard on the web."
    },
    {
        "text": "let\u2019s jump in with an example: <groupofpeople> <person gender=\"male\"> <name>field cady</name> <profession>data scientist</profession> </person> <person gender=\"female\"> <name>ryna</name> <profession>engineer</profession> </person> </groupofpeople> everything enclosed in angle brackets is called a \u201ctag.\u201d every section of the document is bookended by a matching pairs of tags, which tell what type of section it is."
    },
    {
        "text": "the closing tag contains a slash \u201c/\u201d after the \u201c<\u201d."
    },
    {
        "text": "the opening tag can contain other pieces of information about the section \u2013 in this case, \u201cgen- der\u201d is such an attribute."
    },
    {
        "text": "because you can have whatever tag names or addi- tional attributes you like, xml lends itself to making domain\u2010specific description languages."
    },
    {
        "text": "xml sections must be fully nested into each other, so something such as the following is invalid: <a><b></a></b> because the \u201cb\u201d section begins in the middle of the \u201ca\u201d section but doesn\u2019t end until the \u201ca\u201d is already over."
    },
    {
        "text": "for this reason, it is conventional to think of an xml document as a tree structure."
    },
    {
        "text": "every nonleaf node in the tree corresponds to a pair of opening/closing tags, of some type and possibly with some attrib- utes, and the leaf nodes are the actual data."
    },
    {
        "text": "sometimes, we want the start and end tag of a section to be adjacent to each other."
    },
    {
        "text": "in this case, there is a little bit of syntactic sugar, where you put the clos- ing \u201c/\u201d before the closing angle bracket."
    },
    {
        "text": "so,"
    },
    {
        "text": "12.4 xml files 171 <foo a=\"bar\"></foo> is equivalent to <foo a=\"bar\"/> a big difference between json and xml is that the content in xml is ordered."
    },
    {
        "text": "every node in the tree has its children in a particular order \u2013 the order in which they come in the document."
    },
    {
        "text": "they can be of any types and come in any order, but there is an order."
    },
    {
        "text": "processing xml is a little more finicky than processing json, in my experi- ence."
    },
    {
        "text": "this is for two reasons: \u25cf \u25cfit\u2019s easier to refer to a named field in a json object than to search through all the children of a xml node and find the one you\u2019re looking for."
    },
    {
        "text": "\u25cf \u25cfxml nodes often have additional attributes, which are handled separately from the node\u2019s children."
    },
    {
        "text": "\u25cf \u25cfthis isn\u2019t inherent to the data formats, but in practice, json tends to be used in small snippets, for smaller applications where the data has regular struc- ture."
    },
    {
        "text": "so, you typically know exactly how to extract the data you\u2019re looking for."
    },
    {
        "text": "in contrast, xml is liable to be a massive document with many parts, and you have to sift through the whole thing."
    },
    {
        "text": "in python, the xml library offers a variety of ways of processing xml data."
    },
    {
        "text": "the simplest is the elementtree sublibrary, which gives us direct access to the parse tree of the xml."
    },
    {
        "text": "it is shown in this code example, where we parse xml data into a string object, access and modify the data, and then reencode it back to an xml string: >>> import xml.etree.elementtree as et >>> xml_str = \"\"\" <data> <country name=\"liechtenstein\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\"austria\" direction=\"e\"/> <neighbor name=\"switzerland\" direction=\"w\"/> </country> <country name=\"singapore\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\"malaysia\" direction=\"n\"/> </country>"
    },
    {
        "text": "12 data encodings and file formats 172 <country name=\"panama\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\"costa rica\" direction=\"w\"/> <neighbor name=\"colombia\" direction=\"e\"/> </country> </data> \"\"\" >>> root = et.fromstring(xml_str) >>> root.tag 'data' >>> root[0] # gives the zeroth child <element 'country' at 0x1092d4410> >>> root.attrib # dictionary of node\u2019s attributes {} >>> root.getchildren() [<element 'country' at 0x1092d4410>, <element 'country' at 0x1092d47d0>, <element 'country' at 0x1092d4910>] >>> del root[0] # deletes the zeroth child from the tree >>> modified_xml_str = et.tostring(root) the \u201cright\u201d way to manage xml data is called the \u201cdocument object model.\u201d it is a little more standardized across programming languages and web brows- ers, but it is also more complicated to master."
    },
    {
        "text": "the elementtree is fine for sim- ple applications and capable of doing whatever you need it to do."
    },
    {
        "text": "12.5 \u00adhtml files by far the most important variant of xml is html, the language for describ- ing pages on the web."
    },
    {
        "text": "practically speaking, the definition of \u201cvalid\u201d html is that your web browser will parse it as intended."
    },
    {
        "text": "there are differences between browsers, some intentional and some not, and that\u2019s why the same might look different in chrome and internet explorer."
    },
    {
        "text": "but browsers have largely converged on a standard version of html (the most recent official standard is html5), and to a first approximation, that standard is a variant of xml."
    },
    {
        "text": "many web pages could be parsed with an xml parser library."
    },
    {
        "text": "i mentioned in the last section that xml can be used to create domain\u2010spe- cific languages, each of which is defined by its own set of valid tags and their associated attributes."
    },
    {
        "text": "this is the way html works."
    },
    {
        "text": "some of the more notable tags are given in the following table:"
    },
    {
        "text": "12.5 html files 173 tag meaning example <a> hyperlink click <a href=\"www.google."
    },
    {
        "text": "com\">here</a> to go to google <img> image <img src=\"smiley.gif\"> <h1>\u2013<h6> headings of text <h1>the title</h1> <div> division."
    },
    {
        "text": "it doesn\u2019t get rendered but helps to organize the document."
    },
    {
        "text": "often, the \u201cclass\u201d attribute is used to associate the contents of the division with a desired style of text formatting <div class=\"main\u2010text\">my body of text</div> <ul> and <li> unordered lists (usually rendered as bulleted lists) and list items here is a list: <ul> <li>item one</li> <li>item two</li> </ul> the practical problem with processing html data is that, unlike json or even xml, html documents tend to be extremely messy."
    },
    {
        "text": "they are often indi- vidually made, edited by humans, and tweaked until they look \u201cjust right.\u201d this means that there is almost no regularity in structure from one html docu- ment to the next, so the tools for processing html lean toward combing through the entire document to find what it is you\u2019re looking for."
    },
    {
        "text": "the default html tool for python is the htmlparser class, which you use by creating a subclass that inherits from it."
    },
    {
        "text": "an htmlparser works by walking through the document, performing some action each time it hits a start or an end tag or other piece of text."
    },
    {
        "text": "these actions will be user\u2010defined methods on the class, and they work by modifying the parser\u2019s internal state."
    },
    {
        "text": "when the parser has walked through the entire document, its internal state can be que- ried for whatever it is you were looking for."
    },
    {
        "text": "one very important note is that it\u2019s up to the user to keep track of things such as how deeply nested you are within the document\u2019s sections."
    },
    {
        "text": "to illustrate, the following code will pull down the html for a wikipedia , step through its content, and count all hyperlinks that are embedded in the body of the text (i.e., they are within paragraph tags): from htmlparser import htmlparser import urllib topic = \"dangiwa_umar\" url = \"https://en.wikipedia.org/wiki/%s\" % topic class linkcountingparser(htmlparser): in_paragraph = false link_count = 0"
    },
    {
        "text": "12 data encodings and file formats 174 def handle_starttag(self, tag, attrs): if tag=='p': self.in_paragraph = true elif tag=='a' and self.in_paragraph: self.link_count += 1 def handle_endtag(self, tag): if tag=='p': self.in_paragraph = false html = urllib.urlopen(url).read() parser = linkcountingparser() parser.feed(html) print \"there were\", parser.link_count, \\ \"links in the article\" 12.6 \u00adtar files tar is the most popular example of an \u201carchive file\u201d format."
    },
    {
        "text": "the idea is to take an entire directory full of data, possibly including nested subdirectories, and combine it all into a single file that you can send in an e\u2010mail, store somewhere, or whatever you want."
    },
    {
        "text": "there are a number of other archive file formats, such as iso, but in my experience, tar is the most common example."
    },
    {
        "text": "besides their widespread use for archiving files, tar files are also used by the java programming language and its relatives."
    },
    {
        "text": "compiled java classes are stored into jar files, but jar files are created just by tarring individual java class files together."
    },
    {
        "text": "jar is the same format as tar, except that you are only combining java class files rather than arbitrary file types."
    },
    {
        "text": "tarring a directory doesn\u2019t actually compress the data \u2013 it just combines the files into one file that takes up about as much space as the data did originally."
    },
    {
        "text": "so in practice, tar files are almost always then zipped."
    },
    {
        "text": "gzipping in particular is popular."
    },
    {
        "text": "the \u201c.tgz\u201d file extension is used as a shorthand for \u201c.tar.gz\u201d, that is, the directory has been put into a tar file, which was then compressed using the gzip algorithm."
    },
    {
        "text": "tar files are typically opened from the command line, such as the following: $ # this will expand the contents of $ # my_directory.tar into the local directory $ tar -xvf my_directory.tar $ # this command will untar and unzip $ # a directory with has been tarred and g-zipped $ tar -zxf file.tar.gz $ # this command will tar the homework3 directory $ # into the file ilovehomework.tar $ tar -cf ilovehomework.tar homework3"
    },
    {
        "text": "12.8 zip files 175 12.7 \u00adgzip files gzip is the most common compression format that you will see on unix\u2010like systems such as mac and linux."
    },
    {
        "text": "often, it\u2019s used in conjunction with tar to archive the contents of an entire directory."
    },
    {
        "text": "encoding data with gzip is com- paratively slow, but the format has the following advantages: \u25cf \u25cfit compresses data super well."
    },
    {
        "text": "\u25cf \u25cfdata can be decompressed quickly."
    },
    {
        "text": "\u25cf \u25cfit can also be decompressed one line at a time, in case you only want to oper- ate only on part of the data without decompressing the whole file."
    },
    {
        "text": "under the hood, gzip runs on a compression algorithm called deflate."
    },
    {
        "text": "a compressed gzip file is broken into blocks."
    },
    {
        "text": "the first part of each block contains some data about the block, including how the rest of the block is encoded (it will be some type of huffman code, but you don\u2019t need to worry about the details of those)."
    },
    {
        "text": "once the gzip program has parsed this header, it can read the rest of the block 1 byte at a time."
    },
    {
        "text": "this means there is minimal ram being used up, so all the decompression can go on near the top of the ram cache and hence proceed at breakneck speed."
    },
    {
        "text": "the typical commands for gzipping/unzipping from the shell are simple: $ gunzip myfile.txt.gz # creates raw file myfile.txt $ gzip myfile.txt # compresses the file into myfile."
    },
    {
        "text": "txt.gz however, you can typically also just double\u2010click on a file \u2013 most operating systems can open gzip files natively."
    },
    {
        "text": "12.8 \u00adzip files zip files are very similar to gzip files."
    },
    {
        "text": "in fact, they even use the same deflate algorithm under the hood!"
    },
    {
        "text": "there are some differences though, such as the fact that zip can compress an entire directory rather than just individual files."
    },
    {
        "text": "zipping and unzipping files are as easy with zip as with gzip: $ # this puts several files into a single zip file $ zip filename.zip input1.txt input2.txt resume.doc pic1.jpg $ # this will open the zip file and put $ # all of its contents into the current directory $ unzip filename.zip"
    },
    {
        "text": "12 data encodings and file formats 176 12.9 \u00adimage files: rasterized, vectorized, and/or compressed image files can be broken down into two broad categories: rasterized and vec- torized."
    },
    {
        "text": "rasterized files break an image down into an array of pixels and encode things such as the brightness or color of each individual pixel."
    },
    {
        "text": "sometimes, the image file will store the pixel array directly, and other times, it will store some compressed version of the pixel array."
    },
    {
        "text": "almost all machine\u2010generated data will be rasterized."
    },
    {
        "text": "vectorized files, on the other hand, are a mathematical description of what the image should look like, complete with perfect circles, straight lines, and so on."
    },
    {
        "text": "they can be scaled to any size without losing resolution."
    },
    {
        "text": "vectorized files are more likely to be company logos, animations, and similar things."
    },
    {
        "text": "the most common vectorized image format you\u2019re likely to run into is svg, which is actually just an xml file under the hood (as i mentioned before, xml is great for domain\u2010specific languages!)."
    },
    {
        "text": "however, in daily work as a data scientist, you\u2019re most likely to encounter rasterized files."
    },
    {
        "text": "a rasterized image is an array of pixels that, depending on the format, can be combined with metadata and then possibly subjected to some form of compres- sion (sometimes using the deflate algorithm, such as gzip)."
    },
    {
        "text": "there are sev- eral considerations that differentiate between the different formats available: \u25cf \u25cflossy versus lossless."
    },
    {
        "text": "many formats (such as bmp and png) encode the pixel array exactly \u2013 these are called lossless."
    },
    {
        "text": "but others (such as jpeg) allow you to reduce the size of the file by degrading the resolution of your image."
    },
    {
        "text": "\u25cf \u25cfgrayscale versus rbg."
    },
    {
        "text": "if images are black\u2010and\u2010white, then you only need one number per pixel."
    },
    {
        "text": "but if you have a colored image, then there needs to be some way to specify the color."
    },
    {
        "text": "typically, this is done by using rgb encoding, where a pixel is specific by how much red, how much green, and how much blue it contains."
    },
    {
        "text": "\u25cf \u25cftransparency."
    },
    {
        "text": "many images allow pixels to be partly transparent."
    },
    {
        "text": "the \u201calpha\u201d of a pixel ranges from 0 to 1, with 0 being completely transparent and 1 being completely opaque."
    },
    {
        "text": "some of the most important image formats you should be aware of are as follows: \u25cf \u25cfjpeg."
    },
    {
        "text": "this is probably the single most important one in web traffic, prized for its ability to massively compress an image with almost invisible degrada- tion."
    },
    {
        "text": "it is a lossy compression format, stores rgb colors, and does not allow for transparency."
    },
    {
        "text": "\u25cf \u25cfpng."
    },
    {
        "text": "this is maybe the next most ubiquitous format."
    },
    {
        "text": "it is lossless and allows for transparency pixels."
    },
    {
        "text": "personally, i find the transparent pixels make png files super useful when i\u2019m putting together slide decks."
    },
    {
        "text": "12.10 it\u2019s all bytes at the end of the day 177 \u25cf \u25cftiff."
    },
    {
        "text": "tiff files are not common on the internet, but they are a frequent for- mat for storing high\u2010resolution pictures in the context of photography or science."
    },
    {
        "text": "they can be lossy or lossless."
    },
    {
        "text": "the following python code will read an image file."
    },
    {
        "text": "it takes care of any decom- pression or format\u2010specific stuff under the hood and returns the image as a numpy array of integers."
    },
    {
        "text": "it will be a three\u2010dimensional array, with the first two dimensions corresponding to the normal width and height."
    },
    {
        "text": "the image is read in as rbg by default, and the third dimension of the array indicates whether we are measuring the red, blue, or green content."
    },
    {
        "text": "the integers themselves will range from 0 to 255, since each is encoded with a single byte."
    },
    {
        "text": "from scipy.ndimage import imread img = imread('mypic.jpg') if you want to read the image as grayscale, you can pass mode = \"f\" and get a two\u2010dimensional array."
    },
    {
        "text": "if you instead want to include the alpha opacity as a fourth value for each pixel, pass in mode = \"rgba.\""
    },
    {
        "text": "12.10 \u00adit\u2019s all bytes at the end of the day at the lowest level, the data in a computer file is a long array of bits, each of which is set to 0 or 1. that array is broken into 8\u2010bit chunks called bytes."
    },
    {
        "text": "the concept of a byte is both conceptual and physical."
    },
    {
        "text": "on the one hand, we usually break up a file into basic logical units that are composed of bytes, such as hav- ing one byte to encode a letter or a number."
    },
    {
        "text": "you could theoretically create a file format where the basic units were of 5 bits or 11 bits long, but the universal convention is to use bytes."
    },
    {
        "text": "at the same time, the physical hardware of the computer is optimized to process data one byte (or a group of several bytes) at a time."
    },
    {
        "text": "a modern computer\u2019s memory is called ram, for \u201crandom access memory.\u201d \u201crandom\u201d in this case isn\u2019t about probability: it refers to the fact that you can read/modify any part of memory with about the same latency."
    },
    {
        "text": "the memory is physically broken up into bytes for easier processing."
    },
    {
        "text": "the data structures that exist in memory as a program runs are, similarly to raw files, ultimately encoded into bytes."
    },
    {
        "text": "sometimes, the encodings used for a file and a real\u2010time data struc- ture are identical, and sometimes, they are quite different."
    },
    {
        "text": "historically, an atomic type in a programming language was defined to take up a fixed number of bytes."
    },
    {
        "text": "an integer would frequently be allocated 4 bytes, and the integer was encoded in those bytes in binary."
    },
    {
        "text": "having every integer take up the same amount of space was critical, because the physical layout of the bits doesn\u2019t make it clear where one integer (or any other type of variable) ends"
    },
    {
        "text": "12 data encodings and file formats 178 and another begins."
    },
    {
        "text": "these transitions generally occur on the boundaries between bytes, but that\u2019s it."
    },
    {
        "text": "the computer\u2019s \u201cnative language\u201d doesn\u2019t have any notion of integers or any other data type; to the computer, everything is just bytes, so fixed\u2010size variables are critical for keeping track of things."
    },
    {
        "text": "in modern languages such as python, there are more variable\u2010size types."
    },
    {
        "text": "for example, a python string can take up arbitrarily many bytes."
    },
    {
        "text": "however, doing this requires overhead to keep track of where one item ends and another begins, which translates to a substantial performance cost."
    },
    {
        "text": "modern languages tend to try for fixed\u2010size atomic types whenever possible, but then revert to the less\u2010efficient version when necessary."
    },
    {
        "text": "software that is intended to run extremely fast, like python\u2019s numerical libraries, almost always strips out the overhead and limits itself to fixed\u2010size types."
    },
    {
        "text": "12.11 \u00adintegers integers are about the simplest atomic type to understand."
    },
    {
        "text": "back in the day when ram was more expensive, people did all kinds of tricks to try and encode integers using fewer bits, but now things have basically settled out: \u25cf \u25cfan integer gets a fixed number of bytes."
    },
    {
        "text": "eight bytes is also typical, if you\u2019re using a 64\u2010bit computer."
    },
    {
        "text": "\u25cf \u25cfthe integer is encoded in those bits in binary."
    },
    {
        "text": "\u25cf \u25cfone of the bits isn\u2019t interpreted as a binary digit: it is a flag saying whether the integer is negative."
    },
    {
        "text": "if it\u2019s negative, then typically the 0s and 1s are flipped in the rest of the number, for arithmetic efficiency reasons that you don\u2019t need to worry about."
    },
    {
        "text": "this system works seamlessly most of the time, but there is a maximum size of integer that can be handled; 63 bits is only so big."
    },
    {
        "text": "in python, you can get that upper bound in the following way: >> import sys >> sys.maxint 9223372036854775807 this number is 2 to the power of 63: 63 bits to store the number and one to flag that it\u2019s positive."
    },
    {
        "text": "this number is large enough for almost all purposes, but occasionally you need something bigger."
    },
    {
        "text": "oftentimes, you never even realize that you\u2019ve ventured into this area!"
    },
    {
        "text": "in python, if you ever declare a variable equal to something larger than sys.maxint, then python will silently switch over to a different, far less efficient data type called a \"long.\""
    },
    {
        "text": "from a program- mer\u2019s perspective, a long looks, feels, and acts as an int: the only clear sign"
    },
    {
        "text": "12.12 floats 179 that it\u2019s something different is a telltale \"l\" after the number when it\u2019s displayed: >>> 3*sys.maxint 27670116110564327421l the seamless transition is a luxury afforded by using a very high\u2010level lan- guage such as python, and you pay for it in efficiency."
    },
    {
        "text": "it takes overhead to check at every step whether the system needs to switch over to using longs, and if things ever do switch over the performance hit really cranks up."
    },
    {
        "text": "12.12 \u00adfloats floating\u2010point numbers are more complicated than integers, mostly because they are inherently error\u2010prone."
    },
    {
        "text": "a floating number can theoretically have infi- nitely many decimal places, and the computer can only store finitely many of them."
    },
    {
        "text": "innocuous operations such as taking a square root, or even dividing by 3, can balloon a previously tame number into infinite\u2010decimal land."
    },
    {
        "text": "in almost every computer system, a floating\u2010point value is stored as a pair of two numbers, typically a pair of the integers as discussed in the previous section: \u25cf \u25cfone integer stores the digits in the binary representation of the number."
    },
    {
        "text": "\u25cf \u25cfthe other stores the location of the decimal point in the number."
    },
    {
        "text": "the overwhelming advantage to this way of doing things is that it lets us represent both very large and very small numbers with the same degree of accuracy; roundoff error will corrupt the number 1 billion about the same per- centage that it hurts the number 1 billionth."
    },
    {
        "text": "other floating\u2010point schemes were tried out back in the day, but they are now in the dustbin of history."
    },
    {
        "text": "as a data scientist, you don\u2019t need to worry too much about roundoff error; good partial workarounds have been baked into most numerical algorithms, and the fact that ram is so cheap now means that we usually carry around many more decimal places than are ever necessary."
    },
    {
        "text": "however, roundoff issues can show up in subtle ways, as shown in this script: >>> x, y = 0.1, 0.2 >>> x + y 0.30000000000000004 this is because 0.1 and 0.2 both have infinitely many decimal places when expressed in binary, so python only stores an approximation to them."
    },
    {
        "text": "the"
    },
    {
        "text": "12 data encodings and file formats 180 stored value of x is not 0.1; it is the closest number to 0.1 that can be stored as a float."
    },
    {
        "text": "in this case, that number is slightly larger than 0.1, and similarly for 0.2. when you add x and y, these small errors are large enough to add up."
    },
    {
        "text": "if you try to look at the value of x, you will see >>> x 0.1 this number is an illusion!"
    },
    {
        "text": "python is rounding x by a tiny bit before display- ing it, as a visual courtesy to the user."
    },
    {
        "text": "but the error margin on x + y is large enough that python will display it instead."
    },
    {
        "text": "as with large integers, there are computationally very expensive worka- rounds for the limitations of machine floating points."
    },
    {
        "text": "usually, these take the form of either storing numbers as arbitrary\u2010length strings or storing the arith- metic expressions that generated the numbers."
    },
    {
        "text": "these exceedingly expensive, but technically exact, expressions are carried through a computation and can later be approximately cast into the normal style of numbers."
    },
    {
        "text": "personally i\u2019ve never used an exact arithmetic system, and i don\u2019t expect to."
    },
    {
        "text": "they are mostly useful in theoretical math situations where exact equality is important, and this almost never occurs in real\u2010world work."
    },
    {
        "text": "12.13 \u00adtext data the previous two subsections were kind of academic: you generally don\u2019t need to worry about how machines represent numbers in your daily work."
    },
    {
        "text": "this is not the case with strings though: there are several different ways that strings are stored, which have very different tradeoffs, and you must keep an eye toward them."
    },
    {
        "text": "in fact, as i write this, i\u2019m grappling with some nagging string\u2010 type issues in my own work."
    },
    {
        "text": "the code isn\u2019t correctly converting between two different string implementations, and it\u2019s irritating because i thought i fixed the damn thing a while ago."
    },
    {
        "text": "i\u2019m doing this in python; using a high\u2010level lan- guage does not necessarily shield you from string implementation issues."
    },
    {
        "text": "the granddaddy string format is called ascii (pronounced \u201cass\u201d, \u201ckey\u201d)."
    },
    {
        "text": "it is dirt simple, is super efficient, and has stood the test of time."
    },
    {
        "text": "the problem is that it\u2019s set in stone, and it\u2019s limited."
    },
    {
        "text": "anything you can type with a standard american\u2010style keyboard can be encoded into ascii, so you can do a lot with it."
    },
    {
        "text": "but in many modern applications that\u2019s not enough."
    },
    {
        "text": "there are chinese characters, and german letters with an umlaut on top."
    },
    {
        "text": "there are emoticons."
    },
    {
        "text": "there might even be additional types of text that get invented later \u2013 just look at the rise of the emoticon!"
    },
    {
        "text": "in ascii, every character is encoded in a single byte, sometimes called a \u201cchar.\u201d this gives us an interesting phenomenon: there is a mapping between"
    },
    {
        "text": "12.13 text data 181 ascii characters and short integers, since they are encoded by the same byte."
    },
    {
        "text": "it\u2019s not one\u2010to\u2010one, because ascii only specifies characters for numbers up to 127, but a byte can encode up to 255 (some bytes are not valid ascii, but they are still perfectly fine encodings of integers)."
    },
    {
        "text": "quite rationally, the capital \"a\" is the number 65, \"b\" is 66, and so on."
    },
    {
        "text": "the lowercase numbers are later, with 97 for \u201ca,\u201d 98 for \u201cb,\u201d 99 for \u201cc,\u201d and so on."
    },
    {
        "text": "python lets you convert between these using the functions chr() and ord() (for \"ordinal\"): >>> chr(65) \"a\" >>> ord(\"a\") 65 ascii also includes the various special characters that you can type with a keyboard."
    },
    {
        "text": "tab is 9, and newline is 10."
    },
    {
        "text": "\u201c@\u201d is 64. the digits \u201c0\u201d through \u201c9\u201d are 48 to 57. you might think of ascii as the \u201cestablishment\u201d string format that things use if they have to be extremely fault tolerant."
    },
    {
        "text": "python code is supposed to be stored as ascii \u2013 the python interpreter will throw an error if you point it at a file that is not ascii formatted."
    },
    {
        "text": "operating systems use it."
    },
    {
        "text": "plain text files are typi- cally ascii whenever possible."
    },
    {
        "text": "python string objects are stored in ram as ascii."
    },
    {
        "text": "this might be a good time to revisit the way we declare strings in python \u2013 this paragraph is optional but interesting."
    },
    {
        "text": "recall that for the most part we just put the contents of the string in quotation marks, and type whatever we want, as in: >>> my_string = \"abc123\" but some characters, such as tabs and newlines, can\u2019t always be directly typed."
    },
    {
        "text": "in this case, we use the slash character \"\\\" to encode them out of things that we can type."
    },
    {
        "text": "for example: >>> my_tab = \"\\t\" # this is a one\u2010character string >>> my_newline = \"\\n\" # this is too adding the slash before a character in order to encode something is called \u201cescaping\u201d the character."
    },
    {
        "text": "now i\u2019ll give you the keys to the kingdom: if you want super fine\u2010grained control, you can escape \"x\" to tell the computer exactly which ascii bytes should be in a string."
    },
    {
        "text": "if i declare a string such as >>> fancy_string = '\\xaa' then the two characters \"aa\" will be interpreted as the hexadecimal number of the ascii byte you want."
    },
    {
        "text": "hexagesimal is a slightly archaic, base\u201016 way to write numbers, whose 16 digits are 0, 1, 2, ..., 9, a, b, ..., e, f. writing \"\\t\" is just"
    },
    {
        "text": "12 data encodings and file formats 182 a nicer way of writing \"\\x09,\" and \"\\n\" is the same thing as \"\\x0a\" (0a in hexa- gesimal is 10)."
    },
    {
        "text": "in fact, this is more powerful than ascii, because technically ascii numbers only go up through 127, whereas hexagesimal notation lets you put in bytes up to 255, that is, any possible byte."
    },
    {
        "text": "personally, the only time i use hexagesimal notation is when i\u2019m deliberately creating perverse strings for purposes of unit testing, but it\u2019s there if you want it."
    },
    {
        "text": "the other big string standard is known as unicode."
    },
    {
        "text": "unicode is actually a family of encoding standards, all of them aiming to supplement basic ascii with the massive range of other characters needed today and possibly in the future."
    },
    {
        "text": "the main version of unicode available is utf\u20108, and it is fast becoming the most popular encoding around."
    },
    {
        "text": "in this chapter, utf\u20108 will be the one i discuss."
    },
    {
        "text": "the biggest difference between unicode and ascii is that in unicode there is a variable number of bytes that encode each character."
    },
    {
        "text": "this means that all the performance advantages of fixed\u2010sized elements go out the window, but this is the price you must pay for flexibility."
    },
    {
        "text": "however, utf\u20108 is backward compatible with ascii: a chunk of bytes that are valid ascii are also valid utf\u20108."
    },
    {
        "text": "this works because not all bytes are valid ascii \u2013 the ascii integers top out at 127, but a byte can go up to 255. so, if you are reading through an array of unicode and come to a byte that is greater than 127, it signifies that this byte (and possibly the next several) constitutes a non\u2010ascii character."
    },
    {
        "text": "when you upgrade from ascii to 2\u2010byte characters, you get pretty much all characters in western languages."
    },
    {
        "text": "three bytes will give you east asia."
    },
    {
        "text": "four will give you various historical writing systems, mathematical symbols, and emoticons."
    },
    {
        "text": "python has native support for unicode."
    },
    {
        "text": "declaring a string\u2010type variable to be unicode rather than a normal string is as simple as putting a \"u\" outside the parentheses: >>> unicode_str = u\"this is unicode\" python strings are more general compared to ascii or utf\u20108."
    },
    {
        "text": "using the \u201c\\x\u201d trick, you can force python to put an arbitrary collection of bytes into a string object, and they may or may not be valid ascii/unicode."
    },
    {
        "text": "if you have a string and you want to convert it into valid ascii or utf\u20108, then you can say >>> # fails if not valid ascii >>> as_ascii = my_string.decode('ascii') >>> # drops non-ascii characters >>> as_ascii = my_string.decode(\u2019ascii\u2019, \u2019ignore\u2019) >>> # drops non-unicode characters >>> as_utf8 = my_string.decode('utf8', 'ignore')"
    },
    {
        "text": "12.15 glossary 183 12.14 \u00adfurther reading 1 murrell, p, introduction to data technologies, viewed 8 august 2016, http:// statmath.wu.ac.at/courses/data\u2010analysis/ 2 pilgrim, m, 2004, dive into python: python from novice to pro, viewed 7 august 2016, http://www.diveintopython.net/ 12.15 \u00adglossary archiving combining several files or directories into a single file which can later be expanded out to re\u2010create the original files and directories."
    },
    {
        "text": "ascii a text encoding scheme that has one character per byte."
    },
    {
        "text": "it pretty much only covers characters that you are likely to type with a standard american keyboard."
    },
    {
        "text": "bit a single piece of data that can be either 0 or 1. byte eight bits."
    },
    {
        "text": "computer memory is physically grouped into bytes."
    },
    {
        "text": "compression taking a single file and condensing it down into a smaller file."
    },
    {
        "text": "typically, this involves looking for redundancy in the file\u2019s contents and seeing how it can be efficiently encoded."
    },
    {
        "text": "unicode a family of text encoding schemes that cover many more characters compared to ascii (especially alphabets from other languages)."
    },
    {
        "text": "a single character may require a variable number of bytes to encode."
    },
    {
        "text": "utf\u20108 the most popular unicode specification."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 185 13 there is a lot of overlap between the terms \u201cdata science\u201d and \u201cbig data.\u201d in practice, there is a close relationship between them, but really they mean sepa- rate things."
    },
    {
        "text": "big data refers to several trends in data storage and processing, which have posed new challenges, provided new opportunities, and demanded new solutions."
    },
    {
        "text": "often, these big data problems required a level of software engineering expertise that normal statisticians and data analysts weren\u2019t able to handle."
    },
    {
        "text": "it also posed a lot of difficult, ill\u2010posed questions such as how best to segment users based on raw click\u2010stream data."
    },
    {
        "text": "this demand is what turned \u201cdata scientist\u201d into a new, distinct job title."
    },
    {
        "text": "but modern data scientists tackle problems of any scale and only use big data technologies when they\u2019re the right tool for the job."
    },
    {
        "text": "big data is also an area where low\u2010level software engineering concerns become especially important for data scientists."
    },
    {
        "text": "it\u2019s always important that they think hard about the logic of their code, but performance concerns are a strictly secondary concern."
    },
    {
        "text": "in big data though, it\u2019s easy to accidentally add several hours to your code\u2019s runtime, or even have the code fail several hours in due to a memory error, if you do not keep an eye on what\u2019s going on inside the computer."
    },
    {
        "text": "this chapter will start with an overview of two pieces of big data software that are particularly important: the hadoop file system, which stores data on clusters, and the spark cluster computing framework, which can process that data."
    },
    {
        "text": "i will then move on to some of the fundamental concepts that underlie big data frameworks and cluster computing in general, including the famed mapreduce (mr) programming paradigm."
    },
    {
        "text": "13.1 \u00adwhat is big data?"
    },
    {
        "text": "\u201cbig data,\u201d as the term is used today, is a bit of a misnomer."
    },
    {
        "text": "massive datasets have been around for a long time, and nobody gave them a special name."
    },
    {
        "text": "even big data"
    },
    {
        "text": "13 big data 186 today, the largest datasets around are generally well outside of the \u201cbig data\u201d sphere."
    },
    {
        "text": "they are generated from scientific experiments, especially particle accelerators, and processed on custom\u2010made architectures of software and hardware."
    },
    {
        "text": "instead, big data refers to several related trends in datasets (one of which is size) and to the technologies for processing them."
    },
    {
        "text": "the datasets tend to have two properties: 1) they are, as the name suggests, big."
    },
    {
        "text": "there is no special cutoff for when a dataset is \u201cbig.\u201d roughly though, it happens when it is no longer practical to store or process it all on a single computer."
    },
    {
        "text": "instead, we use a cluster of computers, anywhere from a handful of them up to many thousands."
    },
    {
        "text": "the focus is on making our processing scalable, so that it can be distributed over a cluster of arbitrary size with various parts of the analysis going on in parallel."
    },
    {
        "text": "the nodes in the cluster can communicate, but it is kept to a minimum."
    },
    {
        "text": "2) the second thing about big datasets is that they are often \u201cunstructured.\u201d this is a terribly misleading term."
    },
    {
        "text": "it doesn\u2019t mean that there is no structure to the data, but rather that the dataset doesn\u2019t fit cleanly into a traditional relational database, such as sql."
    },
    {
        "text": "prototypical examples would be images, pdfs, html documents, excel files that aren\u2019t organized into clean rows and columns, and machine\u2010generated log files."
    },
    {
        "text": "traditional databases pre- suppose a very rigid structure to the data they contain, and in exchange, they offer highly optimized performance."
    },
    {
        "text": "in big data though, we need the flexibility to process data that comes in any format, and we need to be able to operate on that data in ways that are less predefined."
    },
    {
        "text": "you often pay through the nose for this flexibility when it comes to your software\u2019s runt- ime, since there are very few optimizations that can be prebuilt into the framework."
    },
    {
        "text": "big data requires a few words of caution."
    },
    {
        "text": "the first is that you should be hesitant about using big data tools."
    },
    {
        "text": "they\u2019re all the rage these days, so many people are jumping on the bandwagon blindly."
    },
    {
        "text": "but big data tools are almost always slower, harder to set up, and more finicky than their traditional coun- terparts."
    },
    {
        "text": "this is partly because they\u2019re new technologies that haven\u2019t matured yet, but it\u2019s also inherent to the problems they\u2019re solving: they need to be so flexible to deal with unstructured data, and they need to run on a cluster of computers instead of a stand\u2010alone machine."
    },
    {
        "text": "so if your datasets will always be small enough to process with a single machine, or you only need operations that are supported by sql, you should consider doing that instead."
    },
    {
        "text": "the final word of caution is that even if you are using big data tools, you should probably still be using traditional technologies in conjunction with"
    },
    {
        "text": "13.2 hadoop: the file system and the processor 187 them."
    },
    {
        "text": "for example, i very rarely use big data to do machine learning or data visualization."
    },
    {
        "text": "typically, i use big data tools to extract the relevant features from my data."
    },
    {
        "text": "the extracted features take up much less space compared to the raw dataset, so i can then put the output onto a normal machine and do the actual analytics using something such as pandas."
    },
    {
        "text": "i don\u2019t mean to knock big data tools."
    },
    {
        "text": "they really are fantastic."
    },
    {
        "text": "it\u2019s just that there is so much hype and ignorance surrounding them: like all tools, they are great for some problems and terrible for others."
    },
    {
        "text": "with those disclaimers out of the way, let\u2019s dive in."
    },
    {
        "text": "13.2 \u00adhadoop: the file system and the processor the modern field of big data largely started when google published its seminal paper on mapreduce, a cluster computing framework it had cre- ated to process massive amounts of web data."
    },
    {
        "text": "after reading the paper, an engineer named doug cutting decided to write a free, open\u2010source imple- mentation of the same idea."
    },
    {
        "text": "google\u2019s mr was written in c++, but he decided to do it in java."
    },
    {
        "text": "cutting named this new implementation hadoop, after his daughter\u2019s stuffed elephant."
    },
    {
        "text": "hadoop caught on like wildfire and quickly became almost synonymous with big data."
    },
    {
        "text": "many additional tools were developed that ran on hadoop clusters or that made it easier to write mr jobs for hadoop."
    },
    {
        "text": "there are two parts to hadoop."
    },
    {
        "text": "the first is the hadoop distributed file system (hdfs)."
    },
    {
        "text": "it allows you to store data on a cluster of computers with- out worrying about what data is on which node."
    },
    {
        "text": "instead, you refer to loca- tions in hdfs just as you would for files in a normal directory system."
    },
    {
        "text": "under the hood, hdfs takes care of what data is stored on which node, keeping multiple copies of the data in case some node fails and other boilerplate."
    },
    {
        "text": "the second part of hadoop is the actual mr framework, which reads in data from hdfs, processes it in parallel, and writes its output to hdfs."
    },
    {
        "text": "i\u2019m actually not going to say much about the hadoop mr framework, because ironically it\u2019s a bit of a dinosaur these days (shows you how quickly big data is evolving!)."
    },
    {
        "text": "there is a huge amount of overhead for its mr jobs (most damningly, it always reads its input from disk and writes output to disk, and disk io is much more time\u2010consuming than just doing things in ram)."
    },
    {
        "text": "additionally, it does a really lousy job of integrating with more conventional programming languages."
    },
    {
        "text": "the community\u2019s focus has shifted toward other tools, which still operate on data in hdfs, most notably spark, and i\u2019ll dwell more on them."
    },
    {
        "text": "13 big data 188 13.3 \u00adusing hdfs data can be accessed and manipulated in hdfs through a command line inter- face, which is based on the standard bash shell."
    },
    {
        "text": "the following are the main commands you will use: example command what is does hadoop fs \u2010ls /an/hdfs/location display content of the hdfs directory hadoop fs \u2010copyfromlocal myfile.txt /an/ hdfs/location copy data from the local machine to a location in hdfs hadoop fs \u2010copytolocal /hdfs/path some/ local/directory copy data from hdfs down to the local computer hadoop fs \u2010cat /path/in/hdfs print the contents of a file in hdfs to the screen hadoop fs \u2010mv /hdfs/location1 /hdfs/ location2 move data from one location in hdfs to another hadoop fs \u2010rm /file/in/hdfs delete data in hdfs hadoop fs \u2010rmr /some/hdfs/directory recursively delete a directory in hdfs hadoop fs \u2010appendtofile localfile.txt /user/ hadoop/hadoopfile append local data to a file in hdfs these commands are pretty spartan, but they do everything you need to."
    },
    {
        "text": "one of the first things you will notice as you work with hdfs is that many of the files have names such as part\u2010m\u201000000."
    },
    {
        "text": "this is a near\u2010universal convention among data processing tools in hadoop."
    },
    {
        "text": "the directory containing those files will be the output of a single job that was distributed across multiple nodes in the cluster."
    },
    {
        "text": "the files themselves will be the outputs of different parts of the job that were going on in parallel, and they are stored on the cluster node that generated them."
    },
    {
        "text": "occasionally, it\u2019s more convenient to just use hdfs rather than the big data analysis tools themselves."
    },
    {
        "text": "this is because most of the standard tools involve tremendous overhead, but the hdfs commands are very fast."
    },
    {
        "text": "when used in conjunction with the normal bash shell, you can often get an edge in perfor- mance and convenience over the main tools."
    },
    {
        "text": "for example, if my dataset is rela- tively small, i might do the following to pull out only the rows in a dataset that contain the word \u201cboston\u201d and save them to a local file: hadoop fs -cat /my/dataset/* | grep boston \\ > rows_containing_boston.csv the downside to doing stuff such as this is that the data will all get pulled to the master node in the cluster and processed there \u2013 this isn\u2019t a parallel thing."
    },
    {
        "text": "13.4 example pyspark script 189 but if the dataset in hdfs is small enough for this to be feasible, it can be a lot quicker than the standard ways."
    },
    {
        "text": "13.4 \u00adexample pyspark script pyspark is the most popular way for python users to work with big data."
    },
    {
        "text": "it operates as a python shell, but it has a library called pyspark, which lets you plug into the spark computational framework and parallelize your computa- tions across a cluster."
    },
    {
        "text": "the code reads similarly to normal python, except that there is a sparkcontext object whose methods let you access the spark framework."
    },
    {
        "text": "this script, whose content i will explain later, uses parallel computing to calculate the number of times every word appears in a text document."
    },
    {
        "text": "# create the sparkcontext object from pyspark import sparkconf, sparkcontext conf = sparkconf() sc = sparkcontext(conf=conf) # read file lines and parallelize them # over the cluster in a spark rdd lines = open(\"myfile.txt \") lines_rdd = sc.parallelize(lines) # remove punctuation, make lines lowercase def clean_line(s): s2 = s.strip().lower() s3 = s2.replace(\"."
    },
    {
        "text": "\",\"\").replace(\",\",\"\") return s3 lines_clean = lines_rdd.map(clean_line) # break each line into words words_rdd = lines_clean.flatmap(lambda l: l.split()) # count words def merge_counts(count1, count2): return count1 + count2 words_w_1 = words_rdd.map(lambda w: (w, 1)) counts = words_w_1.reducebykey(merge_counts) # collect counts and display for word, count in counts.collect(): print \"%s: %i \" % (word, count)"
    },
    {
        "text": "13 big data 190 if spark is installed on your computer and you are in the spark home direc- tory, you can run this script on the cluster with the following command: bin/spark-submit --master yarn-client myfile.py alternatively, you can run the same computation on just a single machine with the following command: bin/spark-submit --master local myfile.py 13.5 \u00adspark overview spark is the leading big data processing technology these days in the hadoop ecosystem, having largely replaced traditional hadoop mr. it is usually more efficient, especially if you are chaining several operations together, and it\u2019s tre- mendously easier to use."
    },
    {
        "text": "from a user\u2019s perspective, spark is just a library that you import when you are using either python or scala."
    },
    {
        "text": "spark is written in scala and runs faster when you call it from scala, but this chapter will introduce the python api, which is called pyspark."
    },
    {
        "text": "the example script at the beginning of this chapter was all pyspark."
    },
    {
        "text": "the spark api itself (names of functions, variables, etc.)"
    },
    {
        "text": "is almost identical between the scala version and the python version."
    },
    {
        "text": "the central data abstraction in pyspark is a \u201cresilient distributed dataset\u201d (rdd), which is just a collection of python objects."
    },
    {
        "text": "these objects are distrib- uted across different nodes in the cluster, and generally, you don\u2019t need to worry about which ones are on which nodes."
    },
    {
        "text": "they can be strings, dictionaries, inte- gers \u2013 more or less whatever you want."
    },
    {
        "text": "an rdd is immutable, so its contents cannot be changed directly, but it has many methods that return new rdds."
    },
    {
        "text": "for instance, in the aforementioned example script, we made liberal use of the \u201cmap\u201d method."
    },
    {
        "text": "if you have an rdd called x and a function called f, then x.map(f) will apply f to every element of x and return the results as a new rdd."
    },
    {
        "text": "rdds come in two types: keyed and unkeyed."
    },
    {
        "text": "unkeyed rdds support opera- tions such as map(), which operate on each element of the rdd independently."
    },
    {
        "text": "often though, we want more complex operations, such as grouping all ele- ments that meet some criteria or joining two different rdds."
    },
    {
        "text": "these operations require coordination between different elements of an rdd, and for these operations, you need a keyed rdd."
    },
    {
        "text": "if you have an rdd that consists of two\u2010element tuples, the first element is considered the \u201ckey\u201d and the second element the \u201cvalue.\u201d we created a keyed rdd and processed it in the aforementioned script with the following lines: words_w_1 = words_rdd.map(lambda w: (w, 1)) counts = words_w_1.reducebykey(merge_counts) here words_w_1 will be a keyed rdd, where the keys are the words and the values are all 1. every occurrence of a word in the dataset will give rise to a"
    },
    {
        "text": "13.5 spark overview 191 different element in words_w_1."
    },
    {
        "text": "the next line uses the reducebykey method to group all values that share a key together and then condense them down to a single aggregate value."
    },
    {
        "text": "i should note that the keyed and unkeyed rdds are not separate classes in the pyspark implementation."
    },
    {
        "text": "it\u2019s just that certain operations you can call (such as reducebykey) will assume that the rdd is structured as key\u2013value pairs, and it will fail at runtime if that is not the case."
    },
    {
        "text": "besides rdds, the other key abstraction the user has to be aware of is the sparkcontext class, which interfaces with the spark cluster and is the entry point for spark operations."
    },
    {
        "text": "conventionally, the sparkcontext in an application will be called sc."
    },
    {
        "text": "generally, pyspark operations come in two types: \u25cf \u25cfcalling methods on the sparkcontext, which create an rdd."
    },
    {
        "text": "in the example script, we used parallelize() to move data from local space into the cluster as an rdd."
    },
    {
        "text": "there are other methods that will create rdds from data that is already distributed, by reading it out of hdfs or another storage medium."
    },
    {
        "text": "\u25cf \u25cfcalling methods on rdds, which either return new rdds or produce output of some kind."
    },
    {
        "text": "most operations in spark are what\u2019s called \u201clazy.\u201d when you type lines_clean = lines_rdd.map(clean_line) no actual computation gets done."
    },
    {
        "text": "instead, spark will just keep track of how the rdd lines_clean is defined."
    },
    {
        "text": "similarly, lines_rdd quite possibly doesn\u2019t exist either and is only implicitly defined in terms of some upstream process."
    },
    {
        "text": "as the script runs, spark is piling up a large dependency structure of rdds defined in terms of each other, but never actually creating them."
    },
    {
        "text": "eventually, you will call an operation that produces some output, such as saving an rdd into hdfs or pulling it down into local python data structures."
    },
    {
        "text": "at that point, the dominos start falling, and all of the rdds that you have previously defined will get cre- ated and fed into each other, eventually resulting in the final side effect."
    },
    {
        "text": "by default, an rdd exists only long enough for its contents to be fed into the next stage of processing."
    },
    {
        "text": "if an rdd that you define is never actually needed, then it will never be brought into being."
    },
    {
        "text": "the problem with lazy evaluation is that sometimes we want to reuse an rdd for a variety of different processes."
    },
    {
        "text": "this brings us to one of the most important aspects of spark that differentiates it from traditional hadoop mr: spark can cache an rdd in the ram of the cluster nodes, so that it can be reused as much as you want."
    },
    {
        "text": "by default, an rdd is an ephemeral data structure that only exists long enough for its contents to be passed into the next stage of processing, but a cached rdd can be experimented with in real time."
    },
    {
        "text": "to cache an rdd in memory, you just call the cache() method on it."
    },
    {
        "text": "this method will not actually create the rdd, but it will ensure that the first time the rdd gets created, it is persisted in ram."
    },
    {
        "text": "13 big data 192 there is one other problem with lazy evaluation."
    },
    {
        "text": "say again that we write the line lines_clean = lines_rdd.map(clean_line) but imagine that the clean_line function will fail for some value in lines_rdd."
    },
    {
        "text": "we will not know this at the time: the error will only arise later in the script, when lines_clean is finally forced to be created."
    },
    {
        "text": "if you are debugging a script, a tool that i use is to call the count() method on each rdd as soon as it is declared."
    },
    {
        "text": "the count() method counts the elements in the rdd, which forces the whole rdd to be created, and will raise an error if there are any problems."
    },
    {
        "text": "the count() operation is expensive, and you should certainly not include those steps in code that gets run on a regular basis, but it\u2019s a great debugging tool."
    },
    {
        "text": "13.6 \u00adspark operations this section will give you a rundown of the main methods that you will call on the sparkcontext object and on rdds."
    },
    {
        "text": "together, these methods are everything you will do in a pyspark script that isn\u2019t pure python."
    },
    {
        "text": "the sparkcontext object has the following methods: \u25cf \u25cfsc.parallelize(my_list): takes in a list of python objects and distributes them across the cluster to create an rdd."
    },
    {
        "text": "\u25cf \u25cfsc.textfile(\u201c/some/place/in/hdfs\u201d): takes in the location of text files in hdfs and returns an rdd containing the lines of text."
    },
    {
        "text": "\u25cf \u25cfsc.picklefile(\u201c/some/place/in/hdfs\u201d): takes a location in hdfs that stores python objects that have been serialized using the pickle library."
    },
    {
        "text": "deserializes the python objects and returns them as an rdd."
    },
    {
        "text": "this is a really useful method."
    },
    {
        "text": "\u25cf \u25cfaddfile(\u201cmyfile.txt\u201d): copies myfile.txt from the local machine to every node in the cluster, so that they can all use it in their operations."
    },
    {
        "text": "\u25cf \u25cfaddpyfile(\u201cmylib.py\u201d): copies mylib.py from the local machine to every node in the cluster, so that it can be imported as a library and used by any node in the cluster."
    },
    {
        "text": "the main methods you will use on an rdd are as follows: \u25cf \u25cfrdd.map(func): applies func to every element in the rdd and returns that results as an rdd."
    },
    {
        "text": "\u25cf \u25cfrdd.filter(func): returns an rdd containing only those elements x of rdd for which func(x) evaluates to true."
    },
    {
        "text": "\u25cf \u25cfrdd.flatmap(func): applies func to every element in the rdd."
    },
    {
        "text": "func(x) doesn\u2019t return just a single element of the new rdd: it returns a list of new elements,"
    },
    {
        "text": "13.7 two ways to run pyspark 193 so that one element in the original rdd can turn into many in the new one."
    },
    {
        "text": "or, an element in the original rdd might result in an empty list and hence no elements of the output rdd."
    },
    {
        "text": "\u25cf \u25cfrdd.take(5): computes five elements of rdd and returns them as a python list."
    },
    {
        "text": "very useful when debugging, since it only computes those five elements."
    },
    {
        "text": "\u25cf \u25cfrdd.collect(): returns a python list containing all the elements of the rdd."
    },
    {
        "text": "make sure you only call this if the rdd is small enough that it will fit into the memory of a single computer."
    },
    {
        "text": "\u25cf \u25cfrdd.saveastextfile(\u201c/some/place/in/hdfs\u201d): saves an rdd in hdfs as a text file."
    },
    {
        "text": "useful for an rdd of strings."
    },
    {
        "text": "\u25cf \u25cfrdd.saveaspicklefile(\u201c/some/place/in/hdfs\u201d): serializes every object in pickle format and stores them in hdfs."
    },
    {
        "text": "useful for rdds of complex python objects, such as dictionaries and lists."
    },
    {
        "text": "\u25cf \u25cfrdd.distinct(): filters out all duplicates."
    },
    {
        "text": "\u25cf \u25cfrdd1.union(rdd2): combines elements of rdd1 and rdd2 into a single rdd."
    },
    {
        "text": "\u25cf \u25cfrdd.cache(): whenever rdd is actually created, it will be cached in ram so that it doesn\u2019t have to be re\u2010created later."
    },
    {
        "text": "\u25cf \u25cfrdd.keyby(func): this is a simple wrapper for making keyed rdds, since it is such a common use case."
    },
    {
        "text": "this is equivalent to rdd.map(lambda x: (func(x), x))."
    },
    {
        "text": "\u25cf \u25cfrdd1.join(rdd2): this works on two keyed rdds."
    },
    {
        "text": "if (k, v1) is in rdd1 and (k, v2) is in rdd2, then (k, (v1, v2)) will be in the output rdd."
    },
    {
        "text": "\u25cf \u25cfrdd.reducebykey(func): for every unique key in the keyed rdd, this collects all of its associated values and aggregates them together using func."
    },
    {
        "text": "\u25cf \u25cfrdd.groupbykey(func): func will be passed a tuple of two things \u2013 a key and an iterable object that will give it all of the values in rdd that share that key."
    },
    {
        "text": "its output will be an element in the resulting rdd."
    },
    {
        "text": "13.7 \u00adtwo ways to run pyspark pyspark can be run either by submitting a stand\u2010alone python script or by opening up an interpreted session where you can enter your python commands one at a time."
    },
    {
        "text": "in the previous example, we ran our script by saying bin/spark-submit --master yarn-client myfile.py the spark\u2010submit command is what we use for stand\u2010alone scripts."
    },
    {
        "text": "if instead we wanted to open up an interpreter, we would say bin/pyspark --master yarn-client this would open up a normal\u2010looking python terminal, from which we could import the pyspark libraries."
    },
    {
        "text": "13 big data 194 from the perspective of writing code, the key difference between a stand\u2010 alone script and an interpreter session is that in the script we had to explicitly create the sparkcontext object, which we called sc."
    },
    {
        "text": "it was done with the fol- lowing lines: from pyspark import sparkconf, sparkcontext conf = sparkconf() sc = sparkcontext(conf=conf) if you open up an interpreter though, it will automatically contain the sparkcontext object and call it sc."
    },
    {
        "text": "no need to create it manually."
    },
    {
        "text": "the reason for this difference is that stand\u2010alone scripts often need to set a lot of configuration parameters so that somebody who didn\u2019t write them can still run them reliably."
    },
    {
        "text": "calling various methods on the sparkconf object sets those configurations."
    },
    {
        "text": "the assumption is that if you open an interpreter directly, then you will set the configurations yourself from the command line."
    },
    {
        "text": "13.8 \u00adconfiguring spark clusters are finicky things."
    },
    {
        "text": "you need to make sure that every node has the data it needs, the files it relies on, no node gets overloaded, and so on."
    },
    {
        "text": "you need to make sure that you are using the right amount of parallelism, because it\u2019s easy to make your code slower by having it be too parallel."
    },
    {
        "text": "finally, multiple people usually share a cluster, so the stakes are much higher if you hog resources or crash it (which i have done \u2013 trust me, people get irate when the whole cluster dies)."
    },
    {
        "text": "all of this means that you need to have an eye toward how your job is configured."
    },
    {
        "text": "this section will give you the most crucial parts."
    },
    {
        "text": "all of the configurations can be set from the command line."
    },
    {
        "text": "the ones you are most likely to have to worry about are the following: \u25cf \u25cfname: a human\u2010readable name to give your process."
    },
    {
        "text": "this doesn\u2019t affect the running, but it will show up in the cluster monitoring software so that your sys admin can see what resources you\u2019re taking up."
    },
    {
        "text": "\u25cf \u25cfmaster: this identifies the \u201cmaster\u201d process, which deals with parallelizing your job (or running it in local mode)."
    },
    {
        "text": "usually, \u201cyarn\u2010client\u201d will send your job to the cluster for parallel processing, while \u201clocal\u201d will run it locally."
    },
    {
        "text": "there are other masters available sometimes, but local and yarn are the most common."
    },
    {
        "text": "perhaps surprisingly, the default master is local rather than yarn; you have to explicitly tell pyspark to run in parallel if you want parallelism."
    },
    {
        "text": "\u25cf \u25cfpy\u2010files: a comma\u2010separate list of any python library files that need to be copied to other nodes in the cluster."
    },
    {
        "text": "this is necessary if you want to use that library\u2019s functionality in your pyspark methods, because under the hood, each node in the cluster will need to import the library independently."
    },
    {
        "text": "13.9 under the hood 195 \u25cf \u25cffiles: a comma\u2010separated list of any additional files that should be put in the working directory on each node."
    },
    {
        "text": "this might include configuration files spe- cific to your task that your distributed functionality depends on."
    },
    {
        "text": "\u25cf \u25cfnum\u2010executors: the number of executor processes to spawn in the cluster."
    },
    {
        "text": "they will typically be on separate nodes."
    },
    {
        "text": "the default is 2."
    },
    {
        "text": "\u25cf \u25cfexecutor\u2010cores: the number of cpu cores each executor process should take up."
    },
    {
        "text": "the default is 1. an example of how this might look setting parameters from the command line is as follows: bin/pyspark \\ --name my_pyspark_process \\ --master yarn-client \\ --py-files mylibrary.py,otherlibrary.py \\ --files myfile.txt,otherfile.txt \\ --num-executors 5 \\ --executor-cores 2 if instead you want to set them inside a stand\u2010alone script, it will be as follows: from pyspark import sparkconf, sparkcontext conf = sparkconf() conf.setmaster(\"yarn-client\") conf.setappname(\"my_pyspark_process\") conf.set(\"spark.num.executors\", 5) conf.set(\"spark.executor.cores\", 2) sc = sparkcontext(conf=conf) sc.addpyfile(\"mylibrary.py\") sc.addpyfile(\"otherlibrary.py\") sc.addfile(\"myfile.txt\") sc.addfile(\"otherfile.txt\") 13.9 \u00adunder the hood in my mind, pyspark makes a lot more sense when you understand just a little bit about what\u2019s going on under the hood."
    },
    {
        "text": "here are some of the main points: \u25cf \u25cfwhen you use the \u201cpyspark\u201d command, it will actually run the \u201cpython\u201d command on your computer and just make sure that it links to the appropri- ate spark libraries (and that the sparkcontext object is already in the names- pace, if you\u2019re running in interactive mode)."
    },
    {
        "text": "this means that any python"
    },
    {
        "text": "13 big data 196 libraries you have installed on your main node are available to you within your pyspark script."
    },
    {
        "text": "\u25cf \u25cfwhen running in cluster mode, the spark framework cannot run python code directly."
    },
    {
        "text": "instead, it will kick off a separate python process on each node and run it as a separate subprocess."
    },
    {
        "text": "if your code needs libraries or additional files that are not present on a node, then the process that is on that node will fail."
    },
    {
        "text": "this is the reason you must pay attention to what files get shipped around."
    },
    {
        "text": "\u25cf \u25cfwhenever possible, a segment of your data is confined to a single python process."
    },
    {
        "text": "if you call map(), flatmap(), or a variety of other pyspark operations, each node will operate on its own data."
    },
    {
        "text": "this avoids sending data over the network, and it also means that everything can stay at python objects."
    },
    {
        "text": "\u25cf \u25cfit is very computationally expensive to have the nodes shift data around between them."
    },
    {
        "text": "not only do we have to actually move the data around, but also the python objects must be serialized into a string\u2010like format before we send them over the wire and then rehydrated on the other end."
    },
    {
        "text": "\u25cf \u25cfoperations such as groupbykey() will require serializing data and moving it between nodes."
    },
    {
        "text": "this step in the process is called a \u201cshuffle.\u201d the python pro- cesses are not involved in the shuffle."
    },
    {
        "text": "they just serialize the data and then hand it off to the spark framework."
    },
    {
        "text": "13.10 \u00adspark tips and gotchas here are a few parting tips for using spark, which i have learned from experi- ence and/or hard lessons: 1) rdds of dictionaries make both code and data much more understandable."
    },
    {
        "text": "if you\u2019re working with csv data, always convert it to dictionaries as your first step."
    },
    {
        "text": "yeah, it takes up more space because every dictionary has copies of the keys, but it\u2019s worth it."
    },
    {
        "text": "2) store things in pickle files rather than in text files if you\u2019re likely to operate on them later."
    },
    {
        "text": "it\u2019s just so much more convenient."
    },
    {
        "text": "3) use take() while debugging your scripts to see what format your data is in (rdd of dictionaries?"
    },
    {
        "text": "of tuples?"
    },
    {
        "text": "etc.)."
    },
    {
        "text": "4) running count() on a rdd is a great way to force it to be created, which will bring any runtime errors to the surface sooner rather than later."
    },
    {
        "text": "5) do most of your basic debugging in local mode rather than in distributed mode, since it goes much faster if your dataset is small enough."
    },
    {
        "text": "plus, you reduce the chances that something will fail because of bad cluster configuration."
    },
    {
        "text": "6) if things work fine in local mode but you\u2019re getting weird errors in distrib- uted mode, make sure that you\u2019re shipping the necessary files across the cluster."
    },
    {
        "text": "13.11 the mapreduce paradigm 197 7) if you\u2019re using the \u2010\u2010files option from the command line to distribute files across the cluster, make sure that the list is separated by commas rather than colons."
    },
    {
        "text": "i lost two days of my life to that one... now that we have seen pyspark in action, let\u2019s step back and consider some of what\u2019s going on here in the abstract."
    },
    {
        "text": "13.11 \u00adthe mapreduce paradigm mapreduce is the most popular programming paradigm for big data tech- nologies."
    },
    {
        "text": "it makes programmers write their code in a way that can be easily parallelized across a cluster of arbitrary size or an arbitrarily large dataset."
    },
    {
        "text": "some variant of mr underlies many of the major big data tools, including spark, and probably will for the foreseeable future, so it\u2019s very important to understand it."
    },
    {
        "text": "an mr job takes a dataset, such as a spark rdd, as input."
    },
    {
        "text": "there are then two stages to the job: \u25cf \u25cfmapping."
    },
    {
        "text": "every element of the dataset is mapped, by some function, to a collection of key\u2013value pairs."
    },
    {
        "text": "in pyspark, you can do this with the flatmap method."
    },
    {
        "text": "\u25cf \u25cfreducing."
    },
    {
        "text": "for every unique key, a \u201creduce\u201d process is kicked off."
    },
    {
        "text": "it is fed all of its associated values one at a time, in no particular order, and eventually, it produces some outputs."
    },
    {
        "text": "you can implement this using the reducebykey method in pyspark."
    },
    {
        "text": "and that\u2019s all there is to it: the programmer writes the code for the mapper function, and they write the code for the reducer, and that\u2019s it."
    },
    {
        "text": "no worrying about the size of the cluster, what data is where, and so on."
    },
    {
        "text": "in the example script i gave, spark will end up optimizing the code into a single mr job."
    },
    {
        "text": "here is the code rewritten so as to make it explicit: def mapper(line): l2 = l.strip().lower() l3 = l2.replace(\"."
    },
    {
        "text": "\",\"\").replace(\",\",\"\") words = l3.split() return [(w, 1) for w in words] def reducer_func(count1, count2): return count1 + count2 lines = open(\"myfile.txt\") lines_rdd = sc.parallelize(lines)"
    },
    {
        "text": "13 big data 198 map_stage_out = lines_rdd.flatmap(mapper) reduce_stage_out = \\ map_stage_out.reducebykey(reducer_func) what happens under the hood in an mr job is the following: \u25cf \u25cfthe input dataset starts off being distributed across several nodes in the cluster."
    },
    {
        "text": "\u25cf \u25cfeach of these nodes will, in parallel, apply the mapping function to all of its pieces of data to get key\u2013value pairs."
    },
    {
        "text": "\u25cf \u25cfeach node will use the reducer to condense all of its key\u2013value pairs for a particular word into just a single one, representing how often that word occurred in the node\u2019s data."
    },
    {
        "text": "again, this happens completely in parallel."
    },
    {
        "text": "\u25cf \u25cffor every distinct key that is identified in the cluster, a node in the cluster is chosen to host the reduce process."
    },
    {
        "text": "\u25cf \u25cfevery node will forward each of its partial counts to the appropriate reducer."
    },
    {
        "text": "this movement of data between nodes is often the slowest stage of the whole mr job \u2013 even slower than the actual processing."
    },
    {
        "text": "\u25cf \u25cfevery reduce process runs in parallel on all of its associated values, calculat- ing the final word counts."
    },
    {
        "text": "the overall workflow is displayed in the following diagram: to be or not to be to code or not to code (to, 1) (to, 2) (to, 2) (to, 2) (to, 2) (be, 2) (be, 2) (be, 2) (to, 4) (or, 1) (or, 1) (or, 1) (or, 1) (or, 2) (or, 1) (to, 1) (or, 1) (to, 1) (to, 1) (be, 1) (code, 1) (code, 1) (code, 2) (code, 2) (code, 2) (be, 1) (not, 1) (not, 1) (not, 1) (not, 1) (not, 1) (not, 2) (not, 1) map reduce locally shuffle reduce raw data key/value pairs final output"
    },
    {
        "text": "13.12 performance considerations 199 there is one thing i have done here that breaks from classical mr. i have said that each node uses the reducer to condense all of its key\u2013value pairs for a particular word into just one."
    },
    {
        "text": "that stage is technically a performance optimi- zation called a \u201ccombiner.\u201d i was only able to use a combiner because my reducer was just doing addition, and it doesn\u2019t matter what order you add things up in."
    },
    {
        "text": "in the most general case, those mapper outputs are not con- densed \u2013 they are all sent to whichever node is doing the reducing for that word."
    },
    {
        "text": "this puts a massive strain on the bandwidth between the clusters, so you want to use combiners whenever possible."
    },
    {
        "text": "13.12 \u00adperformance considerations there are several guidelines applicable to any mr framework, including spark: \u25cf \u25cfif you are going to filter data out, do it as early as possible."
    },
    {
        "text": "this reduces network bandwidth."
    },
    {
        "text": "\u25cf \u25cfthe job only finishes when the last reduce process is done, so try to avoid a situation where one reducer is handling most of the key\u2013value pairs."
    },
    {
        "text": "\u25cf \u25cfif possible, more reducers means each one has to handle fewer key\u2013value pairs."
    },
    {
        "text": "in traditional coding, the name of the game in performance optimization is to reduce the number of steps your code takes."
    },
    {
        "text": "this is usually a secondary concern in mr. the biggest concern instead becomes the time it takes to move data from node to node across the network."
    },
    {
        "text": "and the number of steps your code takes doesn\u2019t matter so much \u2013 instead, it\u2019s how many steps your worst node takes."
    },
    {
        "text": "there is one other specific optimization with spark in particular that i should mention, which doesn\u2019t come up all that often but can be a huge deal when it does."
    },
    {
        "text": "sometimes, reducebykey is the wrong method to use."
    },
    {
        "text": "in particular, it is very inefficient when your aggregated values are large, mutable data structure."
    },
    {
        "text": "take this dummy code, for example, which takes all occurrences of a word and puts them into a big list: def mapper(line): return [(w, [w]) for w in line.split()] def red_func(lst1, lst2): return lst1 + lst2 result = lines.flatmap(mapper).reducebykey(red_func) as i\u2019ve written it, every time red_func is called, it is given two potentially very long lists."
    },
    {
        "text": "it will then create a new list in memory (which takes quite a bit"
    },
    {
        "text": "13 big data 200 of time) and then delete the original lists."
    },
    {
        "text": "this is horribly abusive to the mem- ory, and i\u2019ve seen jobs die because of it."
    },
    {
        "text": "intuitively, what you want to do is keep a big list and just append all the words to it, one at a time, rather than constantly creating new lists."
    },
    {
        "text": "that can be accomplished with the aggregatebykey function, which is a little more compli- cated to use compared to reducebykey, but much more efficient if you use it correctly."
    },
    {
        "text": "example code is here: def update_agg(agg_list, new_word): agg_list.append(new_word) return agg_list # same list!"
    },
    {
        "text": "def merge_agg_lists(agg_list1, agg_list2): return agg_list1 + agg_list2 def reducer(l1, l2): return l1 + l2 result = lines.flatmap(mapper).aggregatebykey( [], update_agg, merge_agg_lists) in this case, each node in the cluster will start off with the empty list as its aggregate for a particular word."
    },
    {
        "text": "then it will feed that aggregate, along with each instance of the word, into update_agg."
    },
    {
        "text": "then update_agg will append the new value to the list, rather than creating a new one, and return the updated list as a result."
    },
    {
        "text": "the function merge_agg_lists still operates the original way, but it is only called a few times to merge the outputs of the different nodes."
    },
    {
        "text": "13.13 \u00adfurther reading 1 spark programming guide, viewed 8 august 2016, http://spark.apache.org/ docs/latest/programming\u2010guide.html 2 dean, j & ghemawat, s, mapreduce: simplified data processing on large clusters, paper presented at: sixth symposium on operating system design and implementation, 2014, san francisco, ca 13.14 \u00adglossary big data a movement in the analytics and software community that focuses on large, unstructured datasets and how they can be analyzed on clusters of computers."
    },
    {
        "text": "combiner a performance optimization in mapreduce frameworks where each node partially completes the reduce process on its own key\u2013value pairs."
    },
    {
        "text": "13.14 glossary 201 cluster a collection of computers that can be programmed to coordinate a single computation across them."
    },
    {
        "text": "hadoop a cluster storage and computing framework that has become the main piece of many big data ecosystems."
    },
    {
        "text": "key\u2013value pair a tuple with two elements."
    },
    {
        "text": "the second element is the \u201cvalue,\u201d which is usually a piece of data."
    },
    {
        "text": "the first element is a \u201ckey,\u201d which is usually a label indicating some category the tuple falls into."
    },
    {
        "text": "map an operation where you take a collection of data structures and apply the same function to each of them."
    },
    {
        "text": "the outputs of the functions are, collectively, the output of the process."
    },
    {
        "text": "mapreduce the most prominent paradigm for programming a cluster of computers in a completely parallel way."
    },
    {
        "text": "node a single computer in a cluster."
    },
    {
        "text": "pyspark the python interface to the spark cluster computing framework."
    },
    {
        "text": "rdd short for resilient distributed dataset."
    },
    {
        "text": "reduce an operation where a stream of values are processed one at a time, updating an aggregate with each value."
    },
    {
        "text": "after the last value, the aggregate is returned as the result of the process."
    },
    {
        "text": "resilient distributed dataset the abstraction in spark, an immutable collection of data objects that are distributed across a cluster."
    },
    {
        "text": "spark the leading cluster computing framework."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 203 14 databases play an important role in data science, but data scientists coming from backgrounds other than programming are often woefully ignorant of them."
    },
    {
        "text": "that was certainly my own experience!"
    },
    {
        "text": "in fact, i didn\u2019t even really appre- ciate what the role of a \u201cdatabase\u201d was and why you would use one as opposed to just files of data organized in a directory structure."
    },
    {
        "text": "a database is ultimately just a framework for storing and efficiently accessing data."
    },
    {
        "text": "a prototypical database is a burly server, which holds more data than would fit into a normal computer, stores it in a way that it can be quickly accessed (this usually involves a ton of under\u2010the\u2010hood optimizations that the database user is blissfully ignorant of), and is standing at the ready to field requests from other computers to access or modify the data."
    },
    {
        "text": "the main advan- tage of a database relative to raw files is performance, especially if you are run- ning a time\u2010sensitive service (such as a web )."
    },
    {
        "text": "databases also handle other overhead, such as keeping multiple copies of data synced up and moving data between different storage media."
    },
    {
        "text": "on the smaller end, many pieces of single\u2010computer software will run a data- base under the hood, using it as an efficient way to store one program\u2019s data."
    },
    {
        "text": "on the larger end, many databases span hundreds of different servers, with complicated protocols for syncing with each other, and users access them over the internet or a local network."
    },
    {
        "text": "the idea of a single database in this case is really an abstraction that lets the user ignore which of the physical servers they are actually communicating with."
    },
    {
        "text": "strictly speaking, a \u201cdatabase\u201d refers to the data itself and its organization, while \u201cdatabase management system\u201d (dbms) is the software framework that provides access to that data."
    },
    {
        "text": "in practice, these terms are often used inter- changeably, and i will be pretty casual about the distinction in this book."
    },
    {
        "text": "there are many, many ways that databases can be accessed."
    },
    {
        "text": "in production systems, it is often through programmatic apis that are called through what- ever language you are writing your code in."
    },
    {
        "text": "however, most databases also have their own command\u2010line\u2010based shells."
    },
    {
        "text": "this chapter will focus on that use case."
    },
    {
        "text": "databases"
    },
    {
        "text": "14 databases 204 by far the most important family of databases is the sql family, which sup- ports the relational database (rdb) model (described in the next section)."
    },
    {
        "text": "sql\u2010like databases have been around for a long time."
    },
    {
        "text": "they are generally wick- edly fast and support very extensive processing of the data, but in exchange for this power, they are extremely rigid as to the type and formats of the data you can put in them."
    },
    {
        "text": "in recent years, the so\u2010called nosql databases are often more flexible in the types of data they will store, but they do not offer the same com- putational power."
    },
    {
        "text": "14.1 \u00adrelational databases and mysql\u00ae in an rdb, a dataset is represented by a table with rows (often unordered) and columns."
    },
    {
        "text": "each column has a specific data type associated with it, such as an integer, a time stamp, or a string (typically with a known maximum length."
    },
    {
        "text": "they are called varchars in sql lingo."
    },
    {
        "text": "an rdb has a \u201cquery language\u201d associated with it, which lets users specify which data should be selected and any preprocessing/aggregation that should be done before it is returned."
    },
    {
        "text": "the database is structured so that those queries can be answered extremely efficiently."
    },
    {
        "text": "the sql family is a large class of relational databases, which have nearly identical interfaces."
    },
    {
        "text": "of these, mysql is the most popular open\u2010source version and the one you\u2019re most likely to encounter in data science."
    },
    {
        "text": "this section will be based on mysql, but know that almost everything translates over to the rest of the sql family."
    },
    {
        "text": "in fact, it will apply well outside of that even; sql syntax is ubiquitous, and many data processing languages borrow heavily from it."
    },
    {
        "text": "14.1.1 basic queries and grouping the data in a mysql server consists of a collection of tables, whose columns are of known types."
    },
    {
        "text": "the tables are organized into \u201cdatabases.\u201d a database is just a namespace for tables that keeps them more organized; you can switch between namespaces easily or combine tables from several namespaces in a single analysis."
    },
    {
        "text": "a simple mysql query will illustrate some of the core syntax: use my_database; select name, age from people where state='wa'; the first line is saying that we will be referring to tables in a database called my_database."
    },
    {
        "text": "next, it is assumed that you have a table called \u201cpeople\u201d within"
    },
    {
        "text": "205 14.1 relational databases and mysql my_database, with columns name, age, and state (and possibly other columns)."
    },
    {
        "text": "this query will give you the name and age of all people living in washington state."
    },
    {
        "text": "if we had instead said \u201cselect *\u201d, it would have been shorthand for selecting all of the columns in the table."
    },
    {
        "text": "this selection of rows and columns is the most basic functionality of mysql."
    },
    {
        "text": "it is also possible to omit the use statement and put the name of the data- base explicitly in your query: select name, age from my_database.people where state='wa'; aside from just selecting columns, it is also possible to apply operations to the columns before they are returned."
    },
    {
        "text": "mysql has a wide range of built\u2010in functions for operating on the data fields that can be used in both the select clause and the where clause."
    },
    {
        "text": "for example, the following query will get peo- ple\u2019s first names and whether or not they are a senior citizen: select substring(name,0,locate(name,' ')), (age >= 65) from people where state='wa'; note the somewhat clunky syntax for getting somebody\u2019s first name."
    },
    {
        "text": "the term locate(name,\u2019 \u2019) will give us the index of the space in the name, that is, where the first name ends."
    },
    {
        "text": "then, substring(name,0,locate (name,\u2019 \u2019)) gives us the name up to that point, that is, the first name."
    },
    {
        "text": "in python, it would have made more sense to split the string on whitespace and then take the first parts."
    },
    {
        "text": "but doing so calls into existence a list, which can be an arbitrarily complex data structure."
    },
    {
        "text": "this is anathema in performance\u2010focused mysql!"
    },
    {
        "text": "mysql\u2019s functions generally don\u2019t permit complex data types such as lists; they limit themselves to functions that can be made blindingly efficient when performed at massive scale."
    },
    {
        "text": "this forces us to extract the first name in this roundabout way."
    },
    {
        "text": "the following table summarizes a few of the more useful functions: function name description abs absolute value coalesce take the first non\u2010null value of the functions arguments (very useful after joins) concat concatenate several strings convert_tz convert from one time zone to another date extract the date from a datetime expression"
    },
    {
        "text": "14 databases 206 function name description dayofmonth the data in a month dayofweek day of the week floor round a number down hour get the hour out of a datetime length length of a string in bytes lower return a string in lowercase locate return the index of the first occurrence of a substring in a larger string lpad pad a string to a given length by adding a particular character to its left now the current datetime pow raise a number to a power regexp whether a string matches a regular expression replace replace all occurrences of a particular substring in a string with a different substring sqrt square root of a number trim strip spaces from both sides of a string upper return the upper\u2010case version of a string besides just selecting rows/columns and operating on them, it is possible to aggregate many rows into a single returned value using a group\u2010by state- ment."
    },
    {
        "text": "for example, this query will find the number of people named helen in each state."
    },
    {
        "text": "select state, count(name) from people group by state where first_name='helen'; the count() function used here is just one of many aggregator functions, which condense one columns from many rows into a single value."
    },
    {
        "text": "several oth- ers are listed in the following table: function name description max max value min min value avg average value stddev standard deviation"
    },
    {
        "text": "207 14.1 relational databases and mysql function name description variance variance sum sum it is also possible to group by several fields, as in the following query: select state, city, count(name) from people group by state, city where first_name='helen'; a final word about basic queries is that you can give names to the columns you select, as in the following query: select state as the_state, city as where_they_live, count(name) as num_people from people group by state, city where first_name='helen'; you could also have only renamed some of the columns."
    },
    {
        "text": "this renaming within the select clause doesn\u2019t have any effect if all you\u2019re doing is pulling the data out."
    },
    {
        "text": "however, it becomes extremely useful if you are writing the results of your query into another table with its own column names or if you are work- ing with several tables in the same query."
    },
    {
        "text": "more on those will follow later."
    },
    {
        "text": "14.1.2 joins the final ingredient in the query language is the ability to join one table with another, which is a complicated enough topic that i\u2019m giving it its own section."
    },
    {
        "text": "in a join, several tables are combined into one, with rows from the input tables being matched up based on some criteria (usually having specified fields in common, but you can use other criteria too)."
    },
    {
        "text": "joining is illustrated in this query, which tells us how many employees of each job title live in each state: select p.state, e.job_title, count(p.name) from people p join employees e on p.name=e.name group by p.state, e.job_title; there are two things to notice about the new query."
    },
    {
        "text": "first, there is a join clause, giving the table to be joined with people, and an on clause, giving the"
    },
    {
        "text": "14 databases 208 criteria for when rows in the tables match."
    },
    {
        "text": "the second thing to notice is that \u201cpeople p\u201d and \u201cemployees e\u201d give shorter aliases to the tables, and all columns are prefixed by the alias."
    },
    {
        "text": "this eliminates ambiguity, in case columns of the same name occur in both tables."
    },
    {
        "text": "every row under people will get paired up with every row under employees that it matches in the final table."
    },
    {
        "text": "so, if 5 rows under people have the name helen and 10 rows under employees have the name helen, there will be 50 rows for helen in the joined table."
    },
    {
        "text": "this potential for blowing up the size of your data is one reason that joins can be very costly operations to perform."
    },
    {
        "text": "the aforementioned query performs what is called an \u201cinner join.\u201d this means that if a row under people does not match any rows under employees, then it will not appear in the joined table."
    },
    {
        "text": "similarly, any row under employees that does not match a row under people will be dropped."
    },
    {
        "text": "you could instead have done a \u201cleft outer join.\u201d in that case, an orphaned row under people will still show up in the joined table, but it will have null in all the columns that come from the employees table."
    },
    {
        "text": "similarly, a \u201cright outer join\u201d will make sure that every row under employees shows up at least once."
    },
    {
        "text": "outer joins are extremely common in situations where there is one \u201cprimary\u201d table."
    },
    {
        "text": "say, you are trying to predict whether a person will click on an ad, and you have one table that describes every ad in your database, what company/product it was for, who the ad was shown to, and whether it got clicked on."
    },
    {
        "text": "you might also have a table describing different companies, what industries they are in, and so on."
    },
    {
        "text": "doing a left outer join between the ads and the companies will effec- tively just be adding additional columns to the ad table, giving new features that you might want to train a classifier on or calculate correlations between."
    },
    {
        "text": "any companies that you didn\u2019t show ads for are superfluous for your study, and so get dropped, and any ad for which you happen to be missing the company data still stays put in your analysis."
    },
    {
        "text": "its company\u2010related fields will just be null."
    },
    {
        "text": "14.1.3 nesting queries the key operations in mysql are select, where, groupby, and join."
    },
    {
        "text": "most mysql queries you will see in practice will use each of these at most once, but it is also possible to nest queries within each other."
    },
    {
        "text": "this query takes a table of employees for a company, performs a query that counts how many employees are in each city, and then joins this result back to the original table to find out how many local coworkers each employee has: select ppl.name as employee_name, counts.num_ppl_in_city\u20101 as num_coworkers from ( select city,"
    },
    {
        "text": "209 14.1 relational databases and mysql count(p.name) as num_ppl_in_city from people group by p.city ) counts join people ppl on counts.city=ppl.city; the things to notice in this query are as follows: 1) the subquery is enclosed in parentheses."
    },
    {
        "text": "2) we give this result of the subquery the alias \u201ccounts,\u201d by putting the alias after the parentheses."
    },
    {
        "text": "many sql varieties require that an alias be given whenever you have subqueries."
    },
    {
        "text": "3) we used the inner select clause to give the name num_ppl_in_city to the generated column, as described earlier."
    },
    {
        "text": "this made the overall query much more readable."
    },
    {
        "text": "14.1.4 running mysql and managing the db the previous section described the syntax of mysql queries."
    },
    {
        "text": "now let\u2019s go over the nuts and bolts of how to access the servers, create/destroy tables, and put data in them."
    },
    {
        "text": "there are many, many ways a mysql server can be accessed."
    },
    {
        "text": "the easiest though is by the command line."
    },
    {
        "text": "the following command will open up an inter- active session with a remote mysql server: mysql --host=10.0.0.4 \\ --user=myname \\ --password=mypass mydb in this case, the user and password are your login credentials, mydb is the database on the server you want to access (although you can switch to a differ- ent later with a use statement), and the host is the url of the server (since usually, the mysql server is a physically different computer compared to the one you\u2019re working on)."
    },
    {
        "text": "once you have opened up the mysql shell, the following commands will let you do what you need to do."
    },
    {
        "text": "their syntax should be straightforward."
    },
    {
        "text": "note that each command is terminated by a semicolon."
    },
    {
        "text": "it is fine to break a single com- mand out across several lines, so long as it ends with the semicolon."
    },
    {
        "text": "# first list tables in current db show tables; use my_other_database; # switch to my_other_database drop table table_to_drop; # delete this table create database my_new_database; # create a database"
    },
    {
        "text": "14 databases 210 the syntax for creating a table is somewhat more complicated."
    },
    {
        "text": "here is a simple example: create table myguests ( id int, firstname varchar(30), lastname varchar(30), arrival_date timestamp ); we say create table, the table\u2019s name, and then in parentheses we give the column names and their types."
    },
    {
        "text": "note in this example that varchar(30) means a string of at most 30 characters."
    },
    {
        "text": "this query will create an empty table, which we can then fill in a number of ways."
    },
    {
        "text": "if we only want to add a few entries, we can say insert into myguests (id, firstname, lastname, arrival_date) values (0, \u2019field\u2019, \u2019cady\u2019, \u20192013\u201008\u201013\u2019), (1, \u2019ryna\u2019, \u2019cady\u2019, \u20192013\u201008\u201013\u2019); more often, we will want to load data from a csv file or something of that nature, which can be done in the following way: load data infile '/tmp/test.txt' into table test fields delimited by ','; when you are getting data out of a database by use of a query, it is also com- mon to do it remotely from the command line."
    },
    {
        "text": "the following line of bash code will send a query to a mysql server (which is assumed to be local host in this case, that is, the same computer you\u2019re sending the command from) and then write the results to a local csv file: mysql --host localhost \\ -e 'select * from foo.myguests' \\ > foo.csv 14.2 \u00adkey-value stores the next big database paradigm to know is the key\u2013value store."
    },
    {
        "text": "this is concep- tually similar to a python dictionary on a massive scale, mapping keys (typi- cally strings) to arbitrary data objects."
    },
    {
        "text": "this is fantastic for storing unstructured"
    },
    {
        "text": "14.4 document stores 211 data such as web pages, and key\u2013value stores have spiked in popularity in recent years, playing a particularly large role in the big data movement."
    },
    {
        "text": "the downside though is that key\u2013value stores typically just give you access to the data; they don\u2019t have any of the optimized preprocessing that\u2019s available with an rdb."
    },
    {
        "text": "in many cases, a key\u2013value store is a more efficient way to store relational data in which most fields are blank."
    },
    {
        "text": "for example, let\u2019s say we have a database of people and know a few pieces of information about each one \u2013 many thou- sands of things that we could know about each person, but usually only a few of them are not null for any given person."
    },
    {
        "text": "in that case, it is more efficient to have the person/column tuple be a key and the entry be the value."
    },
    {
        "text": "if the data- base doesn\u2019t contain an entry for the person/column, you can assume that it\u2019s null."
    },
    {
        "text": "examples of key\u2013value stores include oracle nosql database, redis, and dbm."
    },
    {
        "text": "14.3 \u00adwide column stores a wide column store is a table such as a relational database, but it has a huge number of columns and is mostly sparse."
    },
    {
        "text": "unlike relational databases, new col- umns can be added or deleted at will, and a given column will often exist for only a few related rows in the table."
    },
    {
        "text": "in this sense, a wide column store is per- haps more similar to a key\u2013value store where the key contains two fields: a row id and a column name."
    },
    {
        "text": "familiar mysql operations, such as groupby and join, are often not supported."
    },
    {
        "text": "the original wide column store is often considered to be bigtable, an internal tool developed by google and outlined in their paper bigtable: a distributed storage system for structured data."
    },
    {
        "text": "bigtable also has time stamps attached to each cell in the table and time\u2010stamped records of all previous entries."
    },
    {
        "text": "the columns in a bigtable are not completely independent of each other; they are grouped into \u201ccolumn families\u201d that are frequently accessed together."
    },
    {
        "text": "open\u2010source column\u2010oriented databases include cassandra, hbase, and accumulo."
    },
    {
        "text": "14.4 \u00addocument stores document stores are similar to key\u2013value databases, but the values they store are specifically documents of some flexible format, such as xml or json."
    },
    {
        "text": "in addition to simply fetching the data, document stores provide some function- ality for searching through a database and pulling out documents (or parts of them) that match certain criteria or other processing."
    },
    {
        "text": "in this way, they can"
    },
    {
        "text": "14 databases 212 function as a happy medium between relational databases and key\u2013value stores; they are flexible enough to hold nontabular data, but structured enough that they can actually process that data rather than just reading/writing it."
    },
    {
        "text": "the most popular document store is mongodb."
    },
    {
        "text": "it is an open\u2010source, free piece of software produced by mongodb inc. to illustrate what document stores are like, i will give you a quick tutorial on mongodb."
    },
    {
        "text": "14.4.1 mongodb\u00ae similarly to mysql, data in mongo is divided into databases."
    },
    {
        "text": "a database con- tains \u201ccollections,\u201d the analogs of mysql tables."
    },
    {
        "text": "a collection consists of a bunch of documents, stored in a json\u2010like data format called bson."
    },
    {
        "text": "a docu- ment in mongodb is very flexible; they can mix and match fields, data types, layers of nesting, or whatever other variety you need to put in."
    },
    {
        "text": "the only thing every document must contain is a unique identifier field called \u201c_id,\u201d which functions as the primary key of the document."
    },
    {
        "text": "the user can specify the _id when a document is inserted; if there is another document with the same _id, you will get an error."
    },
    {
        "text": "documents written into mongodb without identifiers will be assigned them, with an object called an objectid."
    },
    {
        "text": "an auto- matically generated objectid will encode the time (in seconds) the objectid was created (typically, the time when its associated document was added to mongo), a code for the computer it was generated on (very useful in cluster computing), and the process id that generated the objectid."
    },
    {
        "text": "finally, each pro- cess that is generating objectids will keep track of how many it has created and encode the creation order in them, so that objectids created by the same process within one second of each other will still be distinct."
    },
    {
        "text": "the main part to accessing data in mongodb is the query."
    },
    {
        "text": "the following shell session will open a mongo database, write some data to it, and execute a simple query to find all matching documents: $ mongodb # command to open up a mongodb shell > // specify the database you want to use, like mysql > use some_db > show collections > // insert a new document into the collection \"posts\" > // if posts doesn't exist, it will be created > db.posts.insert({\"name\":\"bob\",\"age\":31}) writeresult({ \"ninserted\" : 1 }) > // prints out the collections in this db."
    },
    {
        "text": "> // posts already exists."
    },
    {
        "text": "> db.posts.find() // no query = show all documents { \"_id\" : objectid(\"55f8cc2f73b593e9ca69c126\"), \"name\" : \"field\" }"
    },
    {
        "text": "14.4 document stores 213 > // shows one document matching given query > db.posts.find({\"name\":\"bob\"}) the query here looks like a json object itself."
    },
    {
        "text": "in a way it is: it maps the name of fields to all requirements we are placing on that field."
    },
    {
        "text": "in the aforementioned example, we wrote \u201cname\u201d:\u201cbob\u201d, which means that \u201cbob\u201d must be the name field in all matching documents."
    },
    {
        "text": "more generally, the query could have included a number of different constraints on the field, expressed in their own json\u2010 like form."
    },
    {
        "text": "here are some additional examples: > db.foobar.insert({\"name\":\"field\",\"age\":31}) > // age greater than 10 > db.foobar.find({\"age\":{$gt:10}}) > // age between 10 and 20 > db.foobar.find({\"age\":{$gt:10, $lt:20}}) > // don\u2019t care what the age is, so long as it\u2019s there > db.foobar.find({\"age\":{$exists:true}}) the expressions starting with \u201c$\u201d are called \u201cquery operators,\u201d and they con- stitute the logic of mongodb queries."
    },
    {
        "text": "before you call me on it, the query {\u201cname\u201d:\u201cbob\u201d} is just syntactic sugar around {\u201cname\u201d:{$eq:\u201cbob\u201d}}."
    },
    {
        "text": "a given mongodb query will ultimately be a nested, json\u2010like object, where the low- est level of nesting maps field names to query operators."
    },
    {
        "text": "besides the query, the find function takes an optional second argument called the projection."
    },
    {
        "text": "this specifies which fields on a document are to be returned (after all, the entire document might be quite large) and, if a docu- ment contains long arrays, how many elements of the arrays should be selected."
    },
    {
        "text": "the following table shows some example projections: projection meaning { _id:0, name:1, age:1} return only the name and age fields for each document."
    },
    {
        "text": "note that we had to explicitly exclude _id; otherwise, it would be returned too { comments: { $slice: 5 } } return the first five elements of the \u201ccomments\u201d field, which is presumed to be an array { comments: { $elemmatch: { userid: 102 } } } return all elements in the comments array that have userid 102 if we want to update documents in a collection, we use the update() com- mand."
    },
    {
        "text": "it takes in two required arguments and a third optional one: \u25cf \u25cfa query statement indicating the documents to be modified."
    },
    {
        "text": "this uses the same query operators as the find() function."
    },
    {
        "text": "14 databases 214 \u25cf \u25cfan update statement, with update operators that indicate what operations should be done to what fields."
    },
    {
        "text": "\u25cf \u25cfoptionally, a set of additional options."
    },
    {
        "text": "the most important additional option is \u201cmulti,\u201d which indicates whether multiple documents can be modified."
    },
    {
        "text": "it defaults to false, which is perhaps counterintuitive."
    },
    {
        "text": "these commands give the basic idea: > // this command updates a single document whose \"item\" field is \"abc\"."
    },
    {
        "text": "> db.inventory.update( { item: \"abc\" }, { $set: { \"details.model\": \"14q2\" } }) > // this one will find all docs in the \"clothing\" category, rename their category to \"apparel\", > // and increment their \"age\" field by 5 > db.inventory.update( { category: \"clothing\" }, {$set: { category: \"apparel\" }, $inc: { age: 5 }}, { multi: true }) 14.5 \u00adfurther reading 1 redmond, e, wilson, j, seven database in seven weeks: a guide to modern databases and the nosql movement, 2012, pragmatic bookshelf, raleigh, nc."
    },
    {
        "text": "2 tahaghoghi, s, williams, h, learning mysql, 2006, o\u2019reilly media, newton, ma."
    },
    {
        "text": "14.6 \u00adglossary bson a json\u2010like data format used by mongodb."
    },
    {
        "text": "database a piece of software that stores data of a particular type in a format that supports low\u2010latency access."
    },
    {
        "text": "db common shorthand for database."
    },
    {
        "text": "document store a db that stores documents, usually in a markup language such as xml or json."
    },
    {
        "text": "key\u2013value store a db that stores data objects by key but doesn\u2019t usually have other querying functionality."
    },
    {
        "text": "mongodb a popular open\u2010source document store that runs on a cluster."
    },
    {
        "text": "mysql an extremely popular open\u2010source version of sql."
    },
    {
        "text": "14.6 glossary 215 query language a lightweight language for specifying database queries."
    },
    {
        "text": "most query languages are based on sql\u2019s syntax."
    },
    {
        "text": "relational algebra an idealized mathematical framework describing the operations of relational databases."
    },
    {
        "text": "real\u2010world rdbs typically include functionality that is not present in pure relational algebra."
    },
    {
        "text": "relational database a database that stores data in tables and supports operations such as selecting columns and joining records from several tables."
    },
    {
        "text": "sql the most popular relational database."
    },
    {
        "text": "wide column store conceptually similar to a relational database, but typically tables have many, many columns, and they do not support operations such as joins and grouping."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 217 15 in my experience, the single most important skill that is often lacking in data scientists is the ability to write decent code."
    },
    {
        "text": "i\u2019m not talking about writing highly optimized numerical routines, designing fancy libraries or anything like that: just keeping a few hundred lines of code clear and manageable for the course of a project is a learned skill."
    },
    {
        "text": "i\u2019ve seen many brilliant data scientists coming from areas such as physics and math, who lack this skill because they never had to write anything longer than a few dozen lines, or they never had to go back to code and update it."
    },
    {
        "text": "there\u2019s nothing worse than seeing a mathematical genius\u2019s data science project go up in flames because their 200\u2010line script was so illeg- ible they couldn\u2019t debug it."
    },
    {
        "text": "that\u2019s one reason this chapter exists: to pass on the message that everybody who writes code is responsible for making sure that their code is clear."
    },
    {
        "text": "the other reason for this chapter is that, in practice, data scientists are often called on to do far more than keep their code readable."
    },
    {
        "text": "some companies have their data scientists focused strictly on analytics work."
    },
    {
        "text": "in many cases though, it falls to data scientists to turn their one\u2010off scripts into reusable data analysis packages that take on a life of their own."
    },
    {
        "text": "other times, data scientists function as junior members of a software engineering team, writing large pieces of pro- duction code that implement their ideas in a real\u2010time product."
    },
    {
        "text": "this chapter will give you a condensed version of what goes into production\u2010level code and what life is like on a software engineering team."
    },
    {
        "text": "15.1 \u00adcoding style coding style is not about being able to write code quickly or even about making sure that your code is correct (although in the long run, it enables both of these)."
    },
    {
        "text": "instead, it is about making your code easy to read and understand."
    },
    {
        "text": "this makes it easier for other people (including, most importantly, your future self software engineering best practices"
    },
    {
        "text": "15 software engineering best practices 218 after you\u2019ve forgotten how your code works) to figure out how your code works, modify it as need be, and debug it."
    },
    {
        "text": "often, it takes a little longer to write your code well, but it is almost always worth the cost."
    },
    {
        "text": "let me give you a quick idea of the difference between good and bad code quality."
    },
    {
        "text": "here is a piece of example code from earlier in the book which, if i may say so myself, i did a pretty good job of writing: from htmlparser import htmlparser import urllib topic = \"dangiwa_umar\" url = \"https://en.wikipedia.org/wiki/%s\" % topic class linkcountingparser(htmlparser): in_paragraph = false link_count = 0 def handle_starttag(self, tag, attrs): if tag=='p': self.in_paragraph = true elif tag=='a' and self.in_paragraph: self.link_count += 1 def handle_endtag(self, tag): if tag=='p': self.in_paragraph = false html = urllib.urlopen(url).read() parser = linkcountingparser() parser.feed(html) print \"there were\", parser.link_count, \\ \"links in the article\" there are no comments or documentation in this code, but it\u2019s fairly clear from my variable names what i\u2019m trying to do, and the logical flow of the pro- gram shows how i\u2019m doing it."
    },
    {
        "text": "some of what i\u2019m doing is specific to the htmlparser library, but even if you aren\u2019t familiar with it, you can infer a lot about how it works from context."
    },
    {
        "text": "if you\u2019re familiar with html and the htmlparser library, the code should be crystal clear."
    },
    {
        "text": "in contrast, here is the same code, incorporating some of the no\u2010nos of coding style: import urllib url = \"https://en.wikipedia.org/wiki/dangiwa_umar\" cont = urllib.urlopen(url).read() from htmlparser import htmlparser class parser(htmlparser): def handle_starttag(self, x, y): if x=='p': self.inp = true if x=='a':"
    },
    {
        "text": "15.1 coding style 219 try: if self.inp: self.lc += 1 except: pass if x=='html': self.lc = 0 def handle_endtag(self, x): if x=='p': self.inp = false p = parser() p.feed(cont) print \"there were\", p.lc, \\ \"links in the article\" among the sins i have committed, here are the following: \u25cf \u25cfmeaningless variable names."
    },
    {
        "text": "\u25cf \u25cfhard\u2010coding the url, rather than having the topic of the wikipedia article called out as its own parameter."
    },
    {
        "text": "\u25cf \u25cf\u201cparser\u201d is a very unhelpful class name."
    },
    {
        "text": "\u25cf \u25cfthe parser has two internal variables that it maintains: inp and lc (in_para- graph and link_count in the good code)."
    },
    {
        "text": "these variables should have been declared upfront with default values, but instead i only create them on the fly as soon as i need them."
    },
    {
        "text": "\u25cf \u25cfthe try\u2013except loop is being used to check whether self.inp has been defined yet."
    },
    {
        "text": "that\u2019s a perverse use of try\u2013except that will confuse people."
    },
    {
        "text": "\u25cf \u25cfin handle_starttag, there are three if-statements."
    },
    {
        "text": "but they are mutually exclusive, so it would have been clearer to use elif."
    },
    {
        "text": "\u25cf \u25cfthe try\u2013except loop is indented an irregular amount."
    },
    {
        "text": "good coding style isn\u2019t a set of rules that i can explain to you."
    },
    {
        "text": "it\u2019s really a mindset that you develop, an impatience with code being hard to read."
    },
    {
        "text": "the only way i know to learn it is by reading and writing a lot of code, so i won\u2019t try to teach it to you in this chapter."
    },
    {
        "text": "however, a few basic principles are as follows: \u25cf \u25cfalways use descriptive names for variables and functions."
    },
    {
        "text": "\u25cf \u25cfmodularity."
    },
    {
        "text": "break the program down into self\u2010contained parts that have clear functions."
    },
    {
        "text": "\u25cf \u25cfavoid having excessive indentation, such as loops within loops within loops."
    },
    {
        "text": "if you\u2019re running into this, try to break the inner parts out into a separate function or subroutine."
    },
    {
        "text": "\u25cf \u25cfuse comments when appropriate."
    },
    {
        "text": "if you do something bizarre in your code, explain why in a comment."
    },
    {
        "text": "also, every file should ideally start with a com- ment that explains what it does."
    },
    {
        "text": "\u25cf \u25cfdon\u2019t overuse comments."
    },
    {
        "text": "they distract from the code itself, which should be mostly clear enough that you can read it directly."
    },
    {
        "text": "comments can also be wrong or out of date, so leave them out if the code speaks for itself."
    },
    {
        "text": "15 software engineering best practices 220 \u25cf \u25cfif you have several blocks of code that do very similar things, it often pays to refactor them into a single routine."
    },
    {
        "text": "\u25cf \u25cftry to separate boilerplate information (such as the way data is formatted) from the core processing logic of the program."
    },
    {
        "text": "for example, the order of columns in a table should usually be specified separately from the code, which processes the table."
    },
    {
        "text": "some people might protest: if data scientists have to do all of this code quality stuff, then what\u2019s the difference between data science code and production code?"
    },
    {
        "text": "my take is this: the goal of general code quality is to make it easy for somebody to read and understand your code."
    },
    {
        "text": "in contrast, the aim of production code is to make this understanding unnecessary."
    },
    {
        "text": "production code is largely about creating apis that are intuitive to use, well documented, and flexible enough to support a variety of use cases."
    },
    {
        "text": "people who use production code should be able to call it as a library without worrying about how it works."
    },
    {
        "text": "with data science programming, on the other hand, if somebody wants to modify your script, then it is fair to expect them to roll up their sleeves and dive into the code itself."
    },
    {
        "text": "15.2 \u00adversion control and git for data scientists a central feature of writing production software is a good version control sys- tem."
    },
    {
        "text": "this is a software framework that tracks changes that you make to a code- base, syncing them with a master copy stored on a server somewhere."
    },
    {
        "text": "it gives you the following massive advantages: \u25cf \u25cfif your computer gets destroyed, all of the code is backed up."
    },
    {
        "text": "\u25cf \u25cfif you\u2019re working on a team, everybody can keep their changes synced up by periodically reading down changes from the server."
    },
    {
        "text": "\u25cf \u25cfif things break, you can go back to a previous version of the codebase that was known to work."
    },
    {
        "text": "the larger and more complex a codebase becomes, the more these become absolutely indispensible."
    },
    {
        "text": "typically, version control works by downloading the codebase as a directory on your computer."
    },
    {
        "text": "editing the codebase is as simple as making changes to the contents of that directory (changing files, adding new files or subdirectories, etc.)"
    },
    {
        "text": "and then telling the version control system to sync those changes."
    },
    {
        "text": "a ver- sion control system will afford at least the following functions: \u25cf \u25cfyou can download the master copy of the codebase."
    },
    {
        "text": "this is often called a \u201ccheckout\u201d or a \u201cclone.\u201d \u25cf \u25cfyou can refresh the code on your computer, incorporating any changes that have been made to the master copy since you originally check out the code."
    },
    {
        "text": "this is often called a \u201cpull.\u201d"
    },
    {
        "text": "15.2 version control and git for data scientists 221 \u25cf \u25cfafter editing the code (changing it, adding new files, etc."
    },
    {
        "text": "), you can write your changes back to the master copy on the server."
    },
    {
        "text": "this is sometimes called a \u201ccommit,\u201d and sometimes a \u201cpush.\u201d \u25cf \u25cfif your changes conflict with changes that somebody else made, there is some way to \u201cmerge\u201d the changes."
    },
    {
        "text": "\u25cf \u25cfyou can include comments associated with your commits that describe what you did."
    },
    {
        "text": "\u25cf \u25cfmultiple users can check out the code and make changes, from multiple computers."
    },
    {
        "text": "\u25cf \u25cfit keeps track of the history of all changes."
    },
    {
        "text": "\u25cf \u25cfin case something gets broken, you can revert specific sets of changes."
    },
    {
        "text": "most modern systems also include the ability to create branches of the entire codebase."
    },
    {
        "text": "if you have a large amount of work that needs to be done, it is often useful to fork off a new branch of the entire codebase."
    },
    {
        "text": "this lets you do what- ever you need to do without worrying about polluting the master branch."
    },
    {
        "text": "later, when all changes have been made, you can merge your changes back into the master branch and deal with any possible conflicts."
    },
    {
        "text": "branches are also useful if you need to make one\u2010off versions of the codebase for some special purpose."
    },
    {
        "text": "for example, maybe there is a customer who wants a customized version of your product."
    },
    {
        "text": "this may involve small changes to many different parts of your codebase and require its own version of the branch."
    },
    {
        "text": "the most popular version control system these days is git, although there are others."
    },
    {
        "text": "the following table is a cheat sheet of the most important git com- mands, if you use git form in the command line."
    },
    {
        "text": "all of them except the first are expected to be executed when you are inside the checked\u2010out directories."
    },
    {
        "text": "sample command what it does git clone https://github."
    },
    {
        "text": "com/someproject.git clones the master branch of the repo git status says what files you have modified or added git add myfile.py makes a change by adding a new file git rm myfile.py makes a change by deleting a file git mv dir1/file.py dir2/file."
    },
    {
        "text": "py makes a change by moving a file git commit myfile.py \u2013m \u201cmy commit message\u201d any changes you have made to the file (including adding it or deleting it) are staged for pushing git push pushes all committed changes to the master copy of the branch you are on git diff myfile.txt displays all changes that i\u2019ve made to the file that have not been committed yet"
    },
    {
        "text": "15 software engineering best practices 222 sample command what it does git branch lists all branches of the repo that your local git knows about and can switch to git checkout my_branch switches to the branch called my_branch git branch my_new_branch creates a new branch called my_new_branch and check it out git merge other_branch merges the changes made in other_branch into the branch you\u2019re currently on the other notable thing about git is the website www.github.com."
    },
    {
        "text": "github allows anybody to create a git repo where the master copy is stored on github\u2019s internal servers."
    },
    {
        "text": "github is free to use provided that you make the codebase open to the public, so there are many fascinating projects hosted on it."
    },
    {
        "text": "many people (yours truly included) will also use github repositories as a place to store some of their personal code snippets."
    },
    {
        "text": "for a fee, github will keep a repository private, so it\u2019s a great way for small companies to get their hands on world\u2010class version control."
    },
    {
        "text": "15.3 \u00adtesting code there\u2019s a spectrum of how rigorously code can be tested."
    },
    {
        "text": "scientists and math- ematicians are typically used to \u201ctesting\u201d code in a pretty casual way \u2013 making sure that it seems to give the correct output, as indicated by a few sanity checks."
    },
    {
        "text": "on the other end of the spectrum, large software projects depend on compli- cated testing frameworks that can sometimes be as complicated as the source code itself."
    },
    {
        "text": "data science tends toward the former, but in practice, it can run the gamut."
    },
    {
        "text": "this section will review some of the standard testing concepts that you would see in a hardcore software environment."
    },
    {
        "text": "at first glance, writing and maintaining testing code might appear to be an irritating burden when you have source code to write."
    },
    {
        "text": "it\u2019s a pain to go back and revise all the tests to reflect the changes you have made, rather than going on to the other changes you have to make."
    },
    {
        "text": "it\u2019s even possible that all of your main code works perfectly, but the tests fail because there was a bug in your testing code."
    },
    {
        "text": "who wants to deal with that?"
    },
    {
        "text": "these are understandable concerns, and they are sometimes valid for small code bases."
    },
    {
        "text": "however, as the number of lines increases, and especially as the software is maintained for a long term rather than being one\u2010off scripts, a robust testing framework becomes absolutely vital."
    },
    {
        "text": "it took me a long time to appreciate its value, but believe me; it\u2019s there."
    },
    {
        "text": "the most obvious advantage of testing code is, of course, that it checks whether your code is right."
    },
    {
        "text": "there is another, less obvious bonus too."
    },
    {
        "text": "oftentimes, the test code is the best documentation of your source code."
    },
    {
        "text": "rather than describing your code\u2019s functionality in nebulous english, it shows the brass"
    },
    {
        "text": "15.3 testing code 223 tacks of how to call your code, which inputs go in, and which outputs are gen- erated."
    },
    {
        "text": "best of all, by simply running all the tests, you can make sure that this \u201cdocumentation\u201d is up to date."
    },
    {
        "text": "15.3.1 unit tests unit tests cover small, self\u2010contained pieces of code logic."
    },
    {
        "text": "for a given piece of code, its unit tests should cover every known edge case of the code, as well as several of the more general cases."
    },
    {
        "text": "many programming languages have libraries that are specifically designed to support unit testing."
    },
    {
        "text": "let\u2019s see how this looks with the python unit testing library."
    },
    {
        "text": "the following code assumes that you\u2019ve written a library called \u201cmymath\u201d containing a func- tion \u201cfib\u201d, which calculates the nth fibonacci number."
    },
    {
        "text": "this code assumes that the zeroth fibonacci number is 0, the first is 1, and every subsequent number is the sum of the previous two: import unittest from mymath import fib class testfibonacci(unittest.testcase): def test0(self): self.assertequal(0, fib(0)) def test1(self): self.assertequal(1, fib(1)) def test2(self): self.assertequal(fib(0)+fib(1), fib(2)) def test10(self): self.assertequal(fib(8)+fib(9), fib(10)) unittest.main() the testcase class is where all of the magic happens."
    },
    {
        "text": "for the given piece of code you want to test, you create a class that inherits from it, in this case, testfibonacci."
    },
    {
        "text": "for every test you want to run on the code, you create a new method, whose name must start with \u201ctest.\u201d these tests use the assertequal function, which is a member function of test case and keeps track of any fail- ures."
    },
    {
        "text": "the main() function looks for all test cases defined in the namespace, runs them all, and reports the results to the user."
    },
    {
        "text": "testcase has a number of other methods, some of which are listed in this table: assertnotequal make sure that two things are different asserttrue make sure that a variable is true assertfalse make sure that a variable is false assertraises make sure that calling some function raises an error"
    },
    {
        "text": "15 software engineering best practices 224 the niftiest application of unit tests is in long\u2010term code maintenance."
    },
    {
        "text": "say that you wrote a code module a while ago, and a small glitch gets discovered."
    },
    {
        "text": "fixing the glitch might involve substantial changes to your source code, and you run the risk of breaking the functionality that was already there."
    },
    {
        "text": "so, you keep all your old unit tests in place and add one that tests the new edge case."
    },
    {
        "text": "as soon as the new test passes, and all of the previous tests still pass, you can be confident that you\u2019ve fixed the problem without breaking anything."
    },
    {
        "text": "another great use for unit tests comes up if you are using git and working with other people."
    },
    {
        "text": "say you\u2019ve made some changes to the codebase and want to make sure that everything still works when combined with changes other peo- ple have made."
    },
    {
        "text": "you can use the \u201cgit pull\u201d command to pull down all their changes, recompile the code if need be, and rerun the unit tests."
    },
    {
        "text": "if they all pass, you can push your changes with confidence."
    },
    {
        "text": "personally, i like to look at source code and test code as two symbiotic halves."
    },
    {
        "text": "they are \u201cin sync\u201d if all of the unit tests pass."
    },
    {
        "text": "the source code is dangerously likely to have a bug, and the test code is dangerously likely to have a bug."
    },
    {
        "text": "but a bug in either will break the symbiosis."
    },
    {
        "text": "so, if the source code and test code are in sync, your code almost certainly does what it\u2019s supposed to do."
    },
    {
        "text": "the proba- bility that both halves have a bug, and those bugs cancel out so that the tests pass, is miniscule."
    },
    {
        "text": "all you have to do is make sure that all of the edge cases in the source code are properly tested."
    },
    {
        "text": "the key limitation of unit tests is that each code module is tested in isolation."
    },
    {
        "text": "unit testing isn\u2019t really intended for code that accesses external resources, such as the internet, a remote mysql server, or other processes running on your computer."
    },
    {
        "text": "if you must unit test that kind of software, the standard way to do it is with something called a \u201cmock.\u201d if your code is designed to access some external api, a mock is an object that mimics that api in a predefined way."
    },
    {
        "text": "for example, a mock of a mysql server might take in queries but always returns a predefined result, without ever trying to access a remote server."
    },
    {
        "text": "now of course, you can write a python script that uses the unittest library but also accesses external resources."
    },
    {
        "text": "this is bad practice: the tests might fail because the external resource is having trouble, or it isn\u2019t available from the computer that is running the tests."
    },
    {
        "text": "but if you want to run unit tests manually as a way to test your own code, calling the external resources explicitly is often easier than creating mocks."
    },
    {
        "text": "15.3.2 integration tests in integration tests, the different code modules are linked up to each other, external resources are put in place, and the system is run in a realistic way."
    },
    {
        "text": "this is where you run into network timeouts, permissions glitches, memory over- flows, and other errors that can only occur at scale."
    },
    {
        "text": "it\u2019s also the trial\u2010by\u2010fire of whether every module correctly understands every other module\u2019s api."
    },
    {
        "text": "15.5 agile methodology 225 there is usually not a standard library for integration testing, because it is so specific to each individual project."
    },
    {
        "text": "it\u2019s more of a stage in the development of a serious piece of software."
    },
    {
        "text": "15.4 \u00adtest-driven development previously, i explained how you can use unit tests to make sure that you\u2019ve fixed a glitch in your code without breaking something that was already work- ing."
    },
    {
        "text": "\u201ctest\u2010driven development\u201d (tdd) is an approach to software engineering that takes this idea to the extreme."
    },
    {
        "text": "you write up the unit tests for your module before you even start on the source code."
    },
    {
        "text": "then, your goal in writing the mod- ule is just to make the unit tests all pass."
    },
    {
        "text": "often you address each test in turn, making sure that it passes while not breaking the others, but ignoring any test that you haven\u2019t come to yet."
    },
    {
        "text": "there are two big advantages to tdd."
    },
    {
        "text": "the first is that it makes you think through your module\u2019s desired functionality right upfront."
    },
    {
        "text": "this forces you to decide on a preliminary api and makes you write code that calls the api so that you can see if it\u2019s painfully clunky."
    },
    {
        "text": "the second advantage of tdd is that, when it comes to writing the source code itself, you are able to laser\u2010focus on a single test rather than trying to hold the entire system in your head at once."
    },
    {
        "text": "it can be very calming, almost meditative."
    },
    {
        "text": "tdd is also only appropriate if you know more or less what your software should ultimately do."
    },
    {
        "text": "this is often not the case in data science, because you don\u2019t know what feature extractions and preprocessing will work until you start playing with the data."
    },
    {
        "text": "in my own experience, data science is often used to fig- ure out exactly which analyses should be performed in what way, and then tdd is used to implement a production version."
    },
    {
        "text": "the other issue with tdd is that sometimes its myopia is not practical."
    },
    {
        "text": "the more different modules interact with each other, the more important it becomes to plan out your software architecture ahead of time."
    },
    {
        "text": "otherwise, each additional unit test may require rejiggering pretty much your entire code base, and you could run into showstopper issues when you think you\u2019re most of the way through the project."
    },
    {
        "text": "15.5 \u00adagile methodology test\u2010driven development is a way for individual programmers to go about get- ting their work done."
    },
    {
        "text": "agile, on the other hand, is a way to organize teams of developers."
    },
    {
        "text": "the term was coined in 2001 in \u201cthe manifesto for agile software development.\u201d the book was written by a group of programmers who were sick of top\u2010down, long\u2010term plans resulting in projects that ultimately fail."
    },
    {
        "text": "the"
    },
    {
        "text": "15 software engineering best practices 226 concept caught on like wildfire, and some variant of agile is ubiquitous in data science and software."
    },
    {
        "text": "the key idea of agile is to make projects more flexible by shortening the development cycle and tightening feedback loops."
    },
    {
        "text": "among the key principles of agile are the following: \u25cf \u25cffrequent collaboration and decision\u2010making by the individual team mem- bers, as opposed to mandates from above \u25cf \u25cfsimilarly frequent communication with clients and stakeholders \u25cf \u25cfmaking sure that there is always a working end\u2010to\u2010end product, even if it doesn\u2019t include all of the features that you will ultimately want."
    },
    {
        "text": "a typical feature of agile teams is a daily morning meeting, often called \u201cstand\u2010up\u201d or \u201cscrum.\u201d typically, this will consist of the team going around in a circle, with every member saying (1) what they accomplished the previous day, (2) what they are planning to do today, and (3) any roadblocks they are running into."
    },
    {
        "text": "agile is often a fantastic way to approach software, but it is not without its own drawbacks."
    },
    {
        "text": "the biggest is that it sometimes comes at the expense of long\u2010 term planning and clear direction."
    },
    {
        "text": "the second issue with agile development is that the focus on rapid feature iteration often leads to an accumulation of \u201ctechnical debt\u201d \u2013 disorganization and instabilities in the codebase that come back to bite you later."
    },
    {
        "text": "15.6 \u00adfurther reading 1 rubin, k, essential scrum: a practical guide to the most popular agile process, 2012, addison\u2010wesley, boston, ma."
    },
    {
        "text": "2 martin, r, clean code: a handbook of agile software craftsmanship, prentice hall, upper saddle river, nj."
    },
    {
        "text": "15.7 \u00adglossary agile development an approach to software development that focuses on getting a minimal product working quickly and then having short \u201csprints\u201d with clear goals that are incremental improvements."
    },
    {
        "text": "git the most popular version control system today."
    },
    {
        "text": "integration test a test that makes sure that several pieces of software work correctly together."
    },
    {
        "text": "perl golf a pejorative term for cramming a lot of functionality into a few lines of code, making it hard to understand."
    },
    {
        "text": "scrum a short, daily team meeting in agile development."
    },
    {
        "text": "15.7 glossary 227 sprint a short period of time (usually 2 weeks) over which concrete goals are set in agile development."
    },
    {
        "text": "technical debt work that will need to be done in the future because of shortsighted and/or expedient programming choices made in the past."
    },
    {
        "text": "test\u2010driven development a system of writing code where you start by writing the unit tests for whatever changes you intend to make and then changing the code until the new unit tests pass and all the old ones still pass."
    },
    {
        "text": "unit test a test that runs a small piece of code in isolation and makes sure that it works correctly."
    },
    {
        "text": "they are particularly useful for test\u2010driven development and for testing edge cases in the code\u2019s logic."
    },
    {
        "text": "version control a piece of software that tracks changes made by multiple users to a repository of code."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 229 16 natural language processing (nlp) is a collection of techniques for working with human language."
    },
    {
        "text": "examples would include flagging e-mails as spam, using twitter to assess public sentiment, and finding which text documents are about similar topics."
    },
    {
        "text": "nlp is an area that many data scientists never actually need to touch."
    },
    {
        "text": "but enough of them end up needing it, and it is sufficiently different from other subjects that it deserves a chapter in this book."
    },
    {
        "text": "this chapter will start with several generic sections about nlp datasets and big-picture concepts."
    },
    {
        "text": "then i will switch gears to the core nlp concepts, moving from the simple, quick-and-dirty techniques to more complicated ones."
    },
    {
        "text": "i also want to emphasize that nlp techniques are not strictly limited to lan- guage."
    },
    {
        "text": "i\u2019ve also seen them used to parse computer log files, figuring out what \u201csentences\u201d the computer generates."
    },
    {
        "text": "personally, i first learned many of the statistical techniques while working with bioinformatics."
    },
    {
        "text": "16.1 \u00addo i even need nlp?"
    },
    {
        "text": "the first question to ask when using nlp is whether you even need it."
    },
    {
        "text": "there is often pressure from customers and bosses to solve problems using nlp, because it is seen as some kind of magical silver bullet."
    },
    {
        "text": "but in my experience, nlp is hard to implement, and it is prone to bizarre errors that are obviously wrong when a human looks at them."
    },
    {
        "text": "i\u2019ve seen people bang their heads against a problem using nlp techniques, only to eventually give up and try solving the problem with regular expres- sions."
    },
    {
        "text": "then lo and behold, the regular expressions work better than the nlp ever did."
    },
    {
        "text": "here are a few thoughts to keep in mind when deciding whether to try nlp: \u25cf \u25cfif your data has a regular structure to it, then you can probably extract what you need without nlp."
    },
    {
        "text": "natural language processing"
    },
    {
        "text": "16 natural language processing 230 \u25cf \u25cfnlp tends to be very effective at tasks such as determining whether two documents have similar content, because simple things such as word fre- quency are very informative."
    },
    {
        "text": "\u25cf \u25cfif you are trying to extract facts from documents, it is almost impossible unless you have standardized language, such as wikipedia or legal contracts."
    },
    {
        "text": "you probably can\u2019t do it with something such as twitter."
    },
    {
        "text": "\u25cf \u25cfit is often hard to make sense of why an nlp algorithm performed in the way it did."
    },
    {
        "text": "\u25cf \u25cfnlp typically requires a lot of training data."
    },
    {
        "text": "16.2 \u00adthe great divide: language versus statistics there are two very different schools of thought in nlp, which use very differ- ent techniques and sometimes are even at odds with one another."
    },
    {
        "text": "i\u2019ll call them \u201cstatistical nlp\u201d and \u201clinguistic nlp.\u201d the linguistic school focuses on under- standing language as language, with techniques such as identifying which words are verbs or parsing the structure of a sentence."
    },
    {
        "text": "this sounds great in theory, but it is often staggeringly difficult in practice because of the myriad ways that humans abuse their languages and break the rules."
    },
    {
        "text": "the statistical school of nlp solves this problem by using massive corpuses of training data to find statistical patterns in language."
    },
    {
        "text": "they might notice that \u201cdog\u201d and \u201cbark\u201d tend to occur frequently together or that the phrase \u201cnigerian prince\u201d is more common in a corpus of e-mails than chance would dictate."
    },
    {
        "text": "personally, i see statistical nlp mostly as a blunt-force workaround for the fact that linguistic nlp is so extraordinarily difficult."
    },
    {
        "text": "in the modern era of massive datasets (such as the web), this divide has become more pronounced, and statistical nlp tends to have the advantage."
    },
    {
        "text": "the best machine translation engines, such as the ones google might use to automatically translate a website, are primarily statistical."
    },
    {
        "text": "they are built by training on thousands of examples of human-done translation, such as news- paper articles published in multiple languages or books that were translated."
    },
    {
        "text": "some linguists protest that this is dodging the scientific problem of figuring out how the human brain really processes language."
    },
    {
        "text": "of course it is, but the bottom line is that the results are generally better."
    },
    {
        "text": "on the other hand, training something like that requires a training corpus and specially crafted machine learning algorithm that is generally beyond the reach of all but the most sophis- ticated nlp users."
    },
    {
        "text": "16.3 \u00adexample: sentiment analysis on stock market articles the following script shows off a tiny taste of what is possible with python\u2019s most popular nlp library: nltk (\u201cnatural language toolkit\u201d)."
    },
    {
        "text": "you set the ticker"
    },
    {
        "text": "16.3 example: sentiment analysis on stock market articles 231 symbol of a stock near the top of the script."
    },
    {
        "text": "it then parses a bunch of recent articles about the stock, gauges them as positive or negative, and prints how many fell into each category."
    },
    {
        "text": "import re import urllib import nltk from nltk.corpus import wordnet as wn from nltk.corpus import stopwords from nltk.stem.wordnet import wordnetlemmatizer ticker = 'csco' url_template = \"https://feeds.finance.yahoo.com/\" + \\ \"rss/2.0/headline?s=%s&region=us&lang=en-us\" def get_article_urls(ticker): # return list of urls for articles about a stock link_pattern = re.compile(r\"<link>[^<]*</link>\") xml_url = url_template % ticker xml_data = urllib.urlopen(xml_url).read() link_hits = re.findall(link_pattern, xml_data) return [h[6:-7] for h in link_hits] def get_article_content(url): # input: url for a news article # output: approx."
    },
    {
        "text": "content of the article # downloads html for an article and then # pulls data from paragraphs in the html paragraph_re = re.compile(r\"<p>."
    },
    {
        "text": "*</p>\") tag_re = re.compile(r\"<[^>]*>\") raw_html = urllib.urlopen(url).read() paragraphs = re.findall(paragraph_re, raw_html) all_text = \" \".join(paragraphs) content = re.sub(tag_re, \"\", all_text) return content def text_to_bag(txt): # input: bunch of text # output: bag-of-words of lemmas # also removes stop words lemmatizer = wordnetlemmatizer() txt_as_ascii = txt.decode( 'ascii', 'ignore').lower() tokens = nltk.tokenize.word_tokenize(txt_as_ascii) words = [t for t in tokens if t.isalpha()] lemmas = [lemmatizer.lemmatize(w) for w in words] stop = set(stopwords.words('english'))"
    },
    {
        "text": "16 natural language processing 232 nostops = [l for l in lemmas if l not in stop] return nltk.freqdist(nostops) def count_good_bad(bag): # input: bag-of-words of lemmas # output: number of words that are good, bad good_synsets = set(wn.synsets('good') + \\ wn.synsets('up')) bad_synsets = set(wn.synsets('bad') + \\ wn.synsets('down')) n_good, n_bad = 0, 0 for lemma, ct in bag.items(): ss = wn.synsets(lemma) if good_synsets.intersection(ss): n_good += 1 if bad_synsets.intersection(ss): n_bad += 1 return n_good, n_bad urls = get_article_urls(ticker) contents = [get_article_content(u) for u in urls] bags = [text_to_bag(txt) for txt in contents] counts = [count_good_bad(txt) for txt in bags] n_good_articles = len([_ for g, b in counts if g > b]) n_bad_articles = len([_ for g, b in counts if g < b]) print \"there are %i good articles and %i bad ones\" % (n_good_articles, n_bad_articles) 16.4 \u00adsoftware and datasets nlp processing is generally very computationally inefficient."
    },
    {
        "text": "even something as simple as determining whether a word is a noun requires consulting a lookup table containing a language\u2019s entire lexicon."
    },
    {
        "text": "more complex tasks such as parsing the meaning of a sentence require figuring out a sentence\u2019s structure, which becomes exponentially more difficult if there are ambiguities in the sentence (which there usually are)."
    },
    {
        "text": "this is all ignoring things such as typos, slang, and breaking grammatical rules."
    },
    {
        "text": "you can partly work around this by training stupider models on very large datasets, but this will just balloon your data size problems."
    },
    {
        "text": "there are a number of standardized linguistic datasets available in the public domain."
    },
    {
        "text": "depending on the dataset, they catalog everything from the defini- tions of words, to which words are synonyms of each other, to grammatical rules."
    },
    {
        "text": "most nlp libraries for any programming language will leverage at least one of these datasets."
    },
    {
        "text": "one lexical database that deserves special mention is wordnet."
    },
    {
        "text": "wordnet covers the english language, and its central concept is the \u201csynset.\u201d a synset is"
    },
    {
        "text": "16.6 central concept: bag-of-words 233 a collection of words with roughly equivalent meanings."
    },
    {
        "text": "casting every word to its associated synset is a great way to compare whether, for example, two sen- tences are discussing the same material using different terms."
    },
    {
        "text": "more impor- tantly, an ambiguous word such as \u201crun,\u201d which has many different possible meanings, is a member of many different synsets; using the correct synset for it is a way to eliminate ambiguity in the sentence."
    },
    {
        "text": "personally, i think of synsets as the words of a separate language, one in which there is no ambiguity and no extraneous synonyms."
    },
    {
        "text": "16.5 \u00adtokenization the first part of any nlp process is simply breaking a piece of text into its constituent parts (usually words)."
    },
    {
        "text": "this process is called \u201ctokenization,\u201d and it is complicated by issues such as punctuation markers, contractions, and a host of other things."
    },
    {
        "text": "is \u201cthat\u2019s\u201d one word, or is it two?"
    },
    {
        "text": "if two, should the second word be \u201cis\u201d or \u201c\u2019s.\u201d the process can become even more complicated if your tokens are sentences rather than words."
    },
    {
        "text": "16.6 \u00adcentral concept: bag-of-words probably, the most basic concept in nlp (aside from some very high-level applications of it) is that of a \u201cbag-of-words,\u201d also called a frequency distribu- tion."
    },
    {
        "text": "it\u2019s a way to turn a piece of free text (a tweet, a word document, or what- ever else) into a numerical vector that you can plug into a machine learning algorithm."
    },
    {
        "text": "the idea is quite simple \u2013 there is a dimension in the vector for every word in the language, and a document\u2019s score in the nth dimension is the number of times the nth word occurs in the document."
    },
    {
        "text": "the piece of text then becomes a vector in a very high-dimensional space."
    },
    {
        "text": "most of this chapter will be about extensions of the bag-of-words model."
    },
    {
        "text": "i will briefly discuss some more advanced topics, but the (perhaps surprising) reality is that data scientists rarely do anything that can\u2019t fit into the bag-of- words paradigm."
    },
    {
        "text": "when you go beyond bag-of-words, nlp quickly becomes a staggeringly complicated task that is usually best left to specialists."
    },
    {
        "text": "my first-ever exposure to nlp was as an intern at google, where they explained to me that this was how part of the search algorithm worked."
    },
    {
        "text": "you condense every website into a bag-of-words and normalize all the vectors."
    },
    {
        "text": "then when a search query comes in, you turn it into a normalized vector too, and then take its dot product with all of the web vectors."
    },
    {
        "text": "this is called the \u201ccosine similarity,\u201d because the dot product of two normalized vectors is just the cosine of the angle between them."
    },
    {
        "text": "the web pages that had high cosine"
    },
    {
        "text": "16 natural language processing 234 similarity were those whose content mostly resembled the query, that is, they were the best search result candidates."
    },
    {
        "text": "the majority of this chapter will be about extensions and refinements of the basic idea of bag-of-words \u2013 we will discuss some of the more intricate (and error-prone) sentence parsing toward the end."
    },
    {
        "text": "right off the cuff, we might want to consider the following extensions to the word vector: \u25cf \u25cfthere\u2019s a staggering number of words in english and an infinite number of potential strings that could appear in text."
    },
    {
        "text": "we need some way to cap them off."
    },
    {
        "text": "\u25cf \u25cfsome words are much more informative than others \u2013 we want to weight them by importance."
    },
    {
        "text": "\u25cf \u25cfsome words don\u2019t usually matter at all."
    },
    {
        "text": "things such as \u201ci\u201d and \u201cis\u201d are often called \u201cstop words,\u201d and we may want to just throw them out at the beginning."
    },
    {
        "text": "\u25cf \u25cfthe same word can come in many forms."
    },
    {
        "text": "we may want to turn every word into a standardized version of itself, so that \u201cran,\u201d \u201cruns,\u201d and \u201crunning\u201d all become the same thing."
    },
    {
        "text": "this is called \u201clemmatization.\u201d \u25cf \u25cfsometimes, several words have the same or similar meanings."
    },
    {
        "text": "in this case, we don\u2019t want a vector of words so much as a vector of meanings."
    },
    {
        "text": "a \u201csynset\u201d is a group of words that are synonyms of each other, so we can use synsets rather than just words."
    },
    {
        "text": "\u25cf \u25cfsometimes, we care more about phrases than individual words."
    },
    {
        "text": "a set of n words in order is called an \u201cn-gram,\u201d and we can use n-grams in place of words."
    },
    {
        "text": "nlp is a deep, highly specialized area."
    },
    {
        "text": "if you want to work on a cutting-edge nlp project that tries to develop real understanding of human language, the knowledge you will need goes well outside the scope of this chapter."
    },
    {
        "text": "however, simple nlp is a standard tool in the data science toolkit, and unless you end up specializing in nlp, this chapter should give you what you need."
    },
    {
        "text": "when it comes to running code that uses bag-of-words, there is an important thing to note."
    },
    {
        "text": "mathematically, you can think of word vectors as normal vectors: an ordered list of numbers, with different indices corresponding to different words in the language."
    },
    {
        "text": "the word \u201cemerald\u201d might correspond to index 25, the word \u201cintegrity\u201d to index 1047, and so on."
    },
    {
        "text": "but generally, these vectors will be stored as a map (in the aforementioned python code, it is a freqdist object) from word names to the numbers associated with those words."
    },
    {
        "text": "there is often no need to actually specify which words correspond to which vector indices, and doing so would add a human-indecipherable layer in your data processing, which is usually a bad idea."
    },
    {
        "text": "in fact, for many applications, it is not even neces- sary to explicitly enumerate the set of all words being captured."
    },
    {
        "text": "this is not just about human readability: the vectors being stored are often quite sparse, so it is more computationally efficient to only store the nonzero entries."
    },
    {
        "text": "you might wonder why we would bother to think of a map from strings to floats as a mathematical vector."
    },
    {
        "text": "if so, fair question."
    },
    {
        "text": "the reason is that vector operations, such as dot products, end up playing a central role in nlp."
    },
    {
        "text": "nlp"
    },
    {
        "text": "16.8 n-grams 235 pipelines will often include stages where they convert from map representa- tions of word vectors to more conventional sparse vectors and matrices, for more complicated linear algebra operations such as matrix decompositions."
    },
    {
        "text": "16.7 \u00adword weighting: tf-idf the first correction to bag-of-words is the idea that some words are more important than others."
    },
    {
        "text": "in some cases, we know a priori which words to pay attention to, but more often we are faced with a corpus of documents and have to determine from it which words are worth counting in the word vector and what weight we should assign to them."
    },
    {
        "text": "the most common way to do this is \u201cterm frequency\u2013inverse document frequency\u201d (tf-idf)."
    },
    {
        "text": "the intuition behind tf-idf is that rarer words are more important."
    },
    {
        "text": "you calculate the word vector for a particular document by counting up all of its words\u2019 frequencies, as usual."
    },
    {
        "text": "but then you divide each count by the frequency of that word in the training corpus."
    },
    {
        "text": "this will dampen down the scores for common words, but balloon them for rare words that hap- pen to occur (comparatively) frequently in this document."
    },
    {
        "text": "generally in a tf-idf, you will only look at words that have some minimal fre- quency in the training corpus."
    },
    {
        "text": "if a word never occurs in the training corpus but shows up in a new document, then surely it shouldn\u2019t have infinite importance."
    },
    {
        "text": "and we don\u2019t want to give aberrantly high importance to a word that shows up only once."
    },
    {
        "text": "five occurrences in the corpus is a common minimum for the word to be counted."
    },
    {
        "text": "even requiring just two will chop out a lot of noise such as typos."
    },
    {
        "text": "16.8 \u00adn-grams often we don\u2019t want to look just at individual words, but phrases."
    },
    {
        "text": "the key term here is an \u201cn-gram\u201d \u2013 a sequence of n words that appear consecutively."
    },
    {
        "text": "a piece of text containing m words can be broken into a collection of m \u2212 n + 1 n-grams, as shown in the following figure for 2-grams: the quick quick brown brown fox over the fox jumped jumped over the lazy lazy dog the quick brown fox jumped over the lazy dog"
    },
    {
        "text": "16 natural language processing 236 you can create a bag-of-words out of n-grams, run tf-idf on them, or model them with a markov chain, just as if they were normal words."
    },
    {
        "text": "the problem with n-grams is that there are so many potential ones out there."
    },
    {
        "text": "most n-grams that appear in a piece of text will occur only once, with the fre- quency decreasing, the larger n is."
    },
    {
        "text": "the general approach here is to only look at n-grams that occur more than a certain number of times in the corpus."
    },
    {
        "text": "16.9 \u00adstop words bag-of-words, tf-idf, and n-grams are fairly general processing techniques, which can be applied to many other areas."
    },
    {
        "text": "now let\u2019s move into some more truly more nlp-oriented extensions to bag-of-words."
    },
    {
        "text": "in most cases, this consists of a preprocessing step that canonicalizes the text \u2013 putting everything in a standard- ized format suitable for downstream processing by something such as tf-idf."
    },
    {
        "text": "the simplest version is to remove what are called \u201cstop words.\u201d these are words such as \u201cthe,\u201d \u201cit,\u201d and \u201cand\u201d that aren\u2019t really informative in and of them- selves."
    },
    {
        "text": "they are critically important if you\u2019re trying to parse the structure of a sentence, but for bag-of-words, they are just noise."
    },
    {
        "text": "there is no absolute defini- tion of \u201cstop words.\u201d frequently, they are found by taking the most common words in a corpus, then going through them by hand to determine which ones aren\u2019t really meaningful."
    },
    {
        "text": "in other cases (such as the aforementioned example script), there is a list of them prebuilt into an nlp library."
    },
    {
        "text": "stop words become problematic when you are using n-grams."
    },
    {
        "text": "for example, the very informative phrase \u201cto be or not to be\u201d is likely to get stopped out entirely!"
    },
    {
        "text": "16.10 \u00adlemmatization and stemming the other big approach is called \u201clemmatization.\u201d the \u201clemma\u201d for a word is the base word from which it is derived."
    },
    {
        "text": "intuitively, if we are making a bag-of- words, then \u201crunning,\u201d \u201cran,\u201d and \u201cruns\u201d should all count the same."
    },
    {
        "text": "in linguistic terms, they are all variations of the lemma \u201crun,\u201d and we would want to turn them all into \u201crun\u201d as a preprocessing step."
    },
    {
        "text": "some form of lemmatization is extremely important in english, but it is criti- cal in many other languages."
    },
    {
        "text": "i was surprised when i first learned that english is actually fairly tame when it comes to modifying our words."
    },
    {
        "text": "languages such as spanish, for example, have an arbitrary \u201cgender\u201d assigned to their nouns, and any adjective will be tweaked to reflect that gender of the noun it is describing."
    },
    {
        "text": "other languages make liberal use of \u201ccase\u201d in their nouns, where the noun is modified to reflect its part of speech."
    },
    {
        "text": "noun case is the difference between \u201ci,\u201d \u201cme,\u201d \u201cmy,\u201d and \u201cmine\u201d; english uses noun case in a few special instances as this, but otherwise, the only common casing is the use of \u201c\u2019s\u201d to indicate possession."
    },
    {
        "text": "16.12 part of speech tagging 237 the problem with lemmatization is that, in general, it is extremely computa- tionally expensive because it requires a certain amount of understanding of the text."
    },
    {
        "text": "extracting the lemma for a given word requires knowing its part of speech, for example, which requires analysis of the surrounding sentence."
    },
    {
        "text": "a simpler approach, which is less accurate than lemmatization but quicker to run and easier to implement, is called \u201cstemming.\u201d the \u201cstem\u201d of a word is very similar to the lemma."
    },
    {
        "text": "the difference is that the lemma is itself a version of the word in question, whereas the stem is just the part of a word that doesn\u2019t change despite the inflection of the word."
    },
    {
        "text": "so between the words \u201cproduce,\u201d \u201cproducer,\u201d \u201cproduct,\u201d \u201cproduction,\u201d and \u201cproducing,\u201d the lemma will be \u201cproduc-\u201d."
    },
    {
        "text": "the implementation of a stemmer generally just applies many different rules of thumb to try reducing things down to their stems."
    },
    {
        "text": "for example, it will typi- cally strip off \u201cing\u201d or \u201cation\u201d if they occur at the end of a word (unless such stripping would leave too few remaining letters)."
    },
    {
        "text": "i have even seen them imple- mented using a big lookup table, which maps the words of the language in their various forms to their respective stems."
    },
    {
        "text": "16.11 \u00adsynonyms intuitively, when we are trying to analyze the text, the words themselves are less important than the \u201cmeaning.\u201d this suggests that when we might want to collapse related terms such as \u201cbig\u201d and \u201clarge\u201d into a single identifier."
    },
    {
        "text": "these identifiers are often called \u201csynsets,\u201d for sets of synonyms."
    },
    {
        "text": "many nlp packages leverage use of synsets as a major component of their ability to understand a piece of text."
    },
    {
        "text": "the simplest use of synsets is to take a piece of text and replace every word with its corresponding synset."
    },
    {
        "text": "this is a souped-up version of lemmatization, because we don\u2019t just collapse \u201crun\u201d and \u201crunning\u201d into a single thing \u2013 we also mix \u201csprinting\u201d in there."
    },
    {
        "text": "ultimately, i think of synsets as constituting the vocabulary of a \u201cclean\u201d language, one in which there is a one-to-one matching between meanings and words."
    },
    {
        "text": "it strips out all the ambiguity that confounds computer programs studying real languages."
    },
    {
        "text": "that\u2019s all very rosy-sounding, but there is a major hitch: in general, a word can belong to several synsets, because a word can have several distinct mean- ings."
    },
    {
        "text": "so doing a translation from the original language to \u201csynset-ese\u201d is not always feasible."
    },
    {
        "text": "16.12 \u00adpart of speech tagging as we move from purely computational techniques toward ones based more closely on language, the next stage is part-of-speech tagging (pos tagging),"
    },
    {
        "text": "16 natural language processing 238 where we identify whether words in a sentence are nouns, verbs, adjectives, and so on."
    },
    {
        "text": "this can be done in nltk as follows: >>> nltk.pos_tag([\"i\", \"drink\", \"milk\"]) [('i', 'prp'), ('drink', 'vbp'), ('milk', 'nn')] in this case, the prp tag tells us that \u201ci\u201d is a prepositional phrase, \u201cdrink\u201d is a verb phrase, and \u201cmilk\u201d is a common noun in singular form."
    },
    {
        "text": "a complete list of the pos tags nltk uses can be seen by calling >>> nltk.help.upenn_tagset() 16.13 \u00adcommon problems this section will give a brief overview of a number of the areas to which nlp can be applied."
    },
    {
        "text": "each section is a massive subject in its own right, with a sophis- ticated suite of techniques and best practices."
    },
    {
        "text": "as a data scientist, you are unlikely to build a state-of-the-art version of any of them."
    },
    {
        "text": "however, you could easily be called upon to do a simple version, and if so, this section should give you some pointers."
    },
    {
        "text": "16.13.1 search one of the most straightforward tasks for nlp is searching through a set of documents to find those that match a query."
    },
    {
        "text": "searching is often broken into two types: navigational and research."
    },
    {
        "text": "in navigational search, there is typically a single document that the user is try- ing to locate, and the goal of the search engine is to find it."
    },
    {
        "text": "research search is much more general \u2013 typically, a user does not know which documents, if any, are relevant to their query, and they expect to be inspecting a lot of them by hand."
    },
    {
        "text": "anybody who has used a modern search engine can imagine the wealth of special cases, caching of common queries, and other work involved in creat- ing such a product."
    },
    {
        "text": "it is well beyond the scope of what most data scientists do."
    },
    {
        "text": "searching is often implemented by extracting a bag-of-words for the query string and for every document in the corpus."
    },
    {
        "text": "especially if you are baking com- plicated linguistic processing into your bag-of-words, vectorizing an entire corpus can be an extremely computationally intensive process."
    },
    {
        "text": "fortunately though, you only have to do it once, and you can store the vectors for later use."
    },
    {
        "text": "executing a search then just consists of vectorizing the query itself (which is typically much easier, because the query is short) and comparing its vector against all those in the database."
    },
    {
        "text": "16.13 common problems 239 the typical way that vectors are compared is called \u201ccosine similarity,\u201d which consists of the following steps: 1) normalize each vector, so that the sum of squares of its numbers is 1.0. this can be done offline for your corpus of documents, so that you only have to normalize the query itself at query time."
    },
    {
        "text": "2) take the dot product of the query and each normalized corpus vector."
    },
    {
        "text": "the resulting number is called the \u201ccosine similarity\u201d because, in the case of two vectors of length 1 (remember we normalized them), the dot product is just the cosine of the angle between them."
    },
    {
        "text": "it will be 1.0 if the vectors are identi- cal, falling off to 0.0 if the query and the text have no words in common."
    },
    {
        "text": "cosines can of course be negative, but in the case of bag-of-words, all components are nonnegative (since you can\u2019t have fewer than 0 occurrences of a word in a piece of text), so the lowest possible cosine similarity is 0."
    },
    {
        "text": "16.13.2 sentiment analysis sentiment analysis is typically used to refer to gauging the tone of a piece of text \u2013 positive, negative, or neutral."
    },
    {
        "text": "this is what we did in the example script at the beginning of this chapter to identify, in a fraction of a second, whether analysts are saying good or bad things about the stock."
    },
    {
        "text": "ideally, we can get this insight before any flesh-and-blood humans have a chance to read the article the old-fashioned way and trade based on it."
    },
    {
        "text": "there are more complicated ver- sions of sentiment analysis that can, for example, determine complicated emotional content such as anger, fear, and elation."
    },
    {
        "text": "but the most common examples focus on \u201cpolarity,\u201d where on the positive\u2013negative continuum a sentiment falls."
    },
    {
        "text": "simple sentiment analysis is often done with handmade lists of keywords."
    },
    {
        "text": "if words such as \u201cbad\u201d and \u201chorrendous\u201d occur a lot in a piece of text, it strongly suggests that the overall tone is negative."
    },
    {
        "text": "that\u2019s what we did in our example script."
    },
    {
        "text": "slightly more sophisticated versions are based on plugging bag-of-words into machine learning pipelines."
    },
    {
        "text": "commonly, you will classify the polarity of some pieces of text by hand and then use them as training data to train a senti- ment classifier."
    },
    {
        "text": "this has the massive advantage that it will implicitly identify key words you might not have thought of and will figure out how much each word should be weighted."
    },
    {
        "text": "if you use extensions of the bag-of-words model, similar to n-grams, you can also identify phrases such as \u201cnose dive,\u201d which deserve a very large weight in sentiment analysis but whose constituent words don\u2019t mean much."
    },
    {
        "text": "the most advanced sentiment analysis goes beyond bag-of-words."
    },
    {
        "text": "in this case, you must do things such as parsing a sentence to figure out which entities are being described with words such as \u201cbad.\u201d however, you can often work"
    },
    {
        "text": "16 natural language processing 240 around this by examining smaller pieces of text."
    },
    {
        "text": "if you are parsing an article on an industry in the stock market, for example, there are likely to be many com- panies discussed in both positive and negative ways."
    },
    {
        "text": "however, a given sentence or paragraph is likely to have one predominant sentiment and only refer to a single company."
    },
    {
        "text": "16.13.3 entity recognition and topic modeling in many cases, we have a corpus of documents and want to determine what \u201cthings\u201d they talk about."
    },
    {
        "text": "this can run the gamut from pulling out specific enti- ties (such as the names of human beings mentioned in a document) to more general topics."
    },
    {
        "text": "typically, identifying specific entities being discussed is referred to as \u201centity recognition\u201d or \u201cnamed entity recognition.\u201d there are many ways to go about this, and you can imagine the amount of hand coding (or massive amounts of training data) that is often required to recognize that \u201crobert\u201d and \u201cbob\u201d are likely to be the same person."
    },
    {
        "text": "entity recognition often makes extensive use of pos tagging, because generally, it is only the nouns in a sentence (and usually, only the proper nouns at that) that are viable candidates for entities."
    },
    {
        "text": "\u201ctopic modeling\u201d usually refers to finding much broader topics."
    },
    {
        "text": "a given piece of text is generally thought of as a combination of several topics, and the critical intuition is that words that are used frequently together tend to be related to the same \u201ctopic.\u201d for example, a document that is half about cats and half about dogs should have similar amounts of dog-related terms (bark, wolf, howl, etc.)"
    },
    {
        "text": "and cat-related terms (purr, litter, etc.)."
    },
    {
        "text": "you might think that this sounds like an obvious place to apply principal com- ponent analysis (pca)."
    },
    {
        "text": "if so, then you\u2019re pretty close to the mark, but not quite hitting it."
    },
    {
        "text": "the usual tool of choice for topic modeling is called latent semantic analysis (lsa), and it is based on a mathematical notion called singular value decomposition (svd)."
    },
    {
        "text": "as with pca, every \u201ctopic\u201d in our corpus corresponds to a vector in word space, and we express every document as a linear combination of these topics."
    },
    {
        "text": "for instance, a particular topic might have large weight on words such as \u201ctouchdown,\u201d \u201cquarterback,\u201d and \u201cball.\u201d the reason we use svd rather than pca is that svd forces our components to be orthogonal to each other."
    },
    {
        "text": "this avoids a potentially awkward situation where one of our topic vectors can be expressed, as least partly, as a combination of other topics."
    },
    {
        "text": "16.14 \u00adadvanced nlp: syntax trees, knowledge, and understanding i promised that i would briefly discuss topics that go beyond what can be done with bag-of-words and require something in the direction of \u201cunderstanding\u201d"
    },
    {
        "text": "16.15 further reading 241 the text."
    },
    {
        "text": "if, for example, you are trying to answer specific question about the data such as \u201cwho is john dating?,\u201d then you will need something beyond bag- of-words."
    },
    {
        "text": "typically in these situations, nlp is used to create \u201cknowledge bases,\u201d which store facts in a format where machines can use them for queries and reasoning."
    },
    {
        "text": "the idea of a knowledge base isn\u2019t really new, and in fact, the mathematical theory of relational databases functioned essentially as a knowledge base that supported logical queries."
    },
    {
        "text": "typically, a knowledge base will have tables that represent facts."
    },
    {
        "text": "for exam- ple, you might have a table called isparentof, which has one column for a par- ent and another for their child."
    },
    {
        "text": "in this way, a knowledge base is very similar to a relational database, and logical questions become equivalent to sql-like queries."
    },
    {
        "text": "for example, we could find all people who have at least two children by saying select a.parent from isparentof as a join isparentof as b on a.parent = b.parent where a.child != b.child you could use a relational database as a poor man\u2019s knowledge system."
    },
    {
        "text": "combing through bodies of text you could, for example, identify every place that somebody is said to be the mother or father of somebody else and use this to populate the isparentof table."
    },
    {
        "text": "the problem with this is that we have to know going into it that fatherhood is a relationship we are interested in and how to parse it out."
    },
    {
        "text": "additionally, there are other logical rules that are not captured in an rdb, such as that fact that while a parent can have many children, every child can have at the most two parents."
    },
    {
        "text": "modern knowledge bases typically augment the sql-like tables with logical rules that describe the categories of things being discussed and the relation- ships between them."
    },
    {
        "text": "these collections of domain-specific categories and rules are also often called \u201contologies.\u201d 16.15 \u00adfurther reading 1 bird, s, klein, e & loper, e, natural language processing with python, 2009, o\u2019reilly media, newton, ma."
    },
    {
        "text": "2 jurafsky, d & martin, j, speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition, 2000, prentice hall, upper saddle river, nj."
    },
    {
        "text": "16 natural language processing 242 16.16 \u00adglossary bag-of-words condensing a piece of text into its word frequencies."
    },
    {
        "text": "entity recognition using text to identify the specific real-world entities being discussed."
    },
    {
        "text": "knowledge base a database storing facts."
    },
    {
        "text": "lemma the base, uninflected form of a word."
    },
    {
        "text": "ontology a collection of concepts and relationships between them that are required for understanding a particular domain of application."
    },
    {
        "text": "part of speech a grammatical part of speech (such as noun, verb, etc.)"
    },
    {
        "text": "of a word in a sentence."
    },
    {
        "text": "pos short for part of speech."
    },
    {
        "text": "sentiment analysis automatically gauging the tone of a piece of text, especially whether it is positive or negative."
    },
    {
        "text": "stem the base part of a word that doesn\u2019t change between various forms of the word."
    },
    {
        "text": "can often be used in place of lemmas."
    },
    {
        "text": "stop word a word that is common and not helpful in assessing a text\u2019s meaning."
    },
    {
        "text": "in many applications, they are often filtered out because they act as noise."
    },
    {
        "text": "topic modeling identifying topics of discussion in documents."
    },
    {
        "text": "often, a topic is modeled as a collection of words (such as \u201cfootball\u201d and \u201cquarterback\u201d), which are usually rare, but which sometimes are all frequent in a document."
    },
    {
        "text": "tf-dif a method of weighting the importance of words so that the less common words are more important."
    },
    {
        "text": "tokenization breaking a piece of text into its \u201ctokens.\u201d the tokens are usually words but will also often split something like \u201cit\u2019s\u201d into two tokens."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 243 17 time series analysis, in my experience, is not as common as you might expect in data science work."
    },
    {
        "text": "however, that seems to be largely an artifact of the data- sets that it has been applied to thus far, which tends to be legacy business spreadsheets and dumps of sql databases."
    },
    {
        "text": "especially as sensor nets become more ubiquitous, time series will come to play a much larger role in daily work."
    },
    {
        "text": "at that point, data scientists will have a lot of catching up to do, because elec- trical engineers have been analyzing time series for decades."
    },
    {
        "text": "typical applications of time series analysis in data science include the following: \u25cf \u25cfpredicting when/whether an event will occur, such as a failure of the machine generating the data \u25cf \u25cfprojecting the value of the time series at future points in time, such as a stock whose price we want to predict \u25cf \u25cfidentifying interesting patterns in a corpus of time series data that is too large for a human to comb through."
    },
    {
        "text": "all of these business applications can ultimately be formulated as machine learning problems."
    },
    {
        "text": "for example: \u25cf \u25cfif we are trying to predict whether a component is at risk of failure, this is a classification problem: we extract various features from the data to date (especially its recent history) and use it to predict a binary variable of whether it will fail soon (say, in the next hour)."
    },
    {
        "text": "\u25cf \u25cflet\u2019s say we want to predict the value of a time series in the future."
    },
    {
        "text": "well, finding the value of the time series an hour from now based on recent meas- urements can be formulated as a regression problem."
    },
    {
        "text": "\u25cf \u25cfif you are just looking for interesting patterns, this is often accomplished using clustering or dimensionality\u2010reduction algorithms."
    },
    {
        "text": "most of this chapter boils down to techniques for converting time series analysis problems into machine learning problems and some of the unique challenges this poses."
    },
    {
        "text": "time series analysis"
    },
    {
        "text": "17 time series analysis 244 17.1 \u00adexample: predicting wikipedia views the following example script downloads a time series of wikipedia views by day."
    },
    {
        "text": "it then does several things: \u25cf \u25cfi plot the raw time series and look at it."
    },
    {
        "text": "\u25cf \u25cfit is visually clear that there are some major outliers, which are likely to con- found any model i care to fit, so i cap them off by setting everything above the 95th percentile to the 95th percentile value."
    },
    {
        "text": "this is crude and brute force, but it works for the time being."
    },
    {
        "text": "\u25cf \u25cfi expect that there will be a weekly periodicity to this signal, so i use the statsmodel library to break it down into a periodic component, a trend com- ponent, and a noise component."
    },
    {
        "text": "this is a very error\u2010prone step, but it can be very useful too."
    },
    {
        "text": "\u25cf \u25cfi break the time series into week\u2010long sliding windows and pull out some salient features from each window."
    },
    {
        "text": "those features are then used to train a regression model for predicting traffic in the next week."
    },
    {
        "text": "it\u2019s not fancy, but this is the kind of thing you are likely to do at least as a first cut with time series data."
    },
    {
        "text": "import urllib, json, pandas as pd, numpy as np, \\ sklearn.linear_model, statsmodels.api as sm \\ matplotlib.pyplot as plt start_date = \"20131010\" end_date = \"20161012\" window_size = 7 topic = \"cat\" url_template = (\"https://wikimedia.org/api/rest_v1\" \"/metrics/pageviews/per-article\" \"/en.wikipedia/all-access/\" \"allagents/%s/daily/%s/%s\") def get_time_series(topic, start, end): url = url_template % (topic, start, end) json_data = urllib.urlopen(url).read() data = json.loads(json_data) times = [rec['timestamp'] for rec in data['items']] values = [rec['views'] for rec in data['items']] times_formatted = pd.series(times).map( lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:8]) time_index = times_formatted.astype('datetime64')"
    },
    {
        "text": "17.1 example: predicting wikipedia views 245 return pd.dataframe( {'views': values}, index=time_index) def line_slope(ss): x=np.arange(len(ss)).reshape((len(ss),1)) linear.fit(x, ss) return linear.coef_ # linearregression object will be # re-used several times linear = sklearn.linear_model.linearregression() df = get_time_series(topic, start_date, end_date) # visualize the raw time series df['views'].plot() plt.title(\" views by day\") plt.show() # blunt-force way to remove outliers max_views = df['views'].quantile(0.95) df.views[df.views > max_views] = max_views # visualize decomposition decomp = sm.tsa.seasonal_decompose(df['views'].values, freq=7) decomp.plot() plt.suptitle(\" views decomposition\") plt.show() # for each day, add features from previous week df['mean_1week'] = pd.rolling_mean( df['views'], window_size) df['max_1week'] = pd.rolling_max( df['views'], window_size) df['min_1week'] = pd.rolling_min( df['views'], window_size) df['slope'] = pd.rolling_apply( df['views'], window_size, line_slope) df['total_views_week'] = pd.rolling_sum( df['views'], window_size) df['day_of_week'] = df.index.astype(int) % 7 day_of_week_cols = pd.get_dummies(df['day_of_week']) df = pd.concat([df, day_of_week_cols], axis=1) # make target variable that we # want to predict: views next week."
    },
    {
        "text": "17 time series analysis 246 # must pad w nans so dates line up df['total_views_next_week'] = list(df['total_views_ week'][window_size:]) + \\ [np.nan for _ in range(window_size)] indep_vars = ['mean_1week', 'max_1week', 'min_1week', 'slope'] + range(6) dep_var = 'total_views_next_week' n_records = df.dropna().shape[0] test_data = df.dropna()[:n_records/2] train_data = df.dropna()[n_records/2:] linear.fit( train_data[indep_vars], train_data[dep_var]) test_preds_array = linear.predict( test_data[indep_vars]) test_preds = pd.series( test_preds_array, index=test_data.index) print \"corr on test data:\", \\ test_data[dep_var].corr(test_preds) the script will produce the following outputs: 90,000 80,000 70,000 60,000 50,000 40,000 30,000 20,000 10,000 0 jul aug sep oct nov dec jan 2016 views by day feb mar apr may"
    },
    {
        "text": "17.2 a typical workflow 247 observed trend views decomposition seasonal residual 0 50 250 200 150 100 300 time 12,000 11,000 10,000 9000 8000 7000 6000 10,500 11,000 10,000 9500 9000 8500 8000 7500 400 600 2000 \u2013400 \u2013600 \u2013800 \u20131000 \u20131200 2000 2500 1500 1000 5000 \u2013500 \u20131000 \u20131500 \u20131500 corr on test data: 0.78864314787 17.2 \u00ada typical workflow a typical time series work flow will look something like this: \u25cf \u25cfresampling and interpolation."
    },
    {
        "text": "time series data often has missing values and/or is sampled at nonuniform rates."
    },
    {
        "text": "however, most algorithms require uniform sampling with no missing data."
    },
    {
        "text": "the first step is to convert the raw input data into uniformly sampled time series."
    },
    {
        "text": "\u25cf \u25cfsometimes i\u2019m working with time\u2010stamped events, rather than time series measurements."
    },
    {
        "text": "in this case, i have to first condense these events into time series data."
    },
    {
        "text": "for example, maybe by counting the events per day."
    },
    {
        "text": "\u25cf \u25cfseries\u2010level preprocessing and denoising."
    },
    {
        "text": "oftentimes, we want to try various methods of removing noise from the data, smoothing outliers, scaling it to appropriate levels, or other types of preprocessing."
    },
    {
        "text": "\u25cf \u25cfwindowing."
    },
    {
        "text": "most applications involve breaking a whole time series down into smaller windows of time, from which we can extract features."
    },
    {
        "text": "how large these windows should be, whether and how much they should over- lap, and how we should decide where to place them are all important questions."
    },
    {
        "text": "17 time series analysis 248 \u25cf \u25cffeature extraction."
    },
    {
        "text": "once we\u2019ve broken the series down into windows, we generally want to extract meaningful features from each window that we can plug into a machine learning model."
    },
    {
        "text": "the other thing that we sometimes do, which is superficially similar to pro- jecting its value but ultimately quite different, is constructing a model of the time series that describes how it behaves as a random process."
    },
    {
        "text": "17.3 \u00adtime series versus time-stamped events there are two very different types of data that are sometimes called \"time series\": 1) actual time series, that is, a sequence of numbers that are associated with different points in time."
    },
    {
        "text": "this will often be things such as the price of a stock at different points in the day, the amount of revenue that was made each day in a month, or temperature measurements coming off of a physical sensor."
    },
    {
        "text": "2) discrete events (or periods of time) that have a time stamp associated with them."
    },
    {
        "text": "this chapter will focus almost exclusively on the first of these, for two reasons: 1) most time series analysis techniques are designed to handle numerical measurement data."
    },
    {
        "text": "there\u2019s not a lot you can say about event data that gen- eralizes beyond a particular domain of application."
    },
    {
        "text": "2) even when we do have event data, often what we really want is still more of a continuous thing."
    },
    {
        "text": "take internet traffic, for example."
    },
    {
        "text": "we might care about predicting how many people will visit a website on tuesday, but we are rarely concerned with how many milliseconds it will be until the next per- son arrives or how likely it is that there will be five people on the site at any moment in time."
    },
    {
        "text": "intuitively, there is a continuous\u2010valued \"traffic density\" that varies across time, and individual human visits are just samples of that density."
    },
    {
        "text": "generally, people convert event logs into time series by dividing up time into fixed\u2010size windows and counting the events in each window."
    },
    {
        "text": "usually, this is done in human\u2010relatable terms such as events per day, events per hour, or something similar, but you can also adjust the windows\u2019 size to the granularity that works best for your analyses."
    },
    {
        "text": "17.4 resampling an interpolation 249 for the rest of this chapter, unless otherwise specified, you can assume that i mean a time series in the traditional sense."
    },
    {
        "text": "17.4 \u00adresampling an interpolation for some terminology, let\u2019s say that we have time stamps t1, t2, ..., tn, and our measurement at time ti is f (ti)."
    },
    {
        "text": "in general, the ti might have different distances between them, but pretty much every algorithm requires equal spacing."
    },
    {
        "text": "so, you need to find a new set of points t1, t2, ..., tn to estimate your signal at, where ti+1 = ti + \u03b4, and get estimates of f (ti)."
    },
    {
        "text": "oftentimes, the easiest way to do it is to set t1 = t1, and then have delta be a parameter that you fiddle with."
    },
    {
        "text": "there are a couple ways to get f (ti)."
    },
    {
        "text": "the easiest is just to find the tj that is closest to ti and adopt its value, so that your interpolated f (x) is a piecewise\u2010 constant function, as follows: even easier to implement is backfilling or forward filling, where you just carry f(ti) forward until you hit the next tj."
    },
    {
        "text": "these methods are crude, but they are built\u2010in to most libraries you\u2019re likely to use, and they\u2019re trivially easy to implement yourself if need be."
    },
    {
        "text": "if your ti are fairly dense, you often don\u2019t need anything more."
    },
    {
        "text": "honestly, it\u2019s my own go\u2010to interpolation method, at least as a first cut, since it\u2019s trivially easy in pandas: >>> # make sure index is already a timestamp >>> df_indexed = df.set_index('timestamp') >>> df_sampled = df_indexed.asfreq( '1min', method='backfill')"
    },
    {
        "text": "17 time series analysis 250 the next simplest interpolation method is piecewise linear, which is shown as follows: the next step up in sophistication is spline interpolation."
    },
    {
        "text": "linear interpola- tion has some obvious pathologies near the known points \u2013 surely no real sig- nal will have \"teeth\" like that."
    },
    {
        "text": "the idea of spline interpolation is that instead of fitting a line to two points, we fit a polynomial to three or more of them."
    },
    {
        "text": "the most common choice is a cubic spline, where values between ti and ti+1 will be interpolated based on ti\u22121 to ti+2."
    },
    {
        "text": "this gives up a nice smooth\u2010looking, continu- ously differentiable function like we see in the following."
    },
    {
        "text": "unfortunately, linear and spline interpolations are not supported in pandas for interpolating at arbitrary points (although pandas does let you use interpo- lation to fill in missing data, with the interpolate() method on series objects)."
    },
    {
        "text": "however, scipy has a nifty method called interp1d that takes in your data points and returns a callable object that you can treat as an interpolated func- tion."
    },
    {
        "text": "by default, it does linear interpolation, but it can also do cubic or a variety of others if you change the optional arguments."
    },
    {
        "text": "for example: >>> import scipy.interpolate as si >>> s = pd.series([0, 2, 4, 6]) >>> s_sqrd = s * s >>> linear_interp = si.interp1d(s, s_sqrd) >>> linear_interp([3,5]) array([ 10., 26.])"
    },
    {
        "text": ">>> cubic_interp = si.interp1d(s, s_sqrd, kind=\"cubic\") >>> cubic_interp([3,5]) array([ 9., 25.])"
    },
    {
        "text": "17.5 smoothing signals 251 a major problem in any interpolation scheme is how to deal with interpolat- ing x\u2010values that fall outside of the range for which we have data."
    },
    {
        "text": "generally, this is called \u201cextrapolation\u201d rather than interpolation, since the point is out- side the range of x\u2010values in our data."
    },
    {
        "text": "extrapolation is a dicey business: the obvious way to do it is to go back to our linear (or other) interpolation, look at the two points closer to the one we want, and extend the line out."
    },
    {
        "text": "but this is likely to give values that are grossly incorrect when you look at the data as a whole."
    },
    {
        "text": "for example: interp1d will throw an error by default unless you pass bounds_error=false in as an optional parameter."
    },
    {
        "text": "even then, it will give you a null value for those points rather than an actual extrapolation."
    },
    {
        "text": "17.5 \u00adsmoothing signals the simplest way to smooth data is to take the moving average."
    },
    {
        "text": "this just means that we replace each measurement with the average of a fixed number k of measurements before it."
    },
    {
        "text": "or equivalently, we could replace it with the average of some before and some after it \u2013 the difference is just in the time stamp."
    },
    {
        "text": "it\u2019s also common to not just take the normal average, but instead a weighted one."
    },
    {
        "text": "it makes a lot of sense that f smooth(ti) would be more influenced by f (ti) than f (ti\u22121)."
    },
    {
        "text": "one of the neatest variations on moving average is called exponential smooth- ing."
    },
    {
        "text": "in exponential smoothing, f smooth(ti) will be a weighted average over f (ti), f (ti\u22121), and so on, with exponentially decreasing weights."
    },
    {
        "text": "this works out the simple formula f t f t smooth 1 1 ( ) = ( )"
    },
    {
        "text": "17 time series analysis 252 f t f t f t smooth i i i smooth i + + ( ) = ( )+ \u2212 ( ) ( ) \u03b1 \u03b1 1 1 the moving average is great and extremely popular."
    },
    {
        "text": "however, it can suffer if your data contains aberrantly large spikes of noise."
    },
    {
        "text": "in many cases, a massive measurement is a pathology that should be replaced, but the moving average will instead smear the short, tall spike into a slightly wider and flatter one, pos- sibly even making it nonobvious that the original data point was an error."
    },
    {
        "text": "for this reason, i\u2019m a fan of the moving median, where you replace each point in the time series with the median of the k points before it."
    },
    {
        "text": "this is much more computationally intensive than a moving average, but it\u2019s more robust."
    },
    {
        "text": "you can also do what i did in the example script at the start of this chapter: cap the values to remove gross outliers."
    },
    {
        "text": "17.6 \u00adlogarithms and other transformations in many domains, it makes sense to apply time series analysis not to the raw data itself, but to some mathematical function of the data."
    },
    {
        "text": "the most important of these is the logarithmic transformation in domains such as finance."
    },
    {
        "text": "there are two reasons you might want to do this: \u25cf \u25cfin the interest of making your time series meaningful, it is sometimes good to have a movement on the y\u2010axis be equally impactful no matter where it starts."
    },
    {
        "text": "taking the logarithm accomplishes this."
    },
    {
        "text": "\u25cf \u25cfoftentimes, if you try to fit a regression model to your data, the standard models won\u2019t do, but they will perform well on the transformation."
    },
    {
        "text": "17.7 \u00adtrends and periodicity in the example script, we said decomp = sm.tsa.seasonal_decompose( df['views'].values, freq=7) decomp.plot() the idea here is that we have data that has a strong cyclic behavior (with a periodicity that we know) overlaid with an overall smooth trend component, plus some random noise."
    },
    {
        "text": "by looking at the trend in isolation, we can get a smoother, seasonality\u2010adjusted version of the time series."
    },
    {
        "text": "if the time series was measuring traffic to a website, for example, we could compare saturday\u2019s value"
    },
    {
        "text": "17.8 windowing 253 with wednesday\u2019s directly, without worrying about the fact that one occurs on a weekend, if we look only at the trend component."
    },
    {
        "text": "if this sounds too good to be true, then yes: often is."
    },
    {
        "text": "first off, seasonal decomposition only works if the periodicity is exact."
    },
    {
        "text": "that\u2019s fine with daily web traffic, but won\u2019t work for many processes in the real world where sometimes things take more, sometimes less time."
    },
    {
        "text": "this decomposition also assumes that the noise, seasonality, and trend are additive; it models fixed-size changes in the amount of traffic, but not proportional changes."
    },
    {
        "text": "the seasonal component is calculated very simply: just by averaging over the corresponding days for the entire time series."
    },
    {
        "text": "the trend curve is then cal- culated by a fairly complicated process called loess."
    },
    {
        "text": "basically, it fits local polynomials to the data and then splices them together: think like spline inter- polation, but without having to cross through all of the data points."
    },
    {
        "text": "17.8 \u00adwindowing now we will start to transition from processing that is specific to time series and start looking at how we can turn time series into machine learning prob- lems."
    },
    {
        "text": "in typical machine learning problem, you have clear \"entities\" that are your objects of study, such as ads that did or didn\u2019t get clicked on or human customers in a database."
    },
    {
        "text": "this is not the case with time series data."
    },
    {
        "text": "we just have a list of measurements that were sampled at some frequency, and how you divide that up into \"entities\" is a touchy question."
    },
    {
        "text": "the typical approach is windowing, where we select equal\u2010size windows from the signal and separately classify them and extract features."
    },
    {
        "text": "for example, we might \u25cf \u25cfbreak our data into 1\u2010hour windows, excluding windows in which the machine we\u2019re monitoring failed; \u25cf \u25cffor each window, extract some features such as average value, fourier com- ponents, and so on; \u25cf \u25cflabel each window according to whether the machine failed within the next hour; \u25cf \u25cfdivide into testing/training data and break out your favorite ml classifier."
    },
    {
        "text": "windowing makes a lot of sense when you think of it in terms of common applications."
    },
    {
        "text": "often in the real world, you want to make a prediction or decision based on the time series data available to date, as of some moment in time."
    },
    {
        "text": "windowing corresponds to making a decision based on data available as of the end of the window."
    },
    {
        "text": "if your window length is w and you have n measurement in your time series, then there are n \u2212 w + 1 possible windows you could select."
    },
    {
        "text": "in the script at the"
    },
    {
        "text": "17 time series analysis 254 beginning of this chapter, we used all of these windows, but generally, you don\u2019t want to do that for two reasons: 1) there are potentially a lot of data points."
    },
    {
        "text": "2) the windows will overlap a lot."
    },
    {
        "text": "this is begging for overfitting, and it also means that you are needlessly feeding almost\u2010duplicate data into whatever ml algorithms you end up using."
    },
    {
        "text": "an alternative option is to line the windows up one after another, so that a new one starts as soon as one ends."
    },
    {
        "text": "this way they are not overlapping, and you are still using all the data."
    },
    {
        "text": "this is often your best option, especially if the win- dow length is \"natural\" for your domain of application, such as 24\u2010hour peri- ods."
    },
    {
        "text": "you could call this the \"covering windows\" approach."
    },
    {
        "text": "there are two potential problems with covering windows: \u25cf \u25cfif there is periodicity that is the length of the window, then all the windows will look artificially similar."
    },
    {
        "text": "say, for example, you\u2019re measuring something (such as internet traffic, temperature, etc.)"
    },
    {
        "text": "that moves on a daily cycle, and your classifier is trained on 24\u2010hour windows that go from midnight to mid- night."
    },
    {
        "text": "if it\u2019s 3 pm right now and you\u2019re trying to predict whether a compo- nent will fail, your data for the last 24 hours will look nothing like any of the training data."
    },
    {
        "text": "\u25cf \u25cfoften, there are events that you want to line up in the windows in a specific way, and you lose this ability."
    },
    {
        "text": "for example, if you\u2019re building an alarm system to predict machine failures, then you probably have several points in time where a failure is known to have occurred."
    },
    {
        "text": "you want to make sure that you have windows in your training data that end shortly before those failures, but with long enough time that you could work to avoid the failure."
    },
    {
        "text": "a failure event is wasted if you put it right in the middle of one of your windows."
    },
    {
        "text": "however you select your windows, bear in mind that it will affect the statisti- cal validity of your results."
    },
    {
        "text": "17.9 \u00adbrainstorming simple features in no particular order, here are some methods that i have used or considered using for extracting features from time series data: \u25cf \u25cfthe average, median, and quartile values \u25cf \u25cfthe standard deviation of values \u25cf \u25cffit a line to the data and give its slope and intercept."
    },
    {
        "text": "you can fit the line using least squares, an l1 penalty, or any other option."
    },
    {
        "text": "\u25cf \u25cffit an exponential decay/growth curve to the data and use the fitted parameters."
    },
    {
        "text": "17.10 better features: time series as vectors 255 bear in mind that there is also more to life than windowing."
    },
    {
        "text": "in real situa- tions, you have access to far more data than just the most recent time win- dow, and it makes sense to extract some features that look further back in time."
    },
    {
        "text": "this is especially the case if you have data from many different time series: the same sliding window might mean something very different if the overall time series has been decaying since it started versus staying relatively constant."
    },
    {
        "text": "what i generally do is that for a particular moment in time, i extract some features that are based only on the trailing window and others that are based on the whole lifetime of the series."
    },
    {
        "text": "some of the latter include the following: \u25cf \u25cfhow long it has been since the time series began, which will often corre- spond to the age of some physical device."
    },
    {
        "text": "\u25cf \u25cfthe point in time that the series began, expressed as a date."
    },
    {
        "text": "this might be important if, say, devices were set up differently at different points in time."
    },
    {
        "text": "\u25cf \u25cfthe usual aggregate window statistics but applied to a window at the begin- ning of the time series."
    },
    {
        "text": "this allows for interesting comparisons to the cur- rent state of affairs."
    },
    {
        "text": "\u25cf \u25cffit a curve to the whole time series and use the fitted parameters."
    },
    {
        "text": "17.10 \u00adbetter features: time series as vectors a time series window is just an array of floating numbers."
    },
    {
        "text": "as such it can be treated as a numerical vector like you\u2019re familiar with from machine learning."
    },
    {
        "text": "this opens up a wide world of additional techniques."
    },
    {
        "text": "most simply, if there is some reference window (perhaps an interesting pat- tern you\u2019ve found), you can measure the distance from other windows to the reference pattern."
    },
    {
        "text": "you can use either the normal euclidean metric dist ,x y x y i i i d ( ) = \u2212 ( ) = \u2211 2 1 or the so\u2010called taxicab metric dist ,x y abs x y i i i d ( ) = \u2212 ( ) = \u2211 1 or any other metric of interest to you."
    },
    {
        "text": "if you are only interested in the shape, then normalize the values in each window before calculating."
    },
    {
        "text": "outside of typi- cal machine learning problems, this approach is also used to find occurrences of a key pattern in a time series (or collection of many time series)."
    },
    {
        "text": "you can also plug your training windows into a clustering algorithm such as k\u2010means."
    },
    {
        "text": "then, when it comes time to extract features from another window, one of the features can be the cluster of which it would have been a part."
    },
    {
        "text": "17 time series analysis 256 what you will see the most often as a feature, though, is the dot product between your window and one or more reference windows."
    },
    {
        "text": "you might, for example, run pca on your training windows."
    },
    {
        "text": "the dominant principal compo- nents will then represent the major patterns present in your training windows."
    },
    {
        "text": "when it comes time for feature extraction, you can see how much each of each major component is in your window."
    },
    {
        "text": "note though that all of these approaches hinge critically on good selection of your windows."
    },
    {
        "text": "if a critical pattern occurs in the beginning of some windows and the end of others, then clustering algorithms won\u2019t recognize that they\u2019re the same thing."
    },
    {
        "text": "pca will have problems too."
    },
    {
        "text": "none of these techniques will be rendered completely ineffective, but they will be massively hampered."
    },
    {
        "text": "this is part of why it is so useful if you have a set of points in time where events hap- pened that you know are relevant, and you can pick some of your windows to be located relative to them."
    },
    {
        "text": "17.11 \u00adfourier analysis: sometimes a magic bullet one of the most important techniques in time series is called fourier analysis; it is often useful in data science and centrally important in engineering and physical sciences."
    },
    {
        "text": "the idea is to decompose your entire signal into a linear combination of signals that vary periodically."
    },
    {
        "text": "for example, we might say that the hourly temperature measured across a decade would have a slow 1\u2010year period (for the seasons) and a fast 24\u2010hour period for the day/night cycle."
    },
    {
        "text": "fourier analysis is a deep, conceptually rich field of mathematics."
    },
    {
        "text": "in this sec- tion, i\u2019m just going to simplify it down dramatically and only discuss the parts of it that are relevant for simple applications in the daily work of a data scientist."
    },
    {
        "text": "the key theorem is this."
    },
    {
        "text": "let\u2019s say you have an array of n numbers x1, x2, ..., xn."
    },
    {
        "text": "then there exist numbers a0, a2, ..., an\u22121 and b1, b2, ..., bn\u22121 such that x a n a m n t n b m n t t m n m m n m = + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = \u2212 = \u2212 \u2211 \u2211 0 1 1 1 1 1 2 1 2 cos sin \u03c0 \u03c0 in this expression, a0 is a constant offset, and every other term is a sinusoidal wave that oscillates with some frequency."
    },
    {
        "text": "collectively, the am and bm are called the \u201cfourier coefficients\u201d or, sometimes, the \u201cspectrum\u201d of your signal."
    },
    {
        "text": "the heart of fourier analysis is a collection of algorithms called fourier transforms, which let us convert from the raw signal to the spectrum and vice versa."
    },
    {
        "text": "an alternative version of the fourier decomposition is to say that x c n c m n t t m n m m = + + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = \u2212 \u2211 0 1 1 1 2 sin \u03c0 \u03c6"
    },
    {
        "text": "17.11 fourier analysis: sometimes a magic bullet 257 in the previous version, for a given m, we had two terms: a m n t m cos 2\u03c0 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 and b m n t m sin 2\u03c0 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7."
    },
    {
        "text": "these are two different sinusoidal signals with the same fre- quency, which we add together."
    },
    {
        "text": "but a mathematical fact is that there are cm and \u03c6m such that they add up to c m n t m m sin 2\u03c0 \u03c6 + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7."
    },
    {
        "text": "this is a single sinusoidal signal, but it happens to be offset in time by \u03c6m."
    },
    {
        "text": "you can get cm by just taking c a b m m m = + 2 2 in practice, you will usually get the am and bm out of a fourier transform and then switch over to the cm for practical applications."
    },
    {
        "text": "if the x array is just a bunch of noise, then the spectrum won\u2019t be particularly interesting."
    },
    {
        "text": "but if the signal is periodic or fluctuating, or even approximately so, then it will become glaringly obvious from looking at the spectrum."
    },
    {
        "text": "often, most of the fourier coefficients will be small, but a handful will be quite large: in these cases, those few coefficients would allow a pretty good reconstruction of our entire original signal."
    },
    {
        "text": "often in real systems, especially physical ones, oscillating signals of different frequency correspond to different underlying physical phenomena, so the fourier decomposition amounts to expressing the data as a linear combination of several real\u2010world processes."
    },
    {
        "text": "the degree to which a physical process is present is indicated by the fourier coefficients that correspond to its frequency."
    },
    {
        "text": "if we were to, say, use those coefficients as a fea- ture in a machine learning algorithm, then that feature (usually a cm) will meas- ure a real\u2010world phenomenon."
    },
    {
        "text": "this might be sounding to you like principal component analysis, where we express our raw signal as linear combination of several \u201cbase signals\u201d that are physically meaningful."
    },
    {
        "text": "yes, you are right."
    },
    {
        "text": "this becomes a linear algebra subject, and both of these approaches fall under the umbrella of finding a \u201cchange of basis.\u201d but that\u2019s an advanced topic; don\u2019t worry about it for now."
    },
    {
        "text": "the algorithm that takes in your raw signal and produces the fourier coeffi- cients is called the \"fast fourier transform\" or fft."
    },
    {
        "text": "the fft is one of the most important algorithms in history \u2013 the ability to calculate fourier transforms efficiently was one of the cornerstones of the information age."
    },
    {
        "text": "it comes built into scipy, as illustrated in the following: >>> from scipy.fftpack import fft, ifft >>> x = np.array([1.0, 2.0, 1.0, -1.0, 1.5]) >>> spec = fft(x)"
    },
    {
        "text": "17 time series analysis 258 >>> spec array([ 4.50000000+0."
    },
    {
        "text": ", 2.08155948-1.65109876j, \u20101.83155948+1.60822041j, -1.83155948-1.60822041j, 2.08155948+1.65109876j]) >>> am = spec.real >>> bm = spec.imag >>> cm = np.abs(spec) >>> x_again = ifft(spec) >>> x_again array([ 1.0+0.j, 2.0+0.j, 1.0+0.j, \u20101.0+0.j, 1.5+0.j]) a note of explanation is due."
    },
    {
        "text": "in this case, x is our raw signal and spec is the fourier transform."
    },
    {
        "text": "x is an array of length n = 5. you might expect spec to be two arrays of numbers, one of length n and another of length n\u22121."
    },
    {
        "text": "instead, it is a single array of length n, but the numbers it contains are complex numbers (here j = \u22121 is an imaginary numbers)."
    },
    {
        "text": "the way it works is that the mth component of spec will be am + j*bm."
    },
    {
        "text": "there are theoretical reasons for this: fourier transforms (unlike many mathematical concepts) work perfectly well with complex numbers, and in fact, they are even more elegant and self\u2010consistent when you think of them with complex numbers."
    },
    {
        "text": "many electrical engineers, who live and breathe fourier analysis, make extensive use of the complex nature of these numbers."
    },
    {
        "text": "but for cruder data science applications, we usually turn them back into real numbers as quickly as possible."
    },
    {
        "text": "i don\u2019t know about you, but complex numbers hurt my head."
    },
    {
        "text": "for data science, we typically do the following things with fourier transforms: \u25cf \u25cfusing them to identify periodicity in a signal."
    },
    {
        "text": "for example, if you measure blood pressure several times a second, the main frequency in the data will be a person\u2019s heart rate."
    },
    {
        "text": "\u25cf \u25cffourier coefficients as features for windows."
    },
    {
        "text": "the amount of, say, 10 hz frequency in a signal is an incredibly important, perhaps very physically meaningful feature that we can extract and plug into a machine learning algorithm."
    },
    {
        "text": "the one thing is you would have to take the magnitude of the fourier coefficient rather than the coefficient itself, since it is a complex number."
    },
    {
        "text": "\u25cf \u25cfsmoothing the data by removing high\u2010frequency jitter."
    },
    {
        "text": "this is sometimes called a \"low\u2010pass filter\" \u2013 you set all the higher coefficients to 0 and recon- struct the signal from that."
    },
    {
        "text": "17.13 further reading 259 \u25cf \u25cfremoving long\u2010time trends to study shorter timescale phenomena."
    },
    {
        "text": "this is called a \"high\u2010pass\" filter, and it works analogously to a low\u2010pass filter: set the low\u2010frequency coefficients to 0, and then reconstruct the signal."
    },
    {
        "text": "17.12 \u00adtime series in context: the whole suite of features in a typical data science application, multiple, distinct types of data will be brought to bear on a single problem."
    },
    {
        "text": "an example is monitoring machines to predict when they will fail."
    },
    {
        "text": "in situations such as this, you will have continuous\u2010\u00ad time sensor measurements."
    },
    {
        "text": "but you may also have additional physical specs about each machine, how long it has been running, the conditions under which it operates, and logs of maintenance that have been done on it."
    },
    {
        "text": "it will be your job to extract features out of all this data that can give you predictions for a particular point in time."
    },
    {
        "text": "so let\u2019s assume that you have data of these types, and your goal is to extract meaningful features for making predictions at a point in time t. what can you do?"
    },
    {
        "text": "most of this chapter has discussed how you can extract features from the time window immediately preceding t. these features are great to have, but they are fairly limited in their scope."
    },
    {
        "text": "really all they tell us is the state of the machine at time t. but the machine has a whole lifetime that we can look at when making a prediction."
    },
    {
        "text": "other features to consider would include the following: \u25cf \u25cfthe age of the machine."
    },
    {
        "text": "it doesn\u2019t get much simpler than that."
    },
    {
        "text": "\u25cf \u25cfhow many times repairs have been done in the machine\u2019s lifetime."
    },
    {
        "text": "\u25cf \u25cfhow many hours the machine has clocked in a particular state, as measured by doing clustering over all time windows in the training data."
    },
    {
        "text": "the number of hours that a machine has been working hard might be a good proxy for the wear\u2010and\u2010tear on its parts."
    },
    {
        "text": "as with all areas of data science, the key to using time series is extracting the right features."
    },
    {
        "text": "the key to extracting features is understanding what real\u2010world phenomena are relevant."
    },
    {
        "text": "17.13 \u00adfurther reading 1 oppenheim, a & schafer, r, digital signal processing, 1975, pearson, new york, ny."
    },
    {
        "text": "2 riley, k, hobson, m & bence, s, mathematical methods for physics and engineering: a comprehensive guide, 3rd edn, 2006, cambridge university press, cambridge, uk."
    },
    {
        "text": "17 time series analysis 260 17.14 \u00adglossary denoising \u00adremoving random noise from a time series."
    },
    {
        "text": "extrapolation using interpolation techniques to estimate a function at a point x that is higher than our highest\u2010known x or lower than the lowest\u2010 known."
    },
    {
        "text": "this is much more error prone than interpolating at an x that is within our known interval."
    },
    {
        "text": "fourier analysis looking at a time series signal as a linear combination of sinusoids with different frequencies."
    },
    {
        "text": "fourier transform the process of going from a raw signal to its fourier decomposition."
    },
    {
        "text": "interpolation using the known values of a function at several points to estimate its value at a location that we don\u2019t know."
    },
    {
        "text": "this is useful when resampling time series data."
    },
    {
        "text": "resampling taking a time series with irregularly spaced time stamps, or time stamps at a frequency we don\u2019t want, and estimating the time series at a desired sampling frequency."
    },
    {
        "text": "seasonal decomposition breaking a time series down as the sum of a periodic term, a smooth \u201ctrend\u201d term, and random noise."
    },
    {
        "text": "spline a method of interpolation where a cubic polynomial is fit to a small number of known points that are close together."
    },
    {
        "text": "that polynomial is used to give interpolated values near those points."
    },
    {
        "text": "sliding window moving a fixed\u2010length window across a time series and calculating some statistic for each window."
    },
    {
        "text": "window a contiguous subset of a time series signal."
    },
    {
        "text": "often, we break a time series into a collection of windows and extract features from each window."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 261 18 so far, this book has tacitly assumed that you understand basic probability, such as the notion of independence and what an average is."
    },
    {
        "text": "this chapter will go into more detail, giving you a little bit of theoretical background in the subject and an overview of the standard tools."
    },
    {
        "text": "in practice, data scientists only need a moderate amount of probability theory for most of their daily work, but that moderate amount is crucially important."
    },
    {
        "text": "probability provides the theoretical basis for almost all of machine learning and most of analytics, and it is a critical mindset for data scientists to be able to adopt."
    },
    {
        "text": "probability is often confused with statistics."
    },
    {
        "text": "the way i would break it down is to say that probability is a collection of techniques for describing the world using mathematical models that include randomness."
    },
    {
        "text": "in particular, probability focuses on what you can derive about the world assuming that it is well described by one of these models."
    },
    {
        "text": "for example, if we assume a certain distribu- tion of human heights, then how many people in a crowd can we expect to be over 5 ft tall?"
    },
    {
        "text": "statistics is more about working backward: given some real\u2010world data, what can we infer about the real\u2010world process (which we imagine to be some probability model) that generated it?"
    },
    {
        "text": "this chapter will attempt to build up the subject of probability in a very intui- tive way."
    },
    {
        "text": "i will start off by showing two of the simplest, most intuitive, and most important probability models."
    },
    {
        "text": "using these as motivation, i will then zoom out and give a more formal treatment of probability concepts."
    },
    {
        "text": "a certain amount of this will be material that we\u2019ve already covered in the book; i\u2019ll just be discuss- ing it in a more mathematical way."
    },
    {
        "text": "finally, i will move on to several of the most important probability distributions for you to know as a data scientist."
    },
    {
        "text": "18.1 \u00adflipping coins: bernoulli random variables the simplest probabilistic model is just flipping a (possibly biased) coin."
    },
    {
        "text": "let\u2019s say that the probability of getting a head is p, and hence, the probability of tails is 1\u2212p."
    },
    {
        "text": "in probability terms, we would say that the flip of such a coin is a probability"
    },
    {
        "text": "18 probability 262 \u201cbernoulli random variable\u201d or bernoulli rv."
    },
    {
        "text": "you might see it denoted as bernoulli(p)."
    },
    {
        "text": "if p is 0.7, then you can visualize the rv with a bar chart: 0 0.2 0.4 0.6 0.8 head tail the assignment of 0.7 to heads and 0.3 to tails is called the \u201cprobability mass function\u201d for this particular random variable."
    },
    {
        "text": "it becomes convenient to describe the rv in terms of numbers rather than sides of a coin."
    },
    {
        "text": "the convention here is to say that heads = 1 and tails = 0. in many cases, there is some kind of payout associated with different outcomes of the random variable."
    },
    {
        "text": "for example, i might give you $5 for every head and demand $2 from you for every tail."
    },
    {
        "text": "the average payout will then be e payout [ ] = + \u2212 ( ) = 0 7 5 0 3 2 2 9 ."
    },
    {
        "text": "* ."
    },
    {
        "text": "* ."
    },
    {
        "text": "the right way to interpret this number is that if you flip the coin n times, where n is some very large number, you will make about 2.9*n dollars."
    },
    {
        "text": "you can see immediately how a bernoulli random variable might generalize to something such as the roll of a dice, where the probability mass function would assign a probability to the numbers 0\u20135."
    },
    {
        "text": "in cases such as this, it is con- ventional to let pi denote the probability of the ith outcome."
    },
    {
        "text": "the only con- straints are that \u25cf \u25cfall the pi are nonnegative, and \u25cf \u25cfthey add up to 1.0. any set of the pi that meets these criteria is a valid probability mass function."
    },
    {
        "text": "a bernoulli random variable is called a \u201cdiscrete\u201d random variable."
    },
    {
        "text": "this means that either it has a finite number of outcomes or all of its possible out- comes can be listed out."
    },
    {
        "text": "so, a random variable that assigns a probability mass to every positive integer is still discrete."
    },
    {
        "text": "a random variable that measures human height (to a precision of arbitrarily many decimal places) is not discrete."
    },
    {
        "text": "18.3 the uniform distribution and pseudorandom numbers 263 18.2 \u00adthrowing darts: uniform random variables bernoulli random variables are the simplest type of discrete random variable."
    },
    {
        "text": "the opposite is what are called \u201ccontinuous\u201d random variables."
    },
    {
        "text": "they can take on any value within a range of numbers."
    },
    {
        "text": "the simplest continuous random variable is the uniform random variable, sometimes called uniform(a,b)."
    },
    {
        "text": "a uniform(a,b) will always be between the numbers a and b, but it is equally likely to be anywhere in that range."
    },
    {
        "text": "for discrete rvs, the probability mass function assigns a finite probability to every possible outcome."
    },
    {
        "text": "for continuous rvs, every exact outcome has proba- bility 0, but certain ranges of outcomes are much more likely than others."
    },
    {
        "text": "we call this relative likelihood the \u201cprobability density function\u201d (pdf)."
    },
    {
        "text": "the pdf for a uniform distribution appears as follows: a b 1 b \u2013 a similarly to probability mass functions, the constraints on a pdf f are that \u25cf \u25cff (x) is never negative, and \u25cf \u25cfthe total area under the curve of f is equal to 1.0. any function f that meets these criteria is a valid pdf."
    },
    {
        "text": "related to the pdf is the \u201ccumulative distribution function\u201d (cdf)."
    },
    {
        "text": "conventionally, if we use the lowercase f () to denote the pdf, we use the upper- case f() to denote the cdf."
    },
    {
        "text": "f(x) is the probability that a random variable\u2019s value will be \u2264x."
    },
    {
        "text": "so, f(x) is a nondecreasing function that goes to 0 as x approaches negative infinity and approaches 1.0 as x becomes large."
    },
    {
        "text": "in places where f (x) is large, f(x) will slope up sharply."
    },
    {
        "text": "in places where f (x) is 0, f(x) will be flat."
    },
    {
        "text": "the pdf tends to be a lot easier to think about and visualize."
    },
    {
        "text": "however, there are situations where it is easier to solve problems using the cdf."
    },
    {
        "text": "18.3 \u00adthe uniform distribution and pseudorandom numbers the uniform(0,1) distribution is the most fundamental probability distribu- tion to understand."
    },
    {
        "text": "it is the simplest one, but it is also the basis for building up"
    },
    {
        "text": "18 probability 264 many more complicated ones, both in mathematical theory and computational practice."
    },
    {
        "text": "for example: \u25cf \u25cfif you want to simulate a bernoulli(p) random variable b, then you can do it by simulating a random value u from a uniform(0,1) distribution."
    },
    {
        "text": "if u < p, then set b = heads."
    },
    {
        "text": "otherwise, set b = tails."
    },
    {
        "text": "\u25cf \u25cfif you want to simulate the roll of a weighted dice, divide the range [0.0, 1.0] up into six regions, the ith of which has size equal to the probability of the ith face."
    },
    {
        "text": "then again draw a value u from a uniform(0,1) distribution."
    },
    {
        "text": "have the dice roll be the region of [0.0, 1.0] into which u falls."
    },
    {
        "text": "\u25cf \u25cfif you want to simulate an exponential random variable (to be discussed later), draw u from a uniform(0,1)."
    },
    {
        "text": "then take \u22121 times log(u)."
    },
    {
        "text": "in general, say you know the cdf fx() of a random variable x. say also that you\u2019re able to compute the inverse of f u x \u2212( ) 1 . then f u x \u2212( ) 1 will be a sample of x if u is drawn from a uniform(0,1)."
    },
    {
        "text": "for these reasons, computational libraries that simulate random variables tend to start with sampling the uniform distribu- tion as their most fundamental operations and build everything up from there."
    },
    {
        "text": "now technically, it is not possible to simulate random numbers with a com- puter."
    },
    {
        "text": "they are deterministic machines, capable only of following predeter- mined rules \u2013 there is no subroutine for flipping a coin."
    },
    {
        "text": "so the standard practice is instead to use \u201cpseudorandom numbers.\u201d the idea is this: you start with an arbitrary sequence of bytes."
    },
    {
        "text": "the bits of that sequence are interpreted as the digits of a uniform(0,1) random number, expressed in binary out to a fixed number of decimal places."
    },
    {
        "text": "in some implementations, only part of the array is interpreted as a number."
    },
    {
        "text": "there is then a complicated (but deterministic!)"
    },
    {
        "text": "mathematical function that mangles the byte array into a new byte array."
    },
    {
        "text": "the new byte array is technically a deterministic function of the old one, but in practice, it bears no noticeable resemblance to the original array."
    },
    {
        "text": "flipping a single bit in the original byte array could change bits anywhere in the output."
    },
    {
        "text": "the new array is treated as a new uniform(0,1) variable, and so on."
    },
    {
        "text": "a lot of work has gone into creating pseudorandom numbers that accurately ape all the properties of true randomness."
    },
    {
        "text": "technically, each sample is a deter- ministic function of the sample before it, but there is no noticeable correlation between them."
    },
    {
        "text": "if you draw enough samples, they will occur equally often in all parts of the range [0.0, 1.0]."
    },
    {
        "text": "they will have the correct average, standard devia- tion, and so on."
    },
    {
        "text": "in short, if the pseudorandom samples had been given to us as a stream of raw data rather than something we had generated ourselves, we would never have figured out that they were anything other than independent samples from a uniform(0,1)."
    },
    {
        "text": "the one great thing about pseudorandom numbers is that you can manually set the initial byte array at the start of a program."
    },
    {
        "text": "in this case, it is called the \u201cseed.\u201d if you do this, then the program becomes fully deterministic, and you can reproduce it exactly between two runs."
    },
    {
        "text": "this means the following:"
    },
    {
        "text": "18.4 nondiscrete, noncontinuous random variables 265 \u25cf \u25cfif there is a bug in a randomized program that only occurs sometimes, you can make it perfectly reproducible and figure out what\u2019s going on."
    },
    {
        "text": "\u25cf \u25cfif you need your analytics results to be exactly reproducible because some- body will scrutinize them, you can set the seed in your scripts."
    },
    {
        "text": "\u25cf \u25cfwhen you are writing tests, you can set the seed and make sure that the output is exactly what\u2019s expected."
    },
    {
        "text": "\u25cf \u25cfoftentimes, you have two pieces of code that you need to make sure work identically (maybe a proof\u2010of\u2010concept and a production version)."
    },
    {
        "text": "the easiest way to do this is to make sure that they produce the same output, given the same input."
    },
    {
        "text": "this becomes impossible if the code includes calls to random numbers, unless you set both of them to have the same random seed."
    },
    {
        "text": "18.4 \u00adnondiscrete, noncontinuous random variables mathematically speaking, you can have random variables that are neither dis- crete nor continuous."
    },
    {
        "text": "for example, take the heights of trees that you have planted."
    },
    {
        "text": "at a given point in time, a certain fraction of them will not have sprouted and will have height 0. this is a finite probability mass at that number."
    },
    {
        "text": "but of those trees that have sprouted, their heights can fall anywhere within a range."
    },
    {
        "text": "in practice, this is not a big deal."
    },
    {
        "text": "you\u2019re not dealing with abstract probability distributions, but finite datasets; a hybrid distribution would show up just as there being multiple identical numbers in the otherwise continuous\u2010valued data."
    },
    {
        "text": "calculating the mean, average, median, or other metrics of interest would still be exactly the same procedure."
    },
    {
        "text": "the place you will run into a problem is with exploratory visualizations."
    },
    {
        "text": "if you do a histogram of heights with our tree example, you will see a massive spike at height = 0. the bell\u2010curve part of the histogram will be squashed down to the point where it\u2019s invisible."
    },
    {
        "text": "a better visualization includes two pictures: a pie chart showing how many heights are zero and how many are nonzero, plus a histogram of only the nonzero heights."
    },
    {
        "text": "the following script simulates some data like this and shows the two visualizations: import numpy as np import pandas as pd import matplotlib.pyplot as plt z = np.zeros(1000) x = np.random.exponential(size=1000) data = np.concatenate([z, x]) pd.series(data).hist(bins=100) plt.title(\"huge spike at zero\")"
    },
    {
        "text": "18 probability 266 but the pie chart/histogram combo gives us breakdown by equal/greater than zero none > 0 = 0 80 distribution when > 0 70 60 50 40 30 20 10 0 0 1 2 3 4 5 6 7 8 9 d = pd.series(data) x = pd.series(x) d.hist(bins=100) plt.title(\"huge spike at zero\") plt.show() (d>0).value_counts().rename( {true:'> 0', false:'= 0'}).plot(kind='pie') plt.title('breakdown by equal/greater than zero') plt.show() x.hist(bins=100) plt.title(\"distribution when > 0\") plt.show() the nai\u0308ve histogram looks as the following: 1200 huge spike at zero 1000 800 600 400 200 0 0 1 2 3 4 5 6 7 8 9"
    },
    {
        "text": "18.5 notation, expectations, and standard deviation 267 in line with the large spike in the first histogram, you can think of hybrid dis- tributions as having a pdf that has an \u201cinfinitely tall spike\u201d at one or more places, and the area under the nonspiked parts of the curve is less than 1. this isn\u2019t mathematically rigorous, but it is useful way to think about it."
    },
    {
        "text": "what is rigorous though is the cumulative distribution function: the cdf jumps up at every spike."
    },
    {
        "text": "18.5 \u00adnotation, expectations, and standard deviation now that you are familiar with some of the key concepts, let\u2019s dive into some of the standard notation and terms."
    },
    {
        "text": "as we\u2019ve already seen, a \u201crandom variable\u201d (rv) is any quantity that can turn out in several different ways."
    },
    {
        "text": "it is typical to use an uppercase letter such as x to refer to the random variable and a lowercase x to refer to a specific value that the variable took on."
    },
    {
        "text": "a single\u2010dimensional random variable is described by a probability mass function if it is discrete or a probability distribution function if it is continuous."
    },
    {
        "text": "you can also have a random variable that returns a random vector of d dimensions."
    },
    {
        "text": "the concepts of probability mass function and probability density generalize naturally."
    },
    {
        "text": "the only constraints on them are that they are always nonnegative, and the probabilities add up to 1.0 (or the area under the curve is 1.0 in the case of continuous rvs)."
    },
    {
        "text": "unless otherwise stated, i will typically tac- itly assume that rvs in this."
    },
    {
        "text": "if we write e x [ ] we mean the average value of the random variable x. the use of \u201ce\u201d in this case refers to \u201cexpectation value,\u201d which is another term for the mean or average."
    },
    {
        "text": "the expectation value is defined to be e x ip i i [ ] = \u2211 for discrete rvs and e x xf x x x [ ] = ( ) \u222b d for continuous rvs."
    },
    {
        "text": "it is common to denote the expectation value of a random variable x by \u03bcx."
    },
    {
        "text": "more generally, you can have a function g() of a random variable, and say e g x g x f x x x ( ) \uf8ee\uf8f0 \uf8f9\uf8fb= ( ) ( ) \u222b d"
    },
    {
        "text": "18 probability 268 and similarly for discrete variables."
    },
    {
        "text": "many key probability concepts can be defined in terms of expectation values of different functions."
    },
    {
        "text": "a key example of something being defined in terms of expectation values is the variance and standard deviation."
    },
    {
        "text": "the \u201cvariance\u201d of x is defined to be var x e x [ ] \u2212 \uf8ee\uf8f0 \uf8f9\uf8fb = \u03bcx 2 and the standard deviation is its square root \u03c3x x = [ ] var the standard deviation gives you, roughly speaking, a measure of how far x \u201ctypically\u201d is from \u03bcx."
    },
    {
        "text": "note that using e[g(x)] lets us use the same notation for discrete or continu- ous rvs, which can be quite convenient."
    },
    {
        "text": "18.6 \u00addependence, marginal and conditional probability oftentimes, you have two random variables x and y and want to consider their behavior together."
    },
    {
        "text": "does knowing something about one tell you something about the other?"
    },
    {
        "text": "for concreteness, let\u2019s assume that they are both discrete rvs, and let pxy denote the probability that x = x and y = y. the \u201cmarginal\u201d probability mass function of x is then pr x x p p x y xy = [ ] = = \u2211 and similarly pr y y p p y x xy = [ ] = = \u2211 these are the probability distributions that we get if we focus on one rv and ignore the other."
    },
    {
        "text": "on the flip side, if we know the value of x and want to infer something about y, then we care about the conditional probability of each y given that x = x: pr | | y y x x p p p y x x xy xg g = = [ ] = = = \u2211 conditional probabilities play a very central, explicit role in bayesian statistics."
    },
    {
        "text": "18.7 understanding the tails 269 it is common to want to know something such as the expectation value of y, given that x = x. we write this as e y x x | = [ ] we say that such statistics about y are \u201cconditioned on\u201d the value of x. the correlation of x and y is defined to be corr , x y e x y x y x y [ ] = \u2212 ( ) \u2212 ( ) \uf8ee\uf8f0 \uf8f9\uf8fb \u03bc \u03bc \u03c3 \u03c3 this is a measure of the degree to which there is a linear relationship between the rvs, of the form y = mx + b. x and y are \u201cindependent\u201d random variables if knowing either tells us noth- ing about the other."
    },
    {
        "text": "mathematically, this means that p p p xy x y = it\u2019s important to note that independence is an extremely strong criterion."
    },
    {
        "text": "it is much stronger than just saying that the correlation is zero."
    },
    {
        "text": "18.7 \u00adunderstanding the tails one of the most important things to understand about a probability distribu- tion is how \u201cheavy\u2010tailed\u201d it is."
    },
    {
        "text": "intuitively, this refers to how often you have extremely large values."
    },
    {
        "text": "human heights are a great example of something this is not heavy\u2010tailed: there is never a person who is over 10 feet tall."
    },
    {
        "text": "net worth, however, is extremely heavy\u2010tailed, since there is the occasional bill gates."
    },
    {
        "text": "heavy\u2010tailed distributions are extremely common in data science, especially web\u2010based applications."
    },
    {
        "text": "places that i\u2019ve seen them arise include web traffic (a few websites are much more popular than others) and bids in online auctions."
    },
    {
        "text": "heavy\u2010tailed distributions are important to be aware of because many of the usual things we do with probability distributions don\u2019t work (at least not in the same way) when things are heavy\u2010tailed."
    },
    {
        "text": "the average value of a heavy\u2010tailed distribution can be notoriously hard to estimate."
    },
    {
        "text": "if you have 100 random people in a room, the average net worth in the room could vary widely \u2013 there is a very realistic chance of having a multi- millionaire who will throw off the average net worth."
    },
    {
        "text": "to see how this works, the following script will simulate a sequence of sam- ples drawn from a pareto distribution, which is heavy\u2010tailed."
    },
    {
        "text": "it will then plot n on the x\u2010axis and the average of the first n samples on the y\u2010axis."
    },
    {
        "text": "you can see that over 1000 trials, the average does not converge to a nice sane average."
    },
    {
        "text": "instead, it occasionally jumps up when a really massive outlier is hit, then trails"
    },
    {
        "text": "18 probability 270 off as the subsequent samples are more modest."
    },
    {
        "text": "as time goes on, it takes larger and larger outliers to cause a large bounce."
    },
    {
        "text": "so, the jumps get rarer and rarer, but they do come."
    },
    {
        "text": "if you ran the simulation forever, the average would increase to infinity."
    },
    {
        "text": "import numpy as np import matplotlib.pyplot as plt np.random.seed(10) means = [] sum = 0.0 for i in range(1, 1000): sum += np.random.pareto(1) means.append(sum / i) plt.plot(means) plt.title(\"average of n samples\") plt.xlabel(\"n\") 7 average of n samples n 6 5 4 3 2 1 0 200 400 600 800 1000 there are a lot of heavy\u2010tailed distributions, and not all of them would dis- play an infinitely growing average."
    },
    {
        "text": "most real\u2010world ones will grow for a while and then eventually plateau at a finite mean."
    },
    {
        "text": "but it will take a very long time for this to happen."
    },
    {
        "text": "so calculating the average of a heavy\u2010tailed distribution is a dicey proposi- tion."
    },
    {
        "text": "but even once you have it, the average is useless for many applications."
    },
    {
        "text": "it is not a \u201ctypical\u201d value."
    },
    {
        "text": "most samples are well below the average value; it\u2019s just that the few outliers are so large they pull it up."
    },
    {
        "text": "18.8 binomial distribution 271 metrics such as the median and quantiles are robust to heavy tails."
    },
    {
        "text": "they have the same reasonable interpretations that they always did."
    },
    {
        "text": "however, in many applications, they aren\u2019t particularly useful because the large outliers are pre- cisely the values that you are interested in!"
    },
    {
        "text": "so, always keep in mind whether a process you are studying is liable to be heavy\u2010tailed."
    },
    {
        "text": "if it is, make sure that you model it with appropriate distributions and treat it with caution."
    },
    {
        "text": "18.8 \u00adbinomial distribution a binomial(n, p) distribution is the number of heads you get from tossing a coin n times, where each toss has an independent probability p of coming up heads."
    },
    {
        "text": "the key things to understand about it are as follows: \u25cf \u25cfit can take on any value from 0 to n. \u25cf \u25cfthe average value will be np, and the probability mass function will be peaked there."
    },
    {
        "text": "\u25cf \u25cfthe standard deviation goes as n . this means that as n grows larger, the probability distribution will be more and more strongly peaked around np."
    },
    {
        "text": "it is instructive to derive the precise formula for binomial(n, p)\u2019s proba- bility mass function."
    },
    {
        "text": "the key insight is that for any specific sequence of n flips that contains k heads, the probability of tossing that sequence is pk(1 \u2212 p)n\u2212k."
    },
    {
        "text": "this is because there are k tosses that must come up heads, each with probability p, and n \u2212 k tosses that must come up tails, each with probability 1 \u2212 p. so to calculate pk, we only need to ask how many such sequences there are."
    },
    {
        "text": "how many ways you can get k heads out of n tosses is a combinatorics ques- tion that lies outside the context of this book."
    },
    {
        "text": "to motivate it though, imagine that n is very large."
    },
    {
        "text": "then: \u25cf \u25cfif k = 0, there is only one possible sequence: n tails."
    },
    {
        "text": "\u25cf \u25cfsimilarly, if k = n, there is only one sequence: all heads."
    },
    {
        "text": "\u25cf \u25cfif k = 1, there will be n possible places it could be."
    },
    {
        "text": "the exact formula is denoted n k \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 and pronounced \u201cn choose k.\u201d it is equal to n k n k n k \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7= \u2212 ( ) !"
    },
    {
        "text": "! !"
    },
    {
        "text": "where x!"
    },
    {
        "text": "= x*(x \u2212 1)*(x \u2212 2)*...*3*2*1 is pronounced \u201cx factorial.\u201d to sample from a binomial distribution in numpy, you can write"
    },
    {
        "text": "18 probability 272 import numpy as np sample = np.random.binomial(200, 0.3) 18.9 \u00adpoisson distribution poisson distributions are used to model systems where there are many events that could happen, and all are independent of each other, but on average only a few of them will happen."
    },
    {
        "text": "a good example is how many people will visit a website on a given day; there are billions of people in the world who could visit, but on average perhaps only a few hundred will do it."
    },
    {
        "text": "imagine taking a binomial(n, p) distribution."
    },
    {
        "text": "set n very large, and set p small enough such that np = \u03bb where \u03bb is some fixed constant."
    },
    {
        "text": "in the limit of making n large and p small, but keeping \u03bb fixed, a binomial distribution will converge to a poisson distribution."
    },
    {
        "text": "the probability mass function is given by p e k k k = \u2212\u03bb \u03bb !"
    },
    {
        "text": "to sample from a poisson distribution in numpy, just write sample = np.random.poisson(200) 18.10 \u00adnormal distribution if there is one probability distribution to know, it is the normal distribution, also called the gaussian."
    },
    {
        "text": "it is the prototypical bell\u2010shaped curve, and its pdf is f x e x ( ) = \u2212 \u2212 ( ) 1 2 2 2 2 \u03c0\u03c3 \u03bc \u03c3 / where \u03bc is its average value and \u03c3 is the standard deviation."
    },
    {
        "text": "such a normal distribution is often called n(\u03bc, \u03c32)."
    },
    {
        "text": "the pdf is displayed in the next ."
    },
    {
        "text": "the most important practical property of the normal distribution is that its probability density is tightly clustered around the mean; it has very light tails, and major outliers are extremely unlikely."
    },
    {
        "text": "for this reason, it can be dangerous to naively fit a normal distribution to your data."
    },
    {
        "text": "in practice, it is common to identify and remove major outliers before fitting a normal distribution to the remaining data."
    },
    {
        "text": "18.11 multivariate gaussian 273 0.40 pdf of a normal (0, 1) 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00\u20132.0 \u20131.5 \u20130.5 0.0 0.5 1.5 2.0 1.0 \u20131.0 theoretically, the normal distribution is most famous because many distri- butions converge to it, if you sample from them enough times and average the results."
    },
    {
        "text": "this applies to the binomial distribution, poisson distribution and pretty much any other distribution you\u2019re likely to encounter (technically, any one for which the mean and standard deviation are finite)."
    },
    {
        "text": "this is captured in the \u201ccentral limit theorem,\u201d which states central limit theorem."
    },
    {
        "text": "let x we a random variable with finite mean \u03bc and standard deviation \u03c3. let x1, x2, ..., xn be a sequence of independent samples of x. then as n approaches infinity n n x n i 1 0 2 \u2211 \u2212 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7\u2192 ( ) \u03bc \u03c3 , 18.11 \u00admultivariate gaussian most of the distributions in this chapter are univariate and don\u2019t generalize in a noteworthy way to higher dimensions."
    },
    {
        "text": "the normal distribution is an excep- tion to that."
    },
    {
        "text": "a normal distribution can be defined for any number d of dimen- sions."
    },
    {
        "text": "the density function always resemble a \u201chill,\u201d which peaks at the distribution\u2019s mean and is ellipsoidal in shape."
    },
    {
        "text": "18 probability 274 in two dimensions, the following shapes give you some idea of what these ellipsoids can look like the following: note that the ellipsoids can stretch out in any direction; they don\u2019t have to stretch along one of the axes."
    },
    {
        "text": "the univariate normal distribution was parameterized by the mean \u03bc and the variance \u03c32, both of which are floating\u2010point numbers."
    },
    {
        "text": "for a multivariate gaussian, the parameters are a d\u2010dimensional vector \u03bc, and a d\u2010by\u2010d matrix \u03c3. \u03c3 is called the \u201ccovariance matrix,\u201d and its (i,j)th component will be cov[xi, xj], where xi and xj are the ith and jth components of the random variable."
    },
    {
        "text": "the pdf at a d\u2010dimensional point x is f x e k x x ( ) = ( ) \u2212 \u2212 \u2212 \u2212 ( ) \u2212 ( ) \u2032 \u2212 2 2 1 2 1 2 1 \u03c0 \u03bc \u03bc / / \u03c3 \u03c3 the multivariate gaussian has the same strengths and weaknesses as the univariate gaussian."
    },
    {
        "text": "it is mathematically convenient, and there are many theo- rems to the effect that other things will converge to it."
    },
    {
        "text": "on the other hand, it has very thin tails and hence does not allow for large outliers."
    },
    {
        "text": "18.12 \u00adexponential distribution the pdf of an exponentially distributed random variable looks like the following:"
    },
    {
        "text": "18.12 exponential distribution 275 1.0 pdf of an exponential(1) 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 0.5 1.0 1.5 2.0 exponentials show up in a lot of places, but they are most useful in modeling the time until some event occurs or the length of time between events."
    },
    {
        "text": "let\u2019s say, for example, that you have people walking into a store."
    },
    {
        "text": "every moment there is a small, fixed probability of somebody walking in, and every moment is independent of every other."
    },
    {
        "text": "in that case, the amount of time between events will be exponentially distributed."
    },
    {
        "text": "the exponential distribution is parameterized by its mean \u03b8, the average time between events."
    },
    {
        "text": "sometimes, you will instead see it parameterized by \u03bb = 1/\u03b8 \u2013 the average rate at which events occur."
    },
    {
        "text": "the pdf of the exponential distribution is f x e x x ( ) = \u2265 \uf8f1 \uf8f2\uf8f4 \uf8f3\uf8f4 \u2212 1 0 0 \u03b8 \u03b8 / if otherwise to sample from it in numpy, you can say np.random.exponential(10) in many applications, the key property of the exponential distribution is that it is \u201cmemoryless.\u201d no matter how long you have been waiting for an event to happen, the time you have left to wait still follows the same exponential distri- bution."
    },
    {
        "text": "an event will happen in the next moment \u2013 or not \u2013 independently of what other events happened previously."
    },
    {
        "text": "18 probability 276 the memoryless property of the exponential distribution is usually taken as the dividing line between heavy\u2010tailed distributions and those that are not heavy\u2010tailed."
    },
    {
        "text": "if we have already waited x time for an event to occur, do we expect to wait more or less time than when we first started?"
    },
    {
        "text": "for exponential random variables, it tells you nothing."
    },
    {
        "text": "in contrast, somebody who is 20 years old is likely to like 20 more years, but somebody who is 90 probably won\u2019t."
    },
    {
        "text": "hence, age is not heavy\u2010tailed."
    },
    {
        "text": "a random person on the street is unlikely to be a millionaire."
    },
    {
        "text": "but if you do happen to pick somebody who has at least $10 mil- lion, there\u2019s a decent chance that they have many millions more."
    },
    {
        "text": "hence, net worth is heavy\u2010tailed."
    },
    {
        "text": "18.13 \u00adlog-normal distribution my go\u2010to heavy\u2010tailed distribution is the log\u2010normal."
    },
    {
        "text": "it is straightforward to understand and simulate."
    },
    {
        "text": "plus, it always has a finite average and standard deviation, which is true of essentially all real\u2010world phenomena."
    },
    {
        "text": "its pdf looks like the following: 0.30 pdf of an lognormal(0, 1) 0.25 0.20 0.15 0.10 0.05 0.00 0 2 4 6 8 10 there is a well\u2010defined peak in the distribution, and the peak is at a location above 0. to the left of the peak, it falls off quickly, becoming 0.0 at x = 0. to the right though, it tapers off much more gradually, allowing for a regular occur- rence of large outliers."
    },
    {
        "text": "the log\u2010normal distribution is best thought of in this way: sample a value x from a normal(\u03bc, \u03c32)."
    },
    {
        "text": "then ex is log\u2010normally distributed."
    },
    {
        "text": "18.14 entropy 277 to sample from it in numpy, you can say np.random.lognormal(mu, sigma) 18.14 \u00adentropy entropy is a way to measure \u201chow random\u201d a random variable is, and it comes from the field of information theory."
    },
    {
        "text": "intuitively, a fair coin is more random than one that is heads 99% of the time."
    },
    {
        "text": "similarly, if a normal distribution has a tiny standard deviation, then its probability mass will be tightly grouped around its mean, and we want to think of it as being less random than one with a larger standard deviation."
    },
    {
        "text": "entropy is notoriously difficult to give a crisp explanation of, but i\u2019ll do my best."
    },
    {
        "text": "to start off with, let\u2019s assume that the random variable x is discrete."
    },
    {
        "text": "the key intuition is that some outcomes of a random variable are \u201cmore surprising\u201d than others, and entropy is e[surprise[x]]."
    },
    {
        "text": "the question then is how to quan- tify how surprising a certain outcome x = x is."
    },
    {
        "text": "the standard way to do it is that surprise x x px = [ ] = \u2212 ( ) ln in that case, the entropy becomes h x e x p p x x x [ ] \u2261 [ ] \uf8ee\uf8f0 \uf8f9\uf8fb= \u2212 ( ) \u2211 surprise ln if x is a continuous random variable, the definition is h x f x f x x [ ] = \u2212 ( ) ( ) ( ) \u222b ln d the choice of the logarithm as a measure of \u201csurprise\u201d might seem arbitrary, but it\u2019s not."
    },
    {
        "text": "intuitively, we would like the surprise to be a function of the prob- ability, so that surprise x x f px = [ ] = ( ) where f is some function."
    },
    {
        "text": "we would like this function to have three key proper- ties."
    },
    {
        "text": "the first two are very straightforward: \u25cf \u25cf f 1 0 0 ."
    },
    {
        "text": "( ) = \u25cf \u25cfas p \u21920 , then f p ( )\u2192\u221e the third property is a little bit more subtle."
    },
    {
        "text": "say there are two random vari- ables x and y that are independent of each other."
    },
    {
        "text": "in this case, the surprise from learning about one shouldn\u2019t affect our surprise from learning about the other."
    },
    {
        "text": "this means that"
    },
    {
        "text": "18 probability 278 \u25cf \u25cf f p p f p f p x x y y x x y y = = = = ( ) = ( )+ ( ) together, these constraints require that you use a logarithm."
    },
    {
        "text": "entropy is used in a lot of contexts."
    },
    {
        "text": "probably the most common is when we are picking a probability distribution to use for some purpose, and we want to pick one that reflects a large amount of ignorance about the situation because we don\u2019t know much about it."
    },
    {
        "text": "this will often give us a rigorous criterion for picking one distribution (or a family of distributions) over another."
    },
    {
        "text": "for example: \u25cf \u25cfif you need a discrete random variable defined over the number 1 to n, then the maximum entropy distribution will assign a probability of 1/n to each number."
    },
    {
        "text": "\u25cf \u25cfif you need a continuous random variable defined over the interval [a, b], then the maximum entropy distribution will be uniform(a, b)."
    },
    {
        "text": "whenever you are doing entropy maximization, constraints of some kind are necessary."
    },
    {
        "text": "if an rv was defined over all the real numbers, for example, you could make the entropy arbitrarily large by spreading the probability mass out very thinly over a very wide area."
    },
    {
        "text": "one final note on entropy: the definitions for discrete and continuous ran- dom variables are not equivalent."
    },
    {
        "text": "let\u2019s say that you tried to apply the dis- crete definition to a uniform(a, b) random variable."
    },
    {
        "text": "you might do so by picking a large number n, dividing [a, b] into n equally\u2010spaced intervals, and looking at the entropy of which interval it falls into."
    },
    {
        "text": "then the entropy you would get is h n n n n n i n i n = \u2212 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = ( ) = ( ) = = \u2211 \u2211 1 1 1 1 1 ln ln ln this expression blows up to infinity for large n!"
    },
    {
        "text": "the way to think about this is that for a continuous rv, you could never correctly guess the exact outcome of a sample, since you would have to be right to infinitely many decimal places."
    },
    {
        "text": "so in that sense, a continuous rv is always infinitely surprising."
    },
    {
        "text": "that\u2019s the rea- son why we use a different definition, one that reflects whether we can make a guess that is close to the right one."
    },
    {
        "text": "you rarely calculate entropy in the course of doing data science work."
    },
    {
        "text": "however, it is ubiquitous as soon as you start getting into the construction and theoretical properties of the tools."
    },
    {
        "text": "18.16 glossary 279 18.15 \u00adfurther reading 1 ross, s, introduction to probability models, 9th edn, 2006, academic press, waltham, ma."
    },
    {
        "text": "2 feller, w, an introduction to probability theory and its applications, vol."
    },
    {
        "text": "1, 3rd edn, 1968, wiley, hoboken, nj."
    },
    {
        "text": "18.16 \u00adglossary bernoulli random variable a random variable describing the flip of a coin that is heads with some probability p. binomial random variable a random variable that is the number of heads in n flips of a coin that is heads with probability p. central limit theorem a theorem describing how the average of many samples from a probability distribution is normally distributed around the distribution\u2019s mean."
    },
    {
        "text": "it applies so long as the distribution being sampled has finite mean and standard deviation."
    },
    {
        "text": "continuous random variable a random variable that takes on values in a continuous set, such as the real numbers."
    },
    {
        "text": "any single number has probability 0 of being the output."
    },
    {
        "text": "instead, the probability is described by a pdf."
    },
    {
        "text": "cumulative distribution function (cdf) if f() is the cdf of a random variable x, then f(x) = pr[x \u2264 x]."
    },
    {
        "text": "discrete random variable a random variable that takes on values in a discrete set, such as {heads, tails} or the integers."
    },
    {
        "text": "entropy a measure of how unpredictable a random variable is."
    },
    {
        "text": "for a discrete rv, it is h x p p i i i [ ] = \u2212 ( ) \u2211 ln ."
    },
    {
        "text": "for a continuous random variable, it is h x f x f x x [ ] = \u2212 ( ) ( ) ( ) \u222b ln d . these definitions are not equivalent; entropy is one of the few areas where continuous and discrete rvs require different treatments."
    },
    {
        "text": "expectation value the average value of a random variable or of a function of a random variable."
    },
    {
        "text": "exponential random variable a type of random variable, which takes on nonnegative numbers."
    },
    {
        "text": "its probability density peaks at 0 and falls off exponentially."
    },
    {
        "text": "it is often used to model the time between events in a random sequence of events."
    },
    {
        "text": "gaussian another name for the normal distribution."
    },
    {
        "text": "heavy\u2010tailed distribution a distribution with heavier tails than an exponential distribution."
    },
    {
        "text": "practically, this means that large outlier values are more frequent than with an exponential distribution."
    },
    {
        "text": "18 probability 280 log\u2010normal distribution a specific heavy\u2010tailed distribution."
    },
    {
        "text": "a sample from it is obtained by sampling from a normal distribution and then taking the exponent of that value."
    },
    {
        "text": "memoryless a property of the exponential distribution."
    },
    {
        "text": "if you know that x > x, then the distribution of x \u2212 x is the same as the original distribution of x. normal distribution the prototypical \u201cbell\u2010shaped curve.\u201d this is a probability distribution with a single clear peak, with distribution that is symmetric on each side of the peak and has very thin tails."
    },
    {
        "text": "poisson random variable a random variable defined over the nonnegative integers."
    },
    {
        "text": "it is used to model how many events happen, when infinitely many can happen, but in practice, only a few are expected, and those events are independent of each other."
    },
    {
        "text": "traffic to a website on a given day is often modeled as a poisson rv."
    },
    {
        "text": "probability density function (pdf) a function that describes the probability distribution of a continuous rv."
    },
    {
        "text": "the pdf for a random variable x must be nonnegative, and the total area under it must be 1.0. the area under only a part of the pdf is the probability that x occurs in that region."
    },
    {
        "text": "probability mass function the analog of the pdf for discrete random variables."
    },
    {
        "text": "it is a function that takes each possible outcome of the random variable and gives the probability of it occurring."
    },
    {
        "text": "pseudorandom numbers a sequence of numbers that are deterministically generated by a computer, but which in practice behave as if they were random."
    },
    {
        "text": "random variable a quantity that randomly takes on any of a number of possible values."
    },
    {
        "text": "standard deviation a measure of how far a random variable \u201ctypically\u201d is from its mean."
    },
    {
        "text": "it is defined as the square root of variance."
    },
    {
        "text": "uniform random variable a continuous random that only takes on values within a range [a, b]."
    },
    {
        "text": "within that range, however, the pdf is flat and each area is equally likely."
    },
    {
        "text": "variance var[x] = e[(x\u2212e[x])2]."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 281 19 i should start off with an explanatory note."
    },
    {
        "text": "a lot of data science really should be considered a subset of statistics."
    },
    {
        "text": "it is largely a matter of historical accident that statistics, data science, and machine learning are seen as different things."
    },
    {
        "text": "the disciplines have evolved largely independently, focusing on very different problems, so they have become different enough that i treat them as separate things in this book."
    },
    {
        "text": "most data scientists, most of the time, don\u2019t really need a thorough knowl- edge of statistics."
    },
    {
        "text": "there are some who live and breathe it, to be sure, but it\u2019s not nearly as useful for data science as one might expect."
    },
    {
        "text": "what\u2019s absolutely crucial, however, is the kind of critical thinking that one usually learns in a statistics class."
    },
    {
        "text": "statistics is all about being extremely, painstakingly careful and rigorous in how we analyze data and the assumptions we make."
    },
    {
        "text": "data science focuses more on how to extract features out of data, and there is usually enough data available that we don\u2019t need to be so exceedingly careful."
    },
    {
        "text": "but data scientists need to be sensitive to the luxury provided by having a lot of data and able to break out more rigorous methods when the data is lacking."
    },
    {
        "text": "this chapter will cover several of the key topics in statistics."
    },
    {
        "text": "in each case, it will focus on the key ideas, insights, and assumptions underlying each topic, rather than rigorous derivations of each formula."
    },
    {
        "text": "19.1 \u00adstatistics in perspective it might seem absurd that most data scientists don\u2019t need statistics."
    },
    {
        "text": "they obvi- ously use some statistical tools such as averages and fitting a line, but how can the mother discipline be demoted to a footnote?"
    },
    {
        "text": "the way i think about it is this: the discipline of statistics is about how to deal with constraints that data scientists don\u2019t usually need to worry about."
    },
    {
        "text": "the most important of these constraints is sample size."
    },
    {
        "text": "data science grew out of big data, where you are almost by definition swimming in data points."
    },
    {
        "text": "statistics"
    },
    {
        "text": "19 statistics 282 in situations such as web traffic, there are more data points than you even need and more features than you know what to do with."
    },
    {
        "text": "your task is to figure out the right way to parse it."
    },
    {
        "text": "once you\u2019ve done the leg work of pulling features out of the data, extracting a business insight can be as simple as looking at a histogram."
    },
    {
        "text": "but this isn\u2019t always the case."
    },
    {
        "text": "if you are testing whether a fertilizer works on crops, every data point will require a significant piece of land to be set aside to experiment on and then a year for the crop cycle."
    },
    {
        "text": "if you are testing whether a medical procedure works, every data point you gather is literally a life\u2010and\u2010 death proposition."
    },
    {
        "text": "statistics is about dealing with these extremely constrained situations, where it is very hard to tell whether a pattern we observe is a truth of the world or a fluke of our data."
    },
    {
        "text": "billions of dollars and potentially even human lives are on the line, so statistics tends to be appropriately nitpicky about every little detail."
    },
    {
        "text": "again, data scientists typically have so much data that they can usually afford to be cavalier."
    },
    {
        "text": "but they can\u2019t always be, and one of the most serious crimes a data scientist can commit is to be fast\u2010and\u2010loose in a situation that demands a more rigorous approach."
    },
    {
        "text": "the most important thing for you to get out of this chapter is a sensitivity to when quick\u2010and\u2010dirty approaches break down."
    },
    {
        "text": "if you can learn to spot those pitfalls before they happen, you can always learn the statistics you need on the fly."
    },
    {
        "text": "19.2 \u00adbayesian versus frequentist: practical tradeoffs and differing philosophies you\u2019ve probably heard of the great divide in statistics between bayesian statis- tics and frequentist (aka classical) statistics."
    },
    {
        "text": "before getting into the details of the differences, i should let you know that the debate is more philosophical than practical; in most problems, they will give close to the same answers."
    },
    {
        "text": "bayesian and frequentist statistics both take in data and then use it to con- struct a statistical model of the world (such as a normal distribution)."
    },
    {
        "text": "the dif- ference is in the relationship between the available data and the models we construct."
    },
    {
        "text": "in classical statistics, the model should be a \u201cbest fit\u201d to the data."
    },
    {
        "text": "we are obligated to make some assumptions about the form of the model (such as having it be a normal distribution) in order to solve problems, but otherwise, we set the model parameters so as to best fit the data."
    },
    {
        "text": "the overriding paradigm in classical statistics is to ask how plausible our data is given a particular model of the world or particular set of parameters and set our parameters so that the data becomes as plausible as possible."
    },
    {
        "text": "if we must make statistical predictions, we do so using this best\u2010fit model."
    },
    {
        "text": "19.3 hypothesis testing: key idea and example 283 in bayesian statistics, there is an additional layer of complexity."
    },
    {
        "text": "we don\u2019t just have the best\u2010fit parameters for the real\u2010world model; we have a confidence distribution over what those best\u2010fit parameters might be."
    },
    {
        "text": "this confidence distribution (which is mathematically equivalent to a probability distribution) isn\u2019t a rigorous best fit to any data: it represents our fallible human belief about what the \u201creal\u201d probability distribution might be."
    },
    {
        "text": "a bayesian model starts off with what\u2019s called a \u201cprior,\u201d which exists without any data."
    },
    {
        "text": "a prior is the initial confidence distribution over possible models of the world."
    },
    {
        "text": "as data comes in, we refine the confidence distribution, hopefully zeroing in on the \u201creal\u201d param- eters that characterize the world as it actually is."
    },
    {
        "text": "if we want to actually make predictions using a bayesian model, we must average our predictions over all the possible values of a model, weighting them by our confidence."
    },
    {
        "text": "with any luck, training a bayesian model will zero in on real\u2010world parame- ters that are pretty close to the best\u2010fit parameters of frequentist statistics, and a model with those parameters will be a pretty good model of the world."
    },
    {
        "text": "classical and bayesian statistics both have their place."
    },
    {
        "text": "bayesian is especially useful in situations where there is expert knowledge available that can be wrapped into a prior, or we have a lot of missing data."
    },
    {
        "text": "classical statistics is often much easier to compute and make use of."
    },
    {
        "text": "this chapter will start with classical techniques and then move on to bayesian statistics at the end."
    },
    {
        "text": "19.3 \u00adhypothesis testing: key idea and example an important domain of statistics is called hypothesis testing."
    },
    {
        "text": "the basic idea is that you think that there is a trend in your data, and you want to assess how likely it is to be a real\u2010world phenomenon versus just a fluke."
    },
    {
        "text": "hypothesis test- ing doesn\u2019t address the question of how strong a trend is; it\u2019s designed for situ- ations where the data is too sparse to quantify trends reliably."
    },
    {
        "text": "a prototypical example for hypothesis testing is this: i have a coin that might or might not be fair."
    },
    {
        "text": "i throw it 10 times and get 9 heads."
    },
    {
        "text": "how likely is it that this coin is loaded?"
    },
    {
        "text": "the first thing to realize is that we can\u2019t answer the question \u201chow likely is this coin to be loaded?\u201d in order to get that number, we would need some a priori knowledge or guess about how likely the coin is to be loaded, plus how loaded it would actually be, and there\u2019s no principled way to acquire that."
    },
    {
        "text": "so, here is the first big idea in classical statistics: instead of saying how likely the dice is to be loaded given the data, we ask how likely the data is given that the coin is not loaded."
    },
    {
        "text": "basically, is it plausible that nine heads is just a fluke?"
    },
    {
        "text": "we assume a fair coin, compute how likely this skewed data is, and use that to gauge whether the fair coin is believable."
    },
    {
        "text": "so, let\u2019s assume a fair coin."
    },
    {
        "text": "every coin flip can be either heads or tails, and they are all independent."
    },
    {
        "text": "this means that there are 210 = 1024 possible ways"
    },
    {
        "text": "19 statistics 284 that the sequence of 10 flips could occur, and they\u2019re all equally likely."
    },
    {
        "text": "there are four ways we could get data that is as skewed (or more skewed) as what we observed: \u25cf \u25cfwe could get 10 heads."
    },
    {
        "text": "there is only one flip sequence that does this."
    },
    {
        "text": "\u25cf \u25cfwe could get 9 heads."
    },
    {
        "text": "the tail could be in any of the flips, so there are 10 ways to get 9 heads."
    },
    {
        "text": "\u25cf \u25cfwe could get 10 tails."
    },
    {
        "text": "there is only one flip sequence for this."
    },
    {
        "text": "\u25cf \u25cfwe could get 9 tails."
    },
    {
        "text": "there are 10 ways to get this."
    },
    {
        "text": "adding all of these up, there are 22 ways to get data that is as skewed as what we saw."
    },
    {
        "text": "then we have 22/1024 = 0.0215, so there is only a 2% chance of getting data such as this from a fair coin."
    },
    {
        "text": "personally, i don\u2019t trust the coin."
    },
    {
        "text": "that\u2019s the basic procedure, and i encourage you to keep it firmly in mind."
    },
    {
        "text": "now let\u2019s frame it in more general statistical parlance."
    },
    {
        "text": "what we have done is called \u201chypothesis testing\u201d: you have found a pattern in the data, and you quantify your confidence in it by calculating how likely that pattern is to be just a fluke."
    },
    {
        "text": "the idea of a fair coin is called the \u201cnull hypothesis\u201d."
    },
    {
        "text": "typically, the null hypothesis means that there is no pattern in the real world: the fertilizer doesn\u2019t do anything for crops, the medicine doesn\u2019t help or hurt patients, the coin is fair, and the dice is not loaded."
    },
    {
        "text": "framing the null hypothesis can be complicated for some problems, but it\u2019s dead simple in this case."
    },
    {
        "text": "then there is some \u201ctest statistic\u201d: a single number that we calculate from our data that quantifies the pattern we are looking for."
    },
    {
        "text": "the key thing about the test statistic is that we can calculate its probability distribution if we assume the null hypothesis."
    },
    {
        "text": "in this case, the test statistic is just the number of heads."
    },
    {
        "text": "in general though, picking a good test statistic can be a tricky problem."
    },
    {
        "text": "finally, there is the likelihood of the test statistic being as extreme as we have observed it being."
    },
    {
        "text": "basically, the likelihood of seeing a pattern that is this extreme in the data if we take the null hypothesis as a given."
    },
    {
        "text": "this number is called the \u201cp\u2010value.\u201d a result is called \u201cstatistically significant\u201d if the p\u2010value is below some specified threshold, and there is a widespread convention of using 0.05 as the cutoff."
    },
    {
        "text": "i can\u2019t emphasize enough though that this is arbitrary, and we\u2019re really dealing with a sliding scale: a p\u2010value of 0.01 is much more meaningful than 0.04, and conversely p\u2010value of 0.07 could still be very important."
    },
    {
        "text": "the most important caveat to hypothesis testing is that it tells us that a pat- tern exists, but does not tell us how strong the pattern actually is."
    },
    {
        "text": "it\u2019s designed for situations where there is so little data that this knowledge is all we can hope for."
    },
    {
        "text": "in the real world, the null hypothesis is rarely 100% true; the weight of abraham lincoln\u2019s copper nose will technically introduce a tiny bias in a coin flip, and if you toss the coin a million times, you will see it."
    },
    {
        "text": "i like to say that in big data situations, if your pattern is weak enough that you even have to ask the p\u2010value, then it is certainly too weak to be of any business value."
    },
    {
        "text": "19.4 multiple hypothesis testing 285 the other important caveat is that you will get false positives."
    },
    {
        "text": "say you do many hypothesis tests in your life, and you always use 5% as your threshold."
    },
    {
        "text": "then of the times when there really is no pattern, 1 in 20 of those times you will find something statistically significant."
    },
    {
        "text": "this phenomenon is a huge problem is the scientific literature, where researchers will often declare victory as soon as the p\u2010value dips low enough to publish a paper."
    },
    {
        "text": "19.4 \u00admultiple hypothesis testing in real situations, we often have several hypotheses that we want to test and see if any one of them is correct."
    },
    {
        "text": "for example, we might test 20 different ads out on different groups of users and see if any one of them increases the click\u2010through rate (relative to a known baseline click\u2010through probability) with confidence threshold 0.05. that is, we will calculate the p\u2010value for each ad and consider all those with p\u2010value < 0.05 to pass our test."
    },
    {
        "text": "the problem is that while each individual ad has only a 5% chance of passing our test by dumb luck, there is a very good chance that some ad will pass by dumb luck."
    },
    {
        "text": "so, the idea is that we must tighten p\u2010value constraints for the individual ads, in order to have a p\u2010value of 0.05 for the overall test."
    },
    {
        "text": "a standard, conservative, and surprisingly simple solution to this problem is called the bonferroni correction."
    },
    {
        "text": "say you have n different ads, and you want a p\u2010value of \u03b1 for your overall test."
    },
    {
        "text": "then you require that an individual ad passes with the smaller p\u2010value of \u03b1/n."
    },
    {
        "text": "we then see that if the null hypothesis is true (i.e., no ad is actually an improvement), then our probability of errantly having a test pass is pr pr some ad passes every ad fails [ ] = \u2212 [ ] = \u2212 \u2212 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = \u2212 \u2212 1 1 1 1 1 \u03b1 n n n n \u03b1 \u03b1 +{ } \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u2248 higher order terms the bonferroni correction is the most common multiple hypothesis correc- tion that you will see, but you should be aware that it is unnaturally stringent."
    },
    {
        "text": "in particular, it assumes that every test you run is independent of every other test you run: if test x does not pass, then test y is still as likely to pass as it ever was."
    },
    {
        "text": "but that often isn\u2019t the case, especially if your different tests are related in some way."
    },
    {
        "text": "for example, say that you have a database of people\u2019s height, weight, and income, and you are trying to see whether there is a statistically significant correlation between their income and some other feature."
    },
    {
        "text": "height and weight"
    },
    {
        "text": "19 statistics 286 are strongly correlated between people, so if the tall people aren\u2019t richer, then the heavy people won\u2019t be either: they are more or less the same people!"
    },
    {
        "text": "intuitively, you want to say that we are testing 1.3 hypotheses or something similar, rather than two."
    },
    {
        "text": "bonferroni would apply if you tested height on one sample of people and weight on an independent set."
    },
    {
        "text": "there are other, compli- cated ways to adjust for situations such as this, but data scientists are unlikely to need anything beyond bonferroni corrections."
    },
    {
        "text": "19.5 \u00adparameter estimation hypothesis testing is, as i mentioned earlier, about measuring whether effects are present, but not about quantifying their magnitude."
    },
    {
        "text": "the latter falls under the umbrella of \u201cparameter estimation,\u201d where we try to estimate the underly- ing parameters that characterize distributions."
    },
    {
        "text": "in parameter estimation, we assume that the data follows some functional form, such as a normal distribution with mean \u03bc and standard deviation \u03b1. we then have some method to estimate these parameters, given the data, as follows: \u03bc \u03bc = = \u2212 ( ) = = \u2211 \u2211 1 1 1 1 2 n x n x i n i i n i \u03c3 in this case, these numbers are called the sample mean and the sample variance."
    },
    {
        "text": "to step back and cast this in statistical terminology, \u03bc and \u03c3 are both test statistics that we calculated from the data."
    },
    {
        "text": "if we threw out our dataset and gathered a new one of the same size from the real\u2010world distribution, \u03bc and \u03c3 would be a little bit different."
    },
    {
        "text": "they have their own probability distributions and are themselves random variables."
    },
    {
        "text": "the question then is how well we can take these random variables as indicators of the actual \u03bc and \u03c3. we will talk about the more general problem of confidence intervals later, but for now, there are two pieces of terminology you should know whenever you are talking about estimators: \u25cf \u25cfconsistency: an estimator \u03bc is \u201cconsistent\u201d if, in the limit of having many different points in your dataset, it converges to the real \u03bc ."
    },
    {
        "text": "\u25cf \u25cfbias: as estimator is \u201cunbiased\u201d if the expectation value of \u03bc is the real \u03b1. most estimators you might cook up on your own are probably going to be consistent."
    },
    {
        "text": "the \u03bc and \u03c3 are both consistent."
    },
    {
        "text": "however, only \u03bc is unbiased; \u03c3 is on average an underestimate of the true standard deviation."
    },
    {
        "text": "19.6 hypothesis testing: t-test 287 this surprised me the first time i learned it, so let me explain."
    },
    {
        "text": "it\u2019s easiest if you imagine that there are only two data points that we use to find the mean and standard deviation."
    },
    {
        "text": "then \u03bc will be located exactly in\u2010between them, at the place that minimizes i n ix =\u2211 \u2212 ( ) 1 2 \u03bc . if we had set \u03bc to be anywhere else, then i n ix =\u2211 \u2212 ( ) 1 2 \u03bc would have been somewhat larger."
    },
    {
        "text": "however, the real \u03bc is some- where else, because our \u03bc is only an estimate of \u03bc . it is an unbiased estimate, so \u03bc is equally likely to be more or less than \u03bc , but in either case i n ix =\u2211 \u2212 ( ) 1 2 \u03bc would get larger."
    },
    {
        "text": "basically, we are making \u03bc an overly good fit for our data, so on average, the sample deviation will be less than the real\u2010world deviation."
    },
    {
        "text": "an unbiased estimator of the standard deviation is \u03c3 = \u2212 \u2212 ( ) =\u2211 1 1 1 2 n x i n i \u03bc this will always be somewhat larger than our original expression, but they converge to the same number as n goes to infinity."
    },
    {
        "text": "19.6 \u00adhypothesis testing: t-test the t\u2010test is a more complicated version of hypothesis testing."
    },
    {
        "text": "it is useful for situations where the thing you measure is a continuous number, rather than a binary coin flip, and you want to assess whether the real\u2010world averages of two distributions are the same."
    },
    {
        "text": "for example, you might have cholesterol measure- ments for patients who did and did not take a cholesterol\u2010lowering medication."
    },
    {
        "text": "can we say confidently that the medicine works?"
    },
    {
        "text": "intuitively, this is a simple problem: draw histograms of each distribution and look at them."
    },
    {
        "text": "if their bell curves are nice and distinct, then the distribu- tions are clearly different."
    },
    {
        "text": "if the bell curves overlap a lot, then either the means are very close"
    },
    {
        "text": "19 statistics 288 or the spreads are very wide do this with enough data points and you get to the point where you can eas- ily conclude that there is a difference."
    },
    {
        "text": "reducing this intuition to a hypothesis test where we can rigorously calculate a p\u2010value is much, much trickier."
    },
    {
        "text": "it is also much more complicated than the coin\u2010flipping p\u2010value i discussed earlier."
    },
    {
        "text": "so before i give you an idea of how you approach the problem mathematically, i\u2019ll show you how to compute it in python: it\u2019s very easy."
    },
    {
        "text": "you have two datasets, and the hypothesis that you are testing is that their means are equal."
    },
    {
        "text": "you do not know the standard deviations."
    },
    {
        "text": "the math and computation is a lot easier if you assume that the standard devia- tions are equal, and that assumption is often made by default, but you can also allow them to be different."
    },
    {
        "text": "scipy\u2019s t\u2010test method is called ttest_ind."
    },
    {
        "text": "it produces two numbers: the t\u2010score (which i will explain in a minute) and the p\u2010value we are seeking: >>> from scipy.stats import ttest_ind >>> t_score, p_value = ttest_ind([1,2,3,4], [2,2.2,3,5]) >>> t_score, p_value = ttest_ind( [1,2,3,4], [2,2.2,3,5], equal_var=false) now i will give you a brief overview of what\u2019s going on under the hood here."
    },
    {
        "text": "recall the hypothesis testing process from earlier: formulate the null hypoth- esis, choose a test statistic that captures the strength of a pattern we have found, and then calculate how much of an outlier that test statistic is."
    },
    {
        "text": "if the test statistic is extremely high, then the null hypothesis is probably bogus."
    },
    {
        "text": "the t\u2010test gives us two choices of a null hypothesis: \u25cf \u25cfthe two datasets come from the same normal distribution."
    },
    {
        "text": "\u25cf \u25cfthe two datasets from normal distributions with the same means, although the standard deviations could be different."
    },
    {
        "text": "an important point is that neither of these hypotheses actually gives us a probability distribution."
    },
    {
        "text": "this is in contrast to coin flips, where assuming a fair coin tells you everything there is to know."
    },
    {
        "text": "in a t\u2010test, there are certain things we might want to know that we cannot calculate, because they depend on the parameters that we have not specified."
    },
    {
        "text": "19.6 hypothesis testing: t-test 289 when formulating the test statistic, the intuition to follow is this: \u25cf \u25cfcalculate the means \u03bc a and \u03bc b of the two datasets and look at how far apart they are."
    },
    {
        "text": "\u25cf \u25cfif there are more points in our datasets, we expect \u03bc a and \u03bc b to be closer to the true \u03bca and \u03bcb ."
    },
    {
        "text": "in that case, the difference between \u03bc a and \u03bc b should be small if the null hypothesis is true."
    },
    {
        "text": "\u25cf \u25cfif the datasets themselves have a lot of variance, \u03bc a and \u03bc b will be worse approximations to the true \u03bca and \u03bcb ."
    },
    {
        "text": "so, the difference between \u03bc a and \u03bc b can be larger."
    },
    {
        "text": "the following test statistic (called the t\u2010statistic) captures the following intuition: t s n s n a b a a b b = \u2212 + \u03bc \u03bc 2 2 where sa 2 is the sample variance of a and similarly for sb 2 . if you run the math, you will find that the distribution of t will be the same no matter the unknown mean of the underlying distribution and no matter the unknown variances."
    },
    {
        "text": "there is a similar, simpler test statistic that we can use if the population vari- ances are assumed to be equal be unknown."
    },
    {
        "text": "if the null hypothesis is true, then t will follow what\u2019s called a t\u2010distribution."
    },
    {
        "text": "a t\u2010distribution looks very much as a normal distribution with mean 0 and standard deviation 1, but it has somewhat heavier tails."
    },
    {
        "text": "if the t\u2010statistic for our data is unusually large or small, then the null hypothesis is probably not correct."
    },
    {
        "text": "the t\u2010test is used to test whether the means of two distributions are the same, but it assumes that the underlying probability distributions are normally distributed."
    },
    {
        "text": "you could formulate an equivalent test assuming different distri- butions if need be, although i\u2019ve personally never seen it done."
    },
    {
        "text": "a failure of the t\u2010test doesn\u2019t necessarily tell you anything about the means; it could also be that your basic assumptions are flawed."
    },
    {
        "text": "if you want to see whether your data is indeed normally distributed, there is (surprise surprise) a hypothesis test for that: several of them, in fact."
    },
    {
        "text": "one of them based on a metric called the z\u2010score can be used as follows: >>> from scipy.stats import normaltest >>> z_score, p_value = normaltest(my_array) the z score works by looking at how heavy the tails of the data are and com- paring them to the standard deviation of the data around its mean."
    },
    {
        "text": "19 statistics 290 19.7 \u00adconfidence intervals when we are trying to estimate a parameter of a real\u2010world distribution from data, it is often important to give confidence intervals, rather than just a single best\u2010fit number."
    },
    {
        "text": "as i did with the t\u2010test, i will show how to use some pre- canned libraries, the most common use case."
    },
    {
        "text": "i will then back up and discuss what we just did in a more abstract, general way that you can apply to novel problems."
    },
    {
        "text": "the most common use for confidence intervals is in calculating the mean of the underlying distribution."
    },
    {
        "text": "if you want to calculate something else, you can often massage your data to turn it into a calculation of the mean."
    },
    {
        "text": "for example, let\u2019s say you want to estimate the standard deviation of a random variable x. well, in that case, we can see that \u03c3 \u03c3 x x e x e x e x e x = \u2212[ ] ( ) \uf8ee \uf8f0\uf8ef \uf8f9 \uf8fb\uf8fa = \u2212[ ] ( ) \uf8ee \uf8f0\uf8ef \uf8f9 \uf8fb\uf8fa 2 2 2 so we can estimate the standard deviation of x by finding the mean of x e x \u2212[ ] ( ) 2 and taking the square root."
    },
    {
        "text": "if we can put confidence intervals on taking a mean, we can put confidence intervals on a lot of other things we might want to estimate."
    },
    {
        "text": "the typical metric to use is the \u201cstandard error of the mean\u201d or sem."
    },
    {
        "text": "if you see a mean reported as 4.1 \u00b1 0.2, then generally 4.1 is the best\u2010fit mean that you get by averaging all of the numbers, and 0.2 is the sem."
    },
    {
        "text": "you can calculate the sem in python as follows: >>> from scipy.stats import sem >>> std_err = sem(my_array) if you assume that the underlying distribution is normal, then the sem will be the standard deviation of the estimate of the mean, and it has a lot of nice mathematical properties."
    },
    {
        "text": "the most notable is that if \u03bc is your sample mean, then the interval \u03bc \u03bc \u2212 + \uf8ee\uf8f0 \uf8f9\uf8fb z z * , * sem sem will contain the mean 95% of the time, 99% of the time, or any other confi- dence threshold you want, depending on how you set the coefficient z. increase that coefficient and you can increase your confidence."
    },
    {
        "text": "the following table"
    },
    {
        "text": "19.8 bayesian statistics 291 shows several typical confidences that people want and the corresponding coefficients: confidence (%) coefficient 99 2.58 95 1.96 90 1.64 it is very tempting to say that there is a \u201c95% chance that the mean is within our interval,\u201d but that\u2019s a little dicey."
    },
    {
        "text": "the real\u2010world mean is within your inter- val or it isn\u2019t \u2013 you just happen not to know which one."
    },
    {
        "text": "the more rigorous interpretation is to say that if the distribution is normally distributed (no mat- ter its mean and standard deviation), and you take a sample of n points and calculate the confidence interval from them, then there is a 95% chance that the interval will contain the actual mean."
    },
    {
        "text": "the randomness is in the data you sample, not in the fixed parameters of the underlying distribution."
    },
    {
        "text": "of course, all of this hinges on the assumption that the real\u2010world distribu- tions are normal."
    },
    {
        "text": "all of the theorems about how to interpret the confidence intervals go out the window when you let go of that assumption, but in prac- tice, things still often work out fine."
    },
    {
        "text": "statisticians have worked out alternative formulas for the intervals that give the same sorts of guarantees for other types of distributions, but you don\u2019t see them often in data science."
    },
    {
        "text": "the sem confidence interval is based on the idea that we want to make sure that the true value of the parameter is contained in the interval some known percentage of the time."
    },
    {
        "text": "another paradigm that you see is that we want the interval to contain all \u201cplausible\u201d values for the parameter."
    },
    {
        "text": "here \u201cplausible\u201d is defined in terms of hypothesis testing: if you hypothesize that your parameter has some value and calculate an appropriate test statistic from the data, do you get a large p\u2010value?"
    },
    {
        "text": "19.8 \u00adbayesian statistics bayesian statistics, similar to classical statistics, assumes that some aspect of the world follows a statistical model with some parameters: similar to a coin that is heads with probability p. classical statistics picks the value of the param- eter that best fits the data (and maybe adds in a confidence interval); there is no room for human input."
    },
    {
        "text": "in bayesian statistics though, we start off with a prob- ability distribution over all possible values of the parameter that represents our confidence that each value is the \u201cright\u201d one."
    },
    {
        "text": "we then update our confidence as every new data point becomes available."
    },
    {
        "text": "in a sense, bayesian statistics is the science of how to refine our guesses in light of data."
    },
    {
        "text": "19 statistics 292 the mathematical basis for bayesian statistics is bayes\u2019 theorem, which looks innocuous enough."
    },
    {
        "text": "for any random variables x and t (which may be vectors, binary variables, or even something very complex), it says that p t d p t p d t p d | | ( ) = ( ) ( ) ( ) as stated, bayes\u2019 theorem is just a simple probability theorem that is true of any random variables t and d. however, it becomes extremely powerful when we have t be a real\u2010world parameter that we are trying to guess from the data."
    },
    {
        "text": "in this case, t isn\u2019t actually random \u2013 it is fixed but unknown, and the \u201cproba- bility distribution\u201d of t is a measure of our own confidence, defined over all the values t, could possibly take."
    },
    {
        "text": "the left\u2010hand side of bayes\u2019 theorem gives us what we were unable to have in classical statistics: the \u201cprobability\u201d that the real\u2010world parameter is equal to a certain value, given the data we have gathered."
    },
    {
        "text": "on the right\u2010hand side, we see that it requires the following: \u25cf \u25cfp(t): our confidence about the parameter before we got the data \u25cf \u25cfp(d|t): how likely the data we saw was assuming that the parameter had a particular value."
    },
    {
        "text": "this often is known or can be modeled well."
    },
    {
        "text": "\u25cf \u25cfp(d): the probability of seeing the data we saw, averaged over all possible values the parameter could have had."
    },
    {
        "text": "to take a concrete example, say that we have a user who may be a male or female, but we don\u2019t know which."
    },
    {
        "text": "so, we set our prior guess to say there is a 50% confidence they are male and 50% that they are female."
    },
    {
        "text": "then say, we learn that they have long hair, so we update our confidence to p p p p female|longhair female longhair|female longhair ( ) = ( ) ( ) ( ) = 1 2 1 2 1 2 p p p longhair|female longhair|female longhair|fema ( ) ( )+ le longhair|female longhair|female longhair|femal ( ) = ( ) ( )+ p p p e ( ) personally, i sometimes have trouble remembering bayes\u2019 theorem: the for- mula for the updated probability is just a little clunky."
    },
    {
        "text": "for me, it\u2019s much easier to think about the odds, that is, the relative probability of female or male."
    },
    {
        "text": "to update the odds, you just multiply by the relative probability of long hair: p p p p p female|longhair male|longhair female male longh ( ) ( ) = ( ) ( ) * air|female longhair|male ( ) ( ) p"
    },
    {
        "text": "19.10 bayesian networks 293 19.9 \u00adnaive bayesian statistics the trickiest part of bayesian statistics is usually calculating p(d|t)."
    },
    {
        "text": "this is chiefly because d is usually a multipart random variable in itself \u2013 typically, a d\u2010dimensional vector \u2013 and the different numbers in it can have very compli- cated dependency structures."
    },
    {
        "text": "for example, if we want to know whether some- body has diabetes, we might measure their blood glucose and sugar levels, and those two numbers have intricate dependencies that require a deep under- standing of biology to model."
    },
    {
        "text": "or, alternatively, if your model has to learn the relationships automatically, it requires a monumental amount of training data, since all co\u2010occurrences have to be seen."
    },
    {
        "text": "for this reason, you will often see people use a \u201cnaive bayes\u201d approach."
    },
    {
        "text": "in naive bayes, we simply assume that all the different variables are independent of each other, conditioned on t. that is, p d t p d t p d t p d t d | | * | * * | ( ) = ( ) ( ) ( ) 1 2 now calculating p(d|t) just requires that we have observed enough data to describe each variable\u2019s relationship with t in isolation."
    },
    {
        "text": "the naive bayes assumption is one of the most dramatic, cavalier oversimpli- fications in the data science world, but in practice, it\u2019s surprisingly effective!"
    },
    {
        "text": "the main thing that it tends to do is that if several of the di are closely related, they will collectively nudge your classifications too far in one direction."
    },
    {
        "text": "for example, let\u2019s say that you\u2019re trying to determine whether somebody is a man or a woman."
    },
    {
        "text": "i might then tell you that they have long hair, that they use a hair tie, and that their hair takes a long time to dry."
    },
    {
        "text": "each of these facts weighs mod- erately in favor of the person being a woman, so a naive bayes classifier would become very confident."
    },
    {
        "text": "but really, these facts are just alternative ways of saying that the person has long hair, and there are still plenty of men with long hair (including the author, once upon a time)."
    },
    {
        "text": "so, the classifier will probably be right, but it will be overconfident."
    },
    {
        "text": "19.10 \u00adbayesian networks if there are many features in your data, then it is a fool\u2019s errand to fully fit a model for p(d|t), with all of the possible interdependencies between the variables."
    },
    {
        "text": "however, there is a happy medium between this impossible task and the dramatic oversimplification of naive bayes."
    },
    {
        "text": "it is called bayesian networks."
    },
    {
        "text": "in a bayesian network, we allow for some of the di to be dependent on each other, and we arrange these dependencies in a graph as the following:"
    },
    {
        "text": "19 statistics 294 gender hair length shampoo time wears hairtie this graph indicates that gender has a relationship with hair length, which perhaps allows us to predict hair length with some accuracy."
    },
    {
        "text": "in many cases, it is a causal relationship, but not always."
    },
    {
        "text": "similarly, hair length can inform us about how long it takes somebody to use shampoo and whether they wear a hair tie."
    },
    {
        "text": "but that is it."
    },
    {
        "text": "gender might correlate with shampoo time, but only because it correlates with hair length: if you condition on the length of some- body\u2019s hair, then gender is independent of shampoo time."
    },
    {
        "text": "similarly, you can maybe guess whether somebody wears a hair tie based on how long they take to shampoo their hair, but the predictive power goes away if you know their hair length."
    },
    {
        "text": "bayesian networks can be very efficient to train and use."
    },
    {
        "text": "in this one, for example, we only need to know the distribution over the possible genders, the distribution on hair length, given the gender, and the distributions of shampoo time and hair tie wearing, given the hair length."
    },
    {
        "text": "when using a bayesian network for real applications, you can leverage domain expertise by choosing how to structure the network."
    },
    {
        "text": "which variables are likely to influence each other, or be independent, or be conditionally inde- pendent?"
    },
    {
        "text": "generally, the topology of a bayesian network is constructed by hand, and afterward, it is trained and evaluated on real\u2010world data."
    },
    {
        "text": "in a later chapter, we will discuss some of the tools available for training and using bayesian networks in python."
    },
    {
        "text": "19.11 \u00adchoosing priors: maximum entropy or domain knowledge if you are trying to train a bayes classifier, it is easy to extract baseline priors from the training data."
    },
    {
        "text": "however, there are other situations where our igno- rance really is complete, so we don\u2019t want to feed any preconceived fairy tales into our priors."
    },
    {
        "text": "the instinctive way to handle this in the case of something such as determining somebody\u2019s gender is to set the baselines equal: 50% chance woman, 50% chance man."
    },
    {
        "text": "19.13 glossary 295 there is a mathematical theory that justifies this approach: using the prior distribution with the maximum entropy."
    },
    {
        "text": "previously, we saw that the entropy of a probability distribution measures how much uncertainty there is in it, so choosing a prior that maximizes the entropy reflects complete ignorance."
    },
    {
        "text": "recall that if t is a discrete variable with n possible states, then entropy is defined as h t p p t n t t [ ] = \u2212 ( ) =\u2211 1 ln in this case, \u2212 ( ) ln pt measures how \u201csurprising\u201d it is to get a particular result t, and \u2212 ( ) p p t t ln is t\u2019s contribution to the overall entropy."
    },
    {
        "text": "if pt is 1, then ln pt ( ) will be 0, so t would contribute nothing to the entropy."
    },
    {
        "text": "taking the opposite extreme, if pt is very small, then the surprise is large, but it happens so rarely that t contributes little entropy."
    },
    {
        "text": "intuitively, there should be a \u201csweet spot\u201d that maximizes the entropy, and it turns out that we can get this by set- ting every pt to the constant 1/n."
    },
    {
        "text": "this definition of entropy has an analog for continuous probability distributions: h t f t f t t [ ] = \u2212 ( ) ( ) ( ) \u222b ln d similarly to the discrete case, the maximum entropy distribution is only coherent if there is a finite interval (or set of them) over which it is defined, and it is just a constant value of f x x x ( ) = \u2212 1 max min 19.12 \u00adfurther reading janert, p, data analysis with open source tools, 2010, o\u2019reilly media, newton, ma."
    },
    {
        "text": "diez, d, barr, c & c\u0327etinkaya\u2010rundel, m, openintro statistics, 3rd edn, 2015, openintro inc. 19.13 \u00adglossary bayesian network a dependency graph between several random variables, used to model which ones are likely to be conditionally dependent on which others."
    },
    {
        "text": "a good bayesian network is more powerful than naive bayes, but still sparse enough that it can be effectively trained on data."
    },
    {
        "text": "19 statistics 296 bonferroni correction a way to adjust the required p\u2010value for testing a hypothesis if you are testing multiple hypotheses at the same time."
    },
    {
        "text": "it accounts for the fact that, while each individual hypothesis is unlikely to pass by pure chance, the probability of some hypothesis passing by dumb luck increases as you test more hypotheses."
    },
    {
        "text": "consistent estimator an estimator that, in the limit of having a lot of data, is guaranteed to converge to the correct value (assuming that the real\u2010 world distribution is, indeed, of the same family that we are assuming)."
    },
    {
        "text": "entropy a measure of how hard it is to predict the outcome of a probability distribution."
    },
    {
        "text": "often, we pick priors to maximize the entropy in bayesian statistics."
    },
    {
        "text": "estimator a test statistic that is used to estimate a parameter in the probability distribution that data is assumed to have been drawn from."
    },
    {
        "text": "hypothesis testing a framework for testing whether a pattern in the data is \u201cstatistically significant,\u201d by looking at how likely it is to occur by random chance in the absence of an underlying real\u2010world phenomenon."
    },
    {
        "text": "multiple hypothesis testing hypothesis testing when there are multiple hypotheses that could be correct, such as multiple medicines all of which are being tested for clinical effectiveness."
    },
    {
        "text": "naive bayes the assumption that all features in a dataset are independent of each other when you condition on the target variable."
    },
    {
        "text": "this makes training bayesian models vastly simpler, and they are often still surprisingly effective."
    },
    {
        "text": "null hypothesis in hypothesis testing, the null hypothesis is the assumption that whatever pattern we have found in the data in just a fluke."
    },
    {
        "text": "for example, let\u2019s say that we have 9 of 10 flips of a coin have been heads and we think that the coin might be biased."
    },
    {
        "text": "the null hypothesis holds that the coin is fair."
    },
    {
        "text": "p\u2010value the probability of seeing a result that is as extreme (or more so) as what we see in the data if the null hypothesis is true."
    },
    {
        "text": "prior in bayesian statistics, this is our confidence distribution over an unknown variable before we have acquired any data."
    },
    {
        "text": "t\u2010test a hypothesis test for determining whether the means of two distributions are different."
    },
    {
        "text": "test statistic any number that is calculated from a dataset."
    },
    {
        "text": "unbiased estimator an estimator that is on average equal to the correct underlying value."
    },
    {
        "text": "the same mean from a dataset is an unbiased estimator of the real mean."
    },
    {
        "text": "however, the sample variance is not an unbiased estimator of the true variance: it is systematically biased to be smaller."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 297 20 so far, the book has focused on quick\u2010and\u2010dirty scripting, in the service of larger analytics goals."
    },
    {
        "text": "the most in\u2010depth i\u2019ve gotten about code is how to do unit testing and work within the context of a conventional software engineer- ing team."
    },
    {
        "text": "this chapter will take a step back and get into some of the more abstract, theoretical aspects of programming languages."
    },
    {
        "text": "this is important to know for two reasons."
    },
    {
        "text": "first, these considerations will often dictate important decisions about what tools to use when."
    },
    {
        "text": "you don\u2019t want to lock yourself into using the wrong technology for a task, especially if you find yourself working to create a large software framework."
    },
    {
        "text": "secondly, tools that are fundamentally different can take some getting used to; understanding the core concepts will ease the transi- tion if you have to pick up a new tool that is profoundly different from what you\u2019re used to."
    },
    {
        "text": "20.1 \u00adprogramming paradigms a \u201cprogramming paradigm\u201d is a conceptual way to think about the logical structure of a program and implement it in code."
    },
    {
        "text": "you can think of it as a sequence of instructions for how to perform the computation, a mathematical specification of what the output should look like, or a range of other options."
    },
    {
        "text": "before i get into the details, the first big thing you should know is that most modern high\u2010level languages support all of these paradigms to one degree or another."
    },
    {
        "text": "this means that to a large degree you can mix and match paradigms depending on what works best for the problem at hand."
    },
    {
        "text": "strictly speaking, all of these paradigms are equivalent; any computation that can be done with one can be done with any of the others."
    },
    {
        "text": "conceptually though, they can be quite different to think about, lending themselves to different applications and even personal temperaments."
    },
    {
        "text": "some people are pretty dogmatic about which paradigms are best, and some language shoehorns you into one in particular."
    },
    {
        "text": "a number of them have fancy programming language concepts"
    },
    {
        "text": "20 programming language concepts 298 theoretical aspects that can be very useful in certain situations."
    },
    {
        "text": "they also tend to be different in terms of the performance and maintenance of the code."
    },
    {
        "text": "the big three paradigms that you will see are often called \u201cimperative,\u201d \u201cobject\u2010oriented,\u201d and \u201cfunctional,\u201d and i will introduce them all in this chapter."
    },
    {
        "text": "python has at least partial support for all of them."
    },
    {
        "text": "20.1.1 imperative in imperative programming, your code is mostly a sequence of instructions for the computer to follow."
    },
    {
        "text": "these could be things such as appending a new ele- ment to a list, overwriting an existing variable in memory, or writing a file."
    },
    {
        "text": "the following python code is an imperative way to read in a csv file con- taining demographic information and then calculate the average age of all peo- ple in each state: lines = open('data.txt') broken_lines = [l.split(',') for l in lines] ages_by_state = {} for bl in broken_lines: state, age = bl[2], bl[5] state = state.strip().lower().replace(\u2019.\u2019,\u2019\u2019) age = float(age) if state in ages_by_state.keys(): ages_by_state[state].append(age) else: ages_by_state[state] = [age] mean_by_state = {} for state, ages in ages_by_state.items(): avg_age = sum(ages) / len(ages) mean_by_state[state] = avg_age out_lines = [state + ',' + str(age) for state, age in state_age_pairs] output_text = '\\n'.join(out_lines) open('output.txt','w').write(output_text) 20.1.2 functional functional programming is largely inspired by the desire to avoid \u201cside effects.\u201d a side effect means any modification that is done to existing variables (such as appending an element to a list or incrementing a number) or any interaction of the program with the outside world (such as printing to the screen)."
    },
    {
        "text": "so, the following would be side effects: print 'hello world!'"
    },
    {
        "text": "a = a + 1"
    },
    {
        "text": "20.1 programming paradigms 299 obviously, we ultimately want our code to actually do something, so side effects are a good thing in general."
    },
    {
        "text": "the problem though is that they make it difficult to reason about our code."
    },
    {
        "text": "the order in which steps are taken matters (did we print the number before or after we incremented it?"
    },
    {
        "text": "), and it can get very hard to keep track of sometimes."
    },
    {
        "text": "i\u2019ve run into this personally when using the python interpreter to help me fiddle with scripts i was writing."
    },
    {
        "text": "i had a collection of variables that i was trying to mas- sage so that they would fit into a statistical model i was building."
    },
    {
        "text": "i would run and rerun portions of my script, making small changes in between runs as i tweaked the logic."
    },
    {
        "text": "eventually, my variables were in good shape, so i thought the code was working correctly."
    },
    {
        "text": "but when i reran the code from scratch, things broke again."
    },
    {
        "text": "it turned out that some of the necessary changes had been made by previous versions of the code, and i had forgotten that those side effects had already happened."
    },
    {
        "text": "in functional programming, your code is broken up into \u201cpure\u201d functions, which take some input (or maybe none at all) and return an output, but have no side effects."
    },
    {
        "text": "here is a much more functional version of the same code from the previous section: def normalize_state(s): return s.strip().lower().replace(\u2019.\u2019,\u2019\u2019) def mean(nums): return sum(nums) / len(nums) def extract_state_age(l): pieces = l.split(\u2019,\u2019) return normalize_state(state), float(age) def get_state_mean(pairs): ages_by_state = {} for age, state in pairs: ages_by_state[state] = \\ ages_by_state.setdefault(state,[]) + [age] state_mean_pairs = [(state, mean(ages)) for state, ages in ages_by_state.items()] return sorted(state_mean_pairs, key=lambda p: p[1]) def format_output(state_age_pairs): out_lines = [state + \u2019,\u2019 + str(age) for state, age in state_age_pairs] return \u2019\\n\u2019.join(out_lines) lines = open('data.txt') state_age_pairs = [extract_state_age(l) for l in lines] output = format_output(state_age_pairs) open('output.txt','w').write(output)"
    },
    {
        "text": "20 programming language concepts 300 in this version, all but the last four lines of code are just defining functions."
    },
    {
        "text": "in the final four lines, i do create some variables based on the already existing ones, but i don\u2019t modify any that were already created."
    },
    {
        "text": "the only side effect is in the last line."
    },
    {
        "text": "technically, within the get_state_mean function, i modify the variable ages_ by_state, so this isn\u2019t 100% functional code at every level, but you get the idea."
    },
    {
        "text": "the real nirvana of functional coding though isn\u2019t just arranging your code into pure functions."
    },
    {
        "text": "it\u2019s treating functions as variables in their own right, which can be passed as arguments into other functions or even generated on the fly."
    },
    {
        "text": "take this code for example."
    },
    {
        "text": "it is a function that takes in a date encoded as a string, figures out the way the string is formatted, and returns a function that will extract the year out of strings with the same format: def get_year_extractor(example_date): # not sure if example_date is yyyymmdd # or yyyy\u2010mm\u2010dd or mm\u2010dd\u2010yyyy if len(example_date)==8: return lambda d: d[:4] elif (example_date[4]==\u2019-\u2019 and example_date[7]==\u2019-\u2019): return lambda d: d[:4] else: return lambda d: [\u20104:] extract_year = get_year_extractor(dates[0]) years = [extract_year(dt) for dt in dates] or, try the following, which is more along the lines of what you see in analytics: def get_stats(data_lst, f): def valid_input(x): try: _ = f(x) return true except: return false vals = [f(d) for d in data_lst if valid_input(d)] return {\u2019num_valid\u2019: len(vals), \u2019num_invalid\u2019: len(data_list)\u2010len(vals), \u2019mean\u2019: sum(vals) / len(vals), \u2019max\u2019: max(vals) } in this case, you might experiment with a wide range of custom\u2010built func- tions, plugging each one into get_stats to see how they all perform."
    },
    {
        "text": "functional programming lends itself well to data science, especially if you are doing some kind of batch processing."
    },
    {
        "text": "there is a clear sequence of logical stages leading from your raw data up through your final analytics outputs \u2013 no user interactions or other things that are hard to think about as pure functions."
    },
    {
        "text": "it is"
    },
    {
        "text": "20.1 programming paradigms 301 easy to plug\u2010and\u2010play with different functions, passing them around to operate on your data in novel ways."
    },
    {
        "text": "in the big data space, spark is a functional frame- work."
    },
    {
        "text": "this is part of why many people (myself included) try to write code in as functional a way as possible."
    },
    {
        "text": "an oft\u2010touted advantage of functional programming is its performance, at least if the code is compiled rather than interpreted."
    },
    {
        "text": "a functional program is more of a mathematical specification for a program\u2019s logic than a set of instruc- tions for the computer."
    },
    {
        "text": "in theory at least, the compiler can look at those speci- fications and figure out a highly efficient way to implement them \u2013 much better than you probably would have if you\u2019d written your code in an imperative way."
    },
    {
        "text": "in practice though, the compiler isn\u2019t usually very smart, and it often imple- ments things in a fairly naive way."
    },
    {
        "text": "if your functional code is in an interpreted language, it will also certainly be naively implemented and pretty slow."
    },
    {
        "text": "there are also certain control structures where functional syntax can become a bit clunky (or at least unintuitive the first time you see it)."
    },
    {
        "text": "check out these two pieces of code, which do the same thing imperatively and functionally: # imperative version my_variable = initial_version while not my_stopping_condition(my_variable): my_variable = my_function(my_variable) # functional version def loop_as_function(variable): if my_stopping_condition(variable): return variable else: return loop_as_variable(my_function(variable)) my_variable = loop_as_function(initial_version) doing 100% functional programming requires that you replace loops with recursion in this way, which many people regard as an eyesore."
    },
    {
        "text": "it\u2019s also gro- tesquely inefficient to do in python because of the under\u2010the\u2010hood boilerplate required for all the nested function calls (it can be much faster in compiled functional languages such as haskell, which use something called the \u201ctail call optimization,\u201d but python doesn\u2019t have that feature)."
    },
    {
        "text": "the granddaddy functional programming language is lisp, which dates back to 1958 but still has devoted adherents."
    },
    {
        "text": "popular, mostly functional languages today include haskell and ml."
    },
    {
        "text": "scala famously emphasizes its functional pro- gramming support, although it can also be written in a very object\u2010oriented way."
    },
    {
        "text": "20.1.3 object\u2010oriented an object\u2010oriented language will package data and the logic that handles the data into user\u2010friendly black boxes called \u201cobjects.\u201d when using an object, you don\u2019t need to worry about how the data is structured or how to untangle that"
    },
    {
        "text": "20 programming language concepts 302 structure; you only interact with special\u2010purpose functions called \u201cmethods\u201d that the object presents to you."
    },
    {
        "text": "python is inherently an object\u2010oriented language."
    },
    {
        "text": "everything you ever use or define in python code, including variables, functions, and even libraries that you import, is an object."
    },
    {
        "text": "every action you ever take in python is calling a method on some object."
    },
    {
        "text": "you might object that simple things such as integer addition are not object methods."
    },
    {
        "text": "actually though, \u201c+\u201d is just syntactic sugar around the \u201c__add__\u201d method, as you can see here: > x, y = 4, 5 > x + y 9 > x.__add__(y) 9 the interesting thing about python is that, even though it\u2019s technically 100% object\u2010oriented, you don\u2019t have to write your code that way."
    },
    {
        "text": "if you read my code, you\u2019ll see that i very rarely define my own classes; i do almost everything with python\u2019s built\u2010in container objects (such as lists and dictionaries) and the objects supplied by the libraries i use."
    },
    {
        "text": "the fact that everything gets imple- mented as an object is incidental; my code reads like a blend of imperative and functional."
    },
    {
        "text": "this is partly a matter of personal taste and partly a reflection of the kind of work i usually do."
    },
    {
        "text": "where object\u2010oriented coding shines is having your code interact with exter- nal resources."
    },
    {
        "text": "something such as a gui, where the user presses buttons at will, is best thought of as an object."
    },
    {
        "text": "the object carries around all of the data you\u2019re working with, the specification of how the gui is laid out, and whatever boil- erplate is required to interact with the computer\u2019s graphics."
    },
    {
        "text": "every button press calls some method on the object, which changes the object\u2019s internal state (i.e., a side effect) and takes any other actions necessary."
    },
    {
        "text": "if you have two processes that interact with each other, if your code connects to an iphone or an interac- tive website, or if the user gets brought into the loop \u2013 all of these situations are best thought of as interactions between objects, via their methods."
    },
    {
        "text": "everything in python is an object, but generally, they\u2019re a prepackaged type of object such as a list or an int."
    },
    {
        "text": "the key feature of object\u2010oriented code is the definition of whole new types of objects."
    },
    {
        "text": "these types of objects are called \u201cclasses.\u201d a class specifies the internal structure of an object, as well as all of its associated methods."
    },
    {
        "text": "you can have many objects that are all of the same class, but the class itself is only defined once."
    },
    {
        "text": "to see how this works, let\u2019s dive into some code: class person: def __init__(self, name, age):"
    },
    {
        "text": "20.1 programming paradigms 303 self.name = name self.age = age def talk(self): print \u2019my name is %s and i am %s years old\u2019 \\ % (self.name, self.age) janice = person('janice smith', 28) bob = person('bob jones', 30) bob.talk() this code defines a class called \u201cperson,\u201d with methods \u201c__init__\u201d and \u201ctalk,\u201d and creates two persons called janice and bob."
    },
    {
        "text": "the first thing that might jump out at you is the goofy word \u201cself.\u201d where did that come from?"
    },
    {
        "text": "it looks like it\u2019s an argument in the functions __init__ and talk, but it\u2019s never there when those objects get called."
    },
    {
        "text": "what gives?"
    },
    {
        "text": "ok, this aspect of python is incredibly confusing for many people, so much so that some of them abandon the language entirely, so let me try to make it clear."
    },
    {
        "text": "when you, the programmer, call a member function on an object, you call it my_object.my_function(argument1, argument2)."
    },
    {
        "text": "however, under the hood, the actual object itself gets passed into the function, silently, as an additional argument before all of the others."
    },
    {
        "text": "so, when you write the code that actually implements the function, you write it so as to accept this additional argument."
    },
    {
        "text": "calling the argument \u201cself\u201d is just a convention, but it\u2019s an almost universal one."
    },
    {
        "text": "if you want to refer to your person\u2019s age within the member function, you say \u201cself.age.\u201d in many other object\u2010oriented languages, you just say \u201cage,\u201d and it is understood that this refers to the \u201cage\u201d field of the object being operated on (these language also don\u2019t require the object to be an argument in the imple- mentation of the function)."
    },
    {
        "text": "however, i think that python\u2019s way, while admit- tedly more verbose, makes the code much more readable."
    },
    {
        "text": "in a long and complicated file, it\u2019s entirely possible that i declared a variable called \u201cage\u201d somewhere else in my script or imported a library called \u201cage.\u201d especially if somebody else implemented the person object and i\u2019m just perusing their code, i might not know that person even has \u201cage\u201d field."
    },
    {
        "text": "so, i may end up having to dig through the rest of the code just to figure out whether \u201cage\u201d is a member of the object or whether it\u2019s something else."
    },
    {
        "text": "saying \u201cself.name\u201d removes all of this ambiguity."
    },
    {
        "text": "you are welcome to hate this aspect of python if you so choose; many great engineers will agree with you."
    },
    {
        "text": "but personally, i love it."
    },
    {
        "text": "now that you understand what \u201cself\u201d means, the \u201ctalk\u201d method should look pretty straightforward."
    },
    {
        "text": "however, __init__ might be something of a mystery."
    },
    {
        "text": "this special\u2010purpose function is what builds an object up when it is first cre- ated."
    },
    {
        "text": "when i said \u201cperson(\u2018janice smith\u2019, 28),\u201d the first thing that happened was an empty person object was created."
    },
    {
        "text": "then, under the hood, its __init__ func- tion was called with itself, the string \u201cjanice smith,\u201d and the number 28 as its arguments."
    },
    {
        "text": "the __init__ function then created the new \u201cname\u201d and \u201cage\u201d fields for this object and populated them accordingly."
    },
    {
        "text": "20 programming language concepts 304 you might hear __init__ referred to as a \u201cconstructor,\u201d which serves the same purpose in other languages."
    },
    {
        "text": "technically, this is incorrect though."
    },
    {
        "text": "a construc- tor aids in actually constructing the object when it first comes into being."
    },
    {
        "text": "in python though, the object technically exists before __init__ is called; it just hasn\u2019t been filled with any data fields yet."
    },
    {
        "text": "the place where objects and classes start to get really interesting is when \u201cinheritance\u201d comes into play."
    },
    {
        "text": "i might define an \u201canimal\u201d class, with methods that apply to all animals."
    },
    {
        "text": "but then i might also want a \u201cparrot\u201d class, which carries over all of the logic and member functions of animal, but adds on oth- ers that are specific to parrots."
    },
    {
        "text": "any parrot would then be both a parrot and an animal."
    },
    {
        "text": "in python, the code might look like this: class animal: def __init__(self, name): self.name = name class parrot(animal): def talk(self): print self.name + \u2018 want a cracker!\u2019 fido = animal(\u2018fido\u2019) polly = bird(\u2018polly\u2019) in this case, the internal structure of a parrot is the same as that of a generic animal; the only difference is that parrot has a \u201ctalk\u201d function."
    },
    {
        "text": "if i wanted to add additional internal structure to parrot, i could do it the following way: class animal: def __init__(self, name): self.name = name class parrot(animal): def __init__(self, name, wingspan): self.wingspan = wingspan animal.__init__(self, name) def talk(self): print self.name + \u2018 want a cracker!\u2019 fido = animal(\u2018fido\u2019) polly = bird(\u2018polly\u2019, 2) in this case, we have given parrot its own __init__ function, which takes in an additional argument and has precedence over the __init__ function of the animal class."
    },
    {
        "text": "in some object\u2010oriented languages, both __init__ functions would then get called when we create polly."
    },
    {
        "text": "in python though, parrot\u2019s __init__ function overrides animal\u2019s entirely, and it is up to the coder to make sure that everything they really wanted from animal.__init__ is salvaged."
    },
    {
        "text": "in this case,"
    },
    {
        "text": "20.2 compilation and interpretation 305 animal.__init__ doesn\u2019t do anything that conflicts with parrot.__init__, so we are free to call animal.__init__ explicitly within parrot.__init__."
    },
    {
        "text": "as with the \u201cself\u201d word in member functions, this makes your code more verbose and argu- ably uglier, but it also makes the logic much more explicit."
    },
    {
        "text": "personally, my code generally doesn\u2019t use class inheritance."
    },
    {
        "text": "the one big exception is that some of python\u2019s libraries provide a very fancy class that wraps a lot of functionality, and you write your own class that inherits from it."
    },
    {
        "text": "for example, the htmlparser class has logic for wading through and parsing html text."
    },
    {
        "text": "i\u2019ve often written classes that inherit from htmlparser and that identify and process specific pieces of information as htmlparser walks through the text."
    },
    {
        "text": "similar to functional programming, the object\u2010oriented paradigm has a broad theoretical foundation and many people who are real purists about using it."
    },
    {
        "text": "hardcore object\u2010oriented code consists almost entirely of defining class and their member functions and makes liberal use of class inheritance and other fancy features."
    },
    {
        "text": "a few other things you should be familiar with: \u25cf \u25cfstatic member data are not associated with individual objects, but with the class itself."
    },
    {
        "text": "for example, there might be a static integer that keeps track of how many instances of a class are currently in existence."
    },
    {
        "text": "\u25cf \u25cfit is possible to have a single class inheriting from multiple other classes."
    },
    {
        "text": "for example, a parrot can be both an animal and a thingthatflies."
    },
    {
        "text": "you can do this in python and many other languages, but it\u2019s relatively rare."
    },
    {
        "text": "20.2 \u00adcompilation and interpretation \u201ccompiled languages\u201d and \u201cinterpreted languages\u201d used to have very clear\u2010cut meanings."
    },
    {
        "text": "advances in recent years though have blurred the distinction into more of a continuum."
    },
    {
        "text": "first, i\u2019ll go over the old\u2010school meanings and then get into some of the modern variants."
    },
    {
        "text": "traditionally, a compiler is a special\u2010purpose program that translates human\u2010 readable code into \u201cmachine code,\u201d which just operates on raw bytes and is directly executed by the microprocessor."
    },
    {
        "text": "in performing the translation, the compiler has to know the details of the microprocessor it\u2019s writing code for and how it expects its machine code to be formatted."
    },
    {
        "text": "the compiler must make all sorts of judgment calls about how a single high\u2010level operation should be translated into a series of low\u2010level operations."
    },
    {
        "text": "some compilers are pretty naive in the way they make these judgment calls, but so\u2010called optimizing compilers make them in very judicious ways and can even reorganize much of your pro- gram so that the behavior is the same, but it runs more efficiently."
    },
    {
        "text": "a language such as c is compiled, so your computer can\u2019t \u201crun\u201d your c code in any direct fashion."
    },
    {
        "text": "first, you have to compile your code into machine code."
    },
    {
        "text": "20 programming language concepts 306 the resulting blob of code is called an \u201cexecutable,\u201d since the computer can actually run it."
    },
    {
        "text": "however, the executable contains no vestige of the fact that it was originally written in c. the original machine code could have been written in any compiled language, or even directly in machine code one byte at a time, and have the executable be the same."
    },
    {
        "text": "an important concept to understand with compiled languages is the differ- ence between something happening at \u201crun time\u201d and at \u201ccompile time.\u201d a syntax error in your code will generally show up as soon as you try to compile it."
    },
    {
        "text": "the compiler will be unable to perform its translation and will throw an error, and you find out sooner rather than later that you screwed something up."
    },
    {
        "text": "other times though, the code will compile just fine, and you will only discover the problem when the computer actually tries to run the machine code."
    },
    {
        "text": "this discovery might happen hours into the run or maybe even after you\u2019ve pushed a product to consumers."
    },
    {
        "text": "for this reason, a lot of work in compiler design boils down to trying to find issues at compile time."
    },
    {
        "text": "in contrast to compiled languages such as c, you have interpreted languages such as python."
    },
    {
        "text": "an interpreter is a special\u2010purpose program that reads and executes your code one line at a time."
    },
    {
        "text": "the interpreter itself is a blob of machine code that was originally written in something such as c. python code is acted on by the interpreter program but never translated into machine code."
    },
    {
        "text": "generally speaking, compiled languages are blindingly fast and efficient rela- tive to interpreted ones."
    },
    {
        "text": "this is partly because interpreted language must incur the overhead of parsing every line of code at run time and partly because there is no opportunity for a compiler to build optimizations in."
    },
    {
        "text": "however, inter- preted languages let you play around and debug your code one line at a time, and they spare you from worrying about what\u2019s going on under the hood."
    },
    {
        "text": "if somebody wanted, they could write an interpreter for a compiled language such as c and, similarly, a compiler for an interpreted language such as python."
    },
    {
        "text": "this has happened in some cases for some languages, but it\u2019s pretty rare."
    },
    {
        "text": "an interpreter for something such as c would completely kill performance, which is the whole reason to use c in the first place."
    },
    {
        "text": "if you wanted to compile python, the compiler cannot always infer the key things about the program (such as which variables are which types) that allow for efficient compiled code."
    },
    {
        "text": "ok, that\u2019s how things used to be."
    },
    {
        "text": "now the gray area."
    },
    {
        "text": "i actually lied when i told you that python is interpreted."
    },
    {
        "text": "many languages in the past were really and truly interpreted, but python can be translated into an intermediate represen- tation called \u201cbytecode.\u201d python will be run one line at a time if you open up an interpreter from the command line."
    },
    {
        "text": "but if you run your python script all at once, or if a library is imported, it is first translated into bytecode."
    },
    {
        "text": "this is far from compilation in the traditional sense, since bytecode is at the same level of abstraction as raw python and gives minimal performance optimizations an even more gray area is the java language."
    },
    {
        "text": "java also compiles to an inter- mediate bytecode, but java bytecode is very low\u2010level relative to the original"
    },
    {
        "text": "20.3 type systems 307 java language."
    },
    {
        "text": "in fact, the interpreter is called the \u201cjava virtual machine\u201d (jvm), since the bytecode feels less like a regular programming language and more like low\u2010level machine code."
    },
    {
        "text": "the java compiler does a ton of optimizations in generating the bytecode, so you get most of the performance benefits of a com- piled language."
    },
    {
        "text": "the jvm mostly just provides you with memory management and access to the computer\u2019s resources."
    },
    {
        "text": "the .net framework that microsoft uses is very similar to the jvm."
    },
    {
        "text": "the c# language is essentially equivalent to java (originally, they were planned to be the same language, but then corporate politics got in the way), and it compiles into .net bytecode in the same way."
    },
    {
        "text": "for both the jvm and .net, there are actually a wide range of languages that can be compiled into the same bytecode, even if java and c# are the flagships."
    },
    {
        "text": "so, bytecode is really becoming the lingua franca of these software environ- ments; the actual machine code is an afterthought that is taken care of by the virtual machine, in a way that is specific to whatever computer it\u2019s running on."
    },
    {
        "text": "the blurring of compilation and interpretation has even taken place on the level of the guts of the computer."
    },
    {
        "text": "without getting into too much historical detail, intel changed their fundamental machine code from what\u2019s called a cisc model (specifically x86) to a more efficient risc model."
    },
    {
        "text": "however, doing this rendered all previously compiled code obsolete, so they hard\u2010coded logic into the actual silicon that translated compiled x86 code into the risc code, in real time as the executable was running."
    },
    {
        "text": "so really, you should think of \u201ccompilation\u201d as just the translation of code that is in one language into some lower\u2010level language."
    },
    {
        "text": "conversely, you should then think of \u201cinterpretation\u201d as plugging code into a \u201cmachine\u201d (virtual or physical) that executes the code."
    },
    {
        "text": "one final complication bears noting."
    },
    {
        "text": "\u201cjust\u2010in\u2010time\u201d (jit) compilation some- times happens on the fly, while an otherwise interpreted language is being run."
    },
    {
        "text": "maybe the compiler couldn\u2019t guarantee, at compile time, that a list would only ever contain integers."
    },
    {
        "text": "however, as the program is running that guarantee might become possible, and the code that processes the list can be recompiled on the fly with the new knowledge."
    },
    {
        "text": "this is a lot of overhead, but sometimes, jit compilation gives huge performance gains."
    },
    {
        "text": "20.3 \u00adtype systems besides the programming paradigm(s) that it implements, a language is also characterized by its \u201ctype system.\u201d every variable defined in your code will have a type associated with it, such as an integer, a dictionary, a string, or a custom\u2010 defined class."
    },
    {
        "text": "performing an operation on an object requires knowing what type it is, so that we know how to parse the object\u2019s underlying bytes and actu- ally implement the operation."
    },
    {
        "text": "there is a large, theoretical discipline devoted to"
    },
    {
        "text": "20 programming language concepts 308 studying type systems and defining their properties, but you don\u2019t need to know about it."
    },
    {
        "text": "there are really just two main concepts you should be familiar with: whether something is \u201cstrongly\u201d or \u201cweakly\u201d typed and whether it is \u201cstatically\u201d or \u201cdynamically\u201d typed."
    },
    {
        "text": "20.3.1 static versus dynamic typing a language is \u201cstatically typed\u201d if the computer figures out, at the time the code is compiled, what the type is of all the variables."
    },
    {
        "text": "this allows the compiler to store and process the data in the most efficient way possible."
    },
    {
        "text": "it is dynamically typed if the types are not known until the code is run, meaning that there will be some additional boilerplate to keep track of what variables are integers, strings, lists, and so on."
    },
    {
        "text": "python is a great example of a dynamically typed language."
    },
    {
        "text": "the interpreter is written in c, and under the hood, every variable is implemented as a c struc- ture called a pyobject."
    },
    {
        "text": "one function of the pyobject structure is to keep track of what the type is of each variable."
    },
    {
        "text": "there is a lot of overhead in this approach."
    },
    {
        "text": "most simply, you have to store more stuff in ram: not just your actual data, but the type metadata."
    },
    {
        "text": "the other problem is that, before your code can per- form some operation (such as \u201c+\u201d) on a variable, it must first check what data type that variable is and hence what the operation means in this context."
    },
    {
        "text": "dynamic typing has many benefits in terms of flexibility, but you pay a large performance cost."
    },
    {
        "text": "in a statically typed language such as c, on the other hand, the compiler can just translate every operation into the appropriate byte\u2010level manipulations, without storing any explicit reference to the data types or any method lookups."
    },
    {
        "text": "many interpreted languages have a particular type of dynamic typing some- times called \u201cduck typing.\u201d this means that a variable is considered to be the \u201cright\u201d type if every operation that is ever called on it is defined when it is called."
    },
    {
        "text": "the term \u201cduck typing\u201d comes from the idea that if it has a quack() method and a walk() method, we may as well call it a duck."
    },
    {
        "text": "in compiled languages, one of the most important steps is for the compiler to figure out what data type every variable is, because that will dictate how every subroutine should operate at the level of byte manipulations."
    },
    {
        "text": "in many lan- guages, such as c, you must do this explicitly and tell the computer what type every variable is (and hence, implicitly, how it should store that variable in bytes)."
    },
    {
        "text": "in some more modern compiled languages, you don\u2019t have to declare the types explicitly, and there are elaborate mechanisms in the compiler that examine your code and infer what types the variables are."
    },
    {
        "text": "20.3.2 strong versus weak typing typing strength is a much fuzzier notion than whether a language is dynami- cally or statically typed."
    },
    {
        "text": "roughly, it means to what degree the language forces"
    },
    {
        "text": "20.5 glossary 309 you to use types and their operations consistently."
    },
    {
        "text": "let me give you several examples: \u25cf \u25cfcombining strings with other types."
    },
    {
        "text": "python is strongly typed in that >>> c = \u201dhello\u201d + 5 will throw an error."
    },
    {
        "text": "however, many other languages will guess that you mean to turn 5 into a string and set c to \u201chello5.\u201d \u25cf \u25cfmixing integers and floats."
    },
    {
        "text": "python is weakly typed in that \u201c3/2\u201d and \u201c3.0/2\u201d are both valid expressions, but they will return different values (1 and 1.5, respec- tively) because integer and floating\u2010point division are different operations."
    },
    {
        "text": "python guesses that in 3.0/2 you actually mean 2.0 and performs the division accordingly."
    },
    {
        "text": "however, a language such as ocaml will actually give a compile\u2010 time error; you have to explicitly turn 2 into 2.0 before diving 3.0 by it."
    },
    {
        "text": "\u25cf \u25cfc is typically a strongly typed language, since you have to declare the type of every variable in the code itself and the code won\u2019t compile if the types don\u2019t match up."
    },
    {
        "text": "however, you can also force the computer to treat a random chunk of bytes as if they were data of a particular type."
    },
    {
        "text": "the random bytes may or may not constitute a valid instance of that type."
    },
    {
        "text": "\u25cf \u25cfremember that the underlying bytes of memory are not typed; c lets you access this flexibility, whereas higher\u2010level languages put layers of security around it."
    },
    {
        "text": "20.4 \u00adfurther reading 1 scott, m, programming language pragmatics, 4th edn, 2015, morgan kaufmann, burlington, ma."
    },
    {
        "text": "2 martin, r, clean code: a handbook of agile software craftsmanship, 2009, prentice hall, upper saddle river, nj."
    },
    {
        "text": "20.5 \u00adglossary anonymous function a function that is never given an explicit name."
    },
    {
        "text": "these are often defined on the fly when they are passed as an argument into another function."
    },
    {
        "text": "compiler a software program that translates human\u2010readable source code into a low\u2010level language that is more suitable for actually running."
    },
    {
        "text": "this is often machine code or bytecode for a virtual machine."
    },
    {
        "text": "constructor a subroutine that constructs a user\u2010defined object in memory, including correctly initializing all of its internal state."
    },
    {
        "text": "duck typing a type system where a variable is considered to have the correct type if every operation that is ever called on it is defined."
    },
    {
        "text": "20 programming language concepts 310 functional programming a programming paradigm where the code consists mostly of pure functions."
    },
    {
        "text": "imperative programming a programming paradigm where the code consists mostly of side effects."
    },
    {
        "text": "inheritance in object\u2010oriented programming, this is defining a new class that inherits the logic and methods of a previously existing class."
    },
    {
        "text": "jit compilation just\u2010in\u2010time compilation."
    },
    {
        "text": "just\u2010in\u2010time compilation compiling parts of an interpreted language at runtime, so as to gain a performance advantage."
    },
    {
        "text": "object\u2010oriented programming a programming paradigm where your code consists mostly of defining new classes and objects, which mask their internal state and present apis that can be accessed by other objects."
    },
    {
        "text": "programming paradigm a way to think about the operation of your program and to break it apart into logical pieces."
    },
    {
        "text": "pure function a subroutine that returns a value and has no side effects."
    },
    {
        "text": "side effect any change in a program\u2019s state that is caused by a function executing."
    },
    {
        "text": "side effects include modifying existing variables in memory and writing output to the screen or the file system."
    },
    {
        "text": "type safety enforcing that operations within a piece of code only ever happen to variables of the appropriate type."
    },
    {
        "text": "virtual machine a piece of software that interprets low\u2010level bytecode (such as java bytecode) and handles the interface between it and the hardware and operating system."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 311 21 this chapter discusses ways that your code can be made to run faster."
    },
    {
        "text": "it roughly breaks into two very distinct topics: \u25cf \u25cfthe theory of how fast an algorithm is in the abstract, independent of the details of the computer or the implementation."
    },
    {
        "text": "you don\u2019t need to know a whole lot about this subject \u2013 mostly just how to avoid a few potentially cata- strophic errors."
    },
    {
        "text": "\u25cf \u25cfvarious nitty\u2010gritty performance optimizations, which mostly involve mak- ing good use of the computer\u2019s memory and cache."
    },
    {
        "text": "the first of these topics relates to figuring out which algorithms are funda- mentally, theoretically better than others."
    },
    {
        "text": "the second topic is about how to eke out real\u2010world performance gains for whatever abstract algorithm you are using."
    },
    {
        "text": "21.1 \u00adexample script the following script examines two different ways to solve the same problem: given a list of items, count how many of the entries are duplicates of some other entry."
    },
    {
        "text": "it sees how long each algorithm takes for lists of varying length and plots out the time."
    },
    {
        "text": "the two ways i solve the problem are as follows: \u25cf \u25cffor each element x in the list, count how many times it occurs in the list \u2013 if it occurs more than once, then it is a duplicate."
    },
    {
        "text": "the key thing here is that counting the occurrences of x will require searching through the whole list and checking whether the elements are equal to x. that is, for every element of the list, we loop through the whole list."
    },
    {
        "text": "\u25cf \u25cfkeep a dictionary that maps elements in the list to how many times they have occurred, then loop through the list, and update this dictionary."
    },
    {
        "text": "at the end, add up all of the counts that are greater than 1. performance and computer memory"
    },
    {
        "text": "21 performance and computer memory 312 for reasons that will become clear, i call these approaches o(n2) and o(n), respectively."
    },
    {
        "text": "import time import matplotlib.pyplot as plt import numpy as np def duplicates_on2(lst): ct = 0 for x in lst: if lst.count(x) > 1: ct += 1 return ct def duplicates_on(lst): cts = {} for x in lst: if cts.has_key(x): cts[x] += 1 else: cts[x] = 1 counts_above_1 = [ct for x, ct in cts.items() if ct > 1] return sum(counts_above_1) def timeit(func, arg): start = time.time() func(arg) stop = time.time() return stop - start times_on, times_on2 = [], [] ns = range(25) for n in ns: lst = list(np.random.uniform(size=n)) times_on2.append(timeit(duplicates_on2, lst)) times_on.append(timeit(duplicates_on, lst)) plt.plot(times_on2, \"--\", label=\"o(n^2)\") plt.plot(times_on, label=\"o(n)\") plt.xlabel(\"length n of list\") plt.ylabel(\"time in seconds\") plt.title(\"time to count entries w duplicates\") plt.legend(loc=\"upper left\") plt.show() there is noise in the data because i ran it on my actual computer \u2013 there are other processes going on that can make this take more or less time."
    },
    {
        "text": "however, some trends are still clear."
    },
    {
        "text": "when the lists are short, the two algorithms are"
    },
    {
        "text": "21.1 example script 313 time to count entries w duplicates time (seconds) o(n2) o(n) 0.000014 0.000012 0.000010 0.000008 0.000006 0.000004 0.000002 0.000000 0 5 10 length n of list 15 20 25 0.00006 0.00005 0.00004 time to count entries w duplicates 0.00003 time (seconds) 0.00002 0.00001 0 10 20 30 length n of list 40 50 0.00000 o(n2) o(n) comparable, with o(n2) often being better."
    },
    {
        "text": "but as the lists get longer, a gap opens up, o(n2) starts to take much longer."
    },
    {
        "text": "i tried it again, with lists up to length 50, and got the following:"
    },
    {
        "text": "21 performance and computer memory 314 we can see that as the list gets longer, o(n) becomes a better way to solve the problem."
    },
    {
        "text": "this has nothing to do with the computer i\u2019m using or the fact that i\u2019m doing it in python: it is a fundamentally better algorithm."
    },
    {
        "text": "21.2 \u00adalgorithm performance and big-o notation the standard way to discuss the theoretical performance of an algorithm in the abstract is called big\u2010o (\u201cbig oh\u201d) notation."
    },
    {
        "text": "in the aforementioned example, we had an algorithm that is o(n) and one that is o(n2)."
    },
    {
        "text": "intuitively saying that an algorithm is o(n2) means that, in the limit of large input, the algorithm\u2019s runt- ime will be approximately some constant times n2."
    },
    {
        "text": "similarly, if the algorithm is o(n), then it will be approximately a constant times n. we can see this visually in the given graph: one curve is more or less a straight line, and the other resembles a parabola."
    },
    {
        "text": "there are two key things about assessing algorithms using big\u2010o notation: \u25cf \u25cfhow long the code actually takes involves a lot of other factors, such as the machine you are running on and how efficiently each step in the algorithm is done."
    },
    {
        "text": "in the aforementioned example, o(n) algorithm actually takes longer for small n, because i used a particularly inefficient implementation."
    },
    {
        "text": "\u25cf \u25cfin the limit of large input size, differences in big\u2010o notation will come to dominate the runtime between two algorithms."
    },
    {
        "text": "basically, big\u2010o notation tells you nothing about how long one step in your algorithm will take, but it does tell you how the number of steps grows as a function of input size."
    },
    {
        "text": "the big\u2010o performance of an algorithm is often called its \u201ccomplexity.\u201d this is an unfortunate choice of terminology, since the number of steps an algo- rithm takes has nothing to do with how complicated those steps are."
    },
    {
        "text": "in fact, in my experience, the less \u201ccomplex\u201d algorithms are often the most complicated to understand, because people do all kinds of complicated tricks to reduce their big\u2010o complexity."
    },
    {
        "text": "but the terminology is universal, so i will use it."
    },
    {
        "text": "look more closely at the o(n2) algorithm."
    },
    {
        "text": "there were n elements in the list that we looped over."
    },
    {
        "text": "for each element x we called the count() method of the list, which looped over the list checking for equality with x. this means that there are n2 comparisons made."
    },
    {
        "text": "since they are made in order, the time for the code to run will be approximately the time required for a compari- son times n2."
    },
    {
        "text": "the other algorithm also loops through the list."
    },
    {
        "text": "however, for a given element x, it only checks whether x is in the cts dictionary and updates that dictionary as needed."
    },
    {
        "text": "under the hood, dictionaries are implemented such that this opera- tion takes a fixed amount of time, regardless of how many elements are in it."
    },
    {
        "text": "that is, checking and updating the dictionary takes o(1) time."
    },
    {
        "text": "so, there are n"
    },
    {
        "text": "21.3 some classic problems: sorting a list and binary search 315 iterations of the outer loop, each of which takes o(1) time, for a total runtime that is o(n)."
    },
    {
        "text": "as a rule of thumb, your data science code should never have an o(n2) algo- rithm that operates on your entire dataset, unless you really mean it."
    },
    {
        "text": "every basic data science operation i\u2019ve shown you so far in the book (training classi- fiers, different relational operations, etc.)"
    },
    {
        "text": "has good asymptotic performance when it\u2019s implemented well (and the libraries i\u2019m showing you all have efficient implementations), so chances are that you wouldn\u2019t run an o(n2) algorithm by accident."
    },
    {
        "text": "it happens though, and it is disastrous when it does."
    },
    {
        "text": "if you end up writing core algorithms yourself, you will have to be very keenly aware of their complexity."
    },
    {
        "text": "21.3 \u00adsome classic problems: sorting a list and binary search this section will show you, by example, how to calculate the big\u2010o complexity of an algorithm by two classic problems: searching for an element in a list and sorting a list."
    },
    {
        "text": "let\u2019s say you have a list of length n and you want to find whether there is an element x in it."
    },
    {
        "text": "then, you might implement the following algorithm: input: list l of length n x initialization: i = 0 algorithm: for i in 1, 2, ..., n: if l[i] == x: return true return false we assume that getting l[i] and comparing it to x is an o(1) operation, and we see that this can happen up to n times."
    },
    {
        "text": "so, the search algorithm is o(n)."
    },
    {
        "text": "now let\u2019s assume that the list is sorted."
    },
    {
        "text": "this assumption allows for a much more efficient algorithm called a binary search."
    },
    {
        "text": "it goes as follows: input: list l of length n x initialization: i = 0 j = n-1"
    },
    {
        "text": "21 performance and computer memory 316 algorithm: while true: y = l[(i+j)/2] if y== x: return true elif j==i or j==i+1: return false else: if y>x: j = (i+j) / 2 else: i = (i+j) / 2 this algorithm\u2019s complexity is more difficult to analyze."
    },
    {
        "text": "the key observation is that after each iteration of the main loop, the distance between i and j divides by 2, and then we rerun the loop on this smaller problem."
    },
    {
        "text": "if we let t(n) be the runtime for a given n, we then see that t n t n ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o 1 2 we can in turn expand the t(n/2) out to see that t n t n ( ) = ( )+ ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o o 1 1 22 t n t n ( ) = ( )+ ( )+ ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o o o 1 1 1 23 t n t n k ( ) = ( )+...+ ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o o 1 1 2 every time you expand t n k 2 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 out, you incur an additional o(1) runtime, and you can continue the expansion until n k 2 becomes o(1)."
    },
    {
        "text": "this will happen after log2(n) steps, so we can see that the binary search is an o(log(n)) algo- rithm."
    },
    {
        "text": "for large n, this will be much faster than o(n)."
    },
    {
        "text": "the efficiency of binary search raises the question of how long it takes to sort a list."
    },
    {
        "text": "the most obvious algorithm is input: list l of length n initialization: newlist = empty list algorithm: while l contains elements: mn = min(l)"
    },
    {
        "text": "21.3 some classic problems: sorting a list and binary search 317 delete mn from l newlist.append(mn) return newlist let\u2019s assume that calculating mn = min(l) requires looping over all of l. then, assume that deleting it from l takes o(1) time."
    },
    {
        "text": "in this case, the loop will first calculate the min of a list of length n, then it will be a list of length n \u2212 1, then n \u2212 2, and so on."
    },
    {
        "text": "in total, this will be n n n n n n n n + \u2212 ( )+( )+...+ + = + ( ) = + = ( ) 1 2 2 1 1 2 2 2 2 2 \u2212 o so, we see that the obvious way to sort a list takes quadratic time, which should suggest to you that there\u2019s a better way to do it."
    },
    {
        "text": "a superior sorting method is called mergesort."
    },
    {
        "text": "it works by dividing l into two equal\u2010sized pieces, recursively mergesorting them both, and then merging the two sorted lists into a single sorted list."
    },
    {
        "text": "it looks like the following: function mergesort input: list l of length n initialization: newlist = empty list algorithm: if len(l)=1: return l else: l1 = l[1:len(l)/2] l2 = l[len(l)/2:] s1 = mergesort(l1) s2 = mergesort(l2) return merge(m1, m2) where the function merge looks like the following: function merge input: lists l1, l2 initialization: newlist = empty list algorithm: while ~l1.isempty() and ~l2.isempty(): if (l1[0] <= l2[0]) or l2 is empty: mn = l1[0] delete l1[0]"
    },
    {
        "text": "21 performance and computer memory 318 else: mn = l2[0] delete l2[0] newlist.append(mn) if ~l1.isempty(): newlist.extend(l1) elif ~l2.isempty(): newlist.extend(l2) return newlist the merge function will be o(n), if n is the length of its longest input."
    },
    {
        "text": "this means that if t(n) is the time to mergesort a list of length n, then t n n t n t n ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o 2 2 t n n t n ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o 2 2 * t n n n t n ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o o 2 2 4 * t n n n t n ( ) = ( )+ ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 o o 2 4 * t n n t n ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 2 2 4 * * o t n k n t n k k ( ) = ( )+ \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 * * o 2 2 we can break this down until n/2k is about 1, that is, k ~ log2(n)."
    },
    {
        "text": "that will give us t n n n n n n ( ) = ( ) ( )+ ( ) = ( ) ( ) ( ) log o o o 2 2 1 2 * * * log log 21.4 \u00adamortized performance and average performance one limitation of big\u2010o complexity is that it is worst\u2010case performance."
    },
    {
        "text": "in the real world though, there are situations where the big\u2010o complexity of an algo- rithm is quite bad, but the observed runtime of the code scales well."
    },
    {
        "text": "21.4 amortized performance and average performance 319 the simplest case of this is certain algorithms that are deliberately rand- omized, in a way that could run slowly but will almost certainly run fast."
    },
    {
        "text": "the quicksort algorithm is the most famous example of this, and it is typically an even better way to sort a list than the mergesort i showed you earlier."
    },
    {
        "text": "it also has o(n*log(n)) runtime, and the algorithm is function quicksort input: list l of length n initialization: newlist = empty list algorithm: elem = random element of l lessthan = [x for x in l if x<elem] morethan = [x for x in l if x>elem] sortedless = quicksort(lessthan) sortedmore = quicksort(morethan) return sortedless + [elem] + sortedmore this algorithm will be o(n2) if, by random chance, whenever quicksort is called (including its recursive calls), it picks elem to be the largest value in the list."
    },
    {
        "text": "quicksort is o(n2) worst-case, but o(n*log(n)) on average."
    },
    {
        "text": "the other big caveat to big\u2010o notation is called \u201camortized analysis.\u201d this comes into play if you are doing many operations that modify a data structure in a running program."
    },
    {
        "text": "often we cannot design it so that all operations take o(1) time, but we can make it so that over the course of time, they average out to be linear."
    },
    {
        "text": "a great example of amortized performance is python dictionaries."
    },
    {
        "text": "usually, adding a new element to a dictionary is an o(1) operation: it doesn\u2019t depend on how many elements are already in the dictionary."
    },
    {
        "text": "as more and more elements are added though, the dictionary\u2019s internal structure (which i\u2019ll describe later \u2013 it\u2019s called a \u201chash map\u201d) starts to fill up, and occasion- ally, the data must be reshuffled to make more room."
    },
    {
        "text": "reshuffling a diction- ary that contains n elements is an o(n) operation, so as the dictionary grows, these reshufflings become more and more costly."
    },
    {
        "text": "however, they also become more rare, so that the average time taken over many changes is o(1)."
    },
    {
        "text": "this script and the figure it generates demonstrate how this works."
    },
    {
        "text": "we start off with an empty dictionary and then add elements to it, one at a time, until there are 10 million of them."
    },
    {
        "text": "the time taken for each addition is recorded, and then we plot them all out."
    },
    {
        "text": "most additions take a fixed, tiny amount of time."
    },
    {
        "text": "but occasionally, an addition will trigger a reshuffling of the data in the dictionary, and the time spikes up."
    },
    {
        "text": "21 performance and computer memory 320 import time import matplotlib.pyplot as plt times, d = [], {} for i in range(10000000): start = time.time() d[i] = i stop = time.time() times.append(stop-start) plt.plot(times) plt.xlabel(\"dictionary size\") plt.ylabel(\"time to add an element (seconds)\") plt.show() 0.25 0.20 0.15 time to add element (seconds) 0.10 0.05 0.00 0.0 0.2 0.4 dictionary size 1e7 0.6 0.8 1.0 21.5 \u00adtwo principles: reducing overhead and managing memory big\u2010o complexity tells you about how to make sure that your code scales and occasionally helps you avoid making catastrophically bad design choices."
    },
    {
        "text": "now let\u2019s move on to the more mundane considerations, which involve the com- puter you\u2019re working on and the tools you\u2019re using."
    },
    {
        "text": "these won\u2019t change your code\u2019s big\u2010o complexity, but they could easily determine whether your code runs fast enough to be of any practical use."
    },
    {
        "text": "21.5 two principles: reducing overhead and managing memory 321 the rest of this chapter will discuss several concrete tips for improving the performance of your code."
    },
    {
        "text": "however, those tips all fall under two broad catego- ries, and i would like to briefly review them: \u25cf \u25cfreducing overhead."
    },
    {
        "text": "many operations that you do incur a certain amount of overhead every time you do them."
    },
    {
        "text": "these little performance penalties add up, especially if you are incurring them over and over again within a loop."
    },
    {
        "text": "\u25cf \u25cfmaking better use of the computer\u2019s memory and caching."
    },
    {
        "text": "the idea of reducing overhead is pretty self\u2010explanatory."
    },
    {
        "text": "understanding how to hack your computer\u2019s memory is a bit more complicated and deserves some explanation."
    },
    {
        "text": "this is especially the case because, in my experience, memory issues are more likely to be a big deal in data science than overhead is."
    },
    {
        "text": "computer memory is also called ram or \u201crandom access memory.\u201d this terminology doesn\u2019t mean that there is anything actually random about it."
    },
    {
        "text": "instead, the idea is that the computer can access any region of memory equally quickly."
    },
    {
        "text": "the time required varies widely by machine, but think on the order of 100 nanoseconds to move 1 byte of data from ram to somewhere it can be processed."
    },
    {
        "text": "your computer only has a finite amount of ram."
    },
    {
        "text": "if you force the computer to operate on more data than it can fit into physical memory, two things might happen: either your program will fail or your operating system will scramble to more data between ram and the data storage medium (such as a hard disk) \u2013 this is an extremely time\u2010consuming operation called \u201cpaging.\u201d so, one of the first principles of managing memory is to not take up too much of it."
    },
    {
        "text": "within the ram, memory is not all equal."
    },
    {
        "text": "the computer will store a copy of part of ram in what\u2019s called the cache, a piece of memory that has much faster read/write time (around an order of magnitude faster)."
    },
    {
        "text": "when you try to read a byte of ram, the computer first looks in the cache."
    },
    {
        "text": "if that byte is stored there, it just reads the copy, only resorting to the actual ram in the case of a \u201ccache miss.\u201d similarly, if the program needs to modify a byte, it will first look for a cached copy of that byte and only modify that if it finds one."
    },
    {
        "text": "the computer will periodically write changes from the cache back to ram in a batch process."
    },
    {
        "text": "actually, there are usually several levels of cache, each one smaller and more rapid access than the one below it."
    },
    {
        "text": "the runtime of a program will often be dominated by how often the processor can find that data it\u2019s looking for in the top levels of the cache."
    },
    {
        "text": "together, the ram, disk, and various cache levels form the \u201cmemory hierar- chy,\u201d where each layer is faster but smaller than the ones below it."
    },
    {
        "text": "every time the computer needs a piece of data, it will look for it in the top level of the hierarchy, then the one below it, and so on, until the data is found."
    },
    {
        "text": "if a piece of data is located far down the hierarchy, then accessing it can be excruciatingly slow \u2013 partly because the access is inherently slow, but also because we just wasted time looking for the data higher up in the hierarchy."
    },
    {
        "text": "for this reason,"
    },
    {
        "text": "21 performance and computer memory 322 how often the data can be found in the high levels is often the single most important contributor to a program\u2019s performance."
    },
    {
        "text": "21.6 \u00adperformance tip: use numerical libraries when applicable operating on numpy arrays (directly or through a library such as pandas) is a lot more efficient than operating on python objects."
    },
    {
        "text": "the following code looks at the time required to increment all elements of a list of numbers, for a python list and for a numpy array."
    },
    {
        "text": "it plots the python/numpy ratio as a function of the list length, and you can see that the ratio starts high and gets worse."
    },
    {
        "text": "there are two big factors that contribute to this: \u25cf \u25cfit takes longer to add 1 to a number in pure python than in numpy."
    },
    {
        "text": "this is because python doesn\u2019t know until runtime that the number is an integer."
    },
    {
        "text": "it must check for each number what its data type is and hence how to compute \u201cx + 1.\u201d this additional overhead will make python a constant factor slower than numpy."
    },
    {
        "text": "\u25cf \u25cfthe python data structure takes up a lot more space compared to the numpy array, because it must also carry around metadata that specifies what data type each list element is."
    },
    {
        "text": "all of this extra memory means that you fill up the high levels of cache faster in python than in numpy, meaning that python does comparatively worse and worse for longer lists."
    },
    {
        "text": "import time, numpy as np, matplotlib.pyplot as plt def time_numpy(n): a = np.arange(n) start = time.time() bigger = a + 1 stop = time.time() return stop - start def time_python(n): l = range(n) start = time.time() bigger = [x+1 for x in l] stop = time.time() return stop - start n_trials = 10 ns = range(20, 500) ratios = [] for n in ns: python_total = sum([time_python(n)"
    },
    {
        "text": "21.7 performance tip: delete large structures you don\u2019t need 323 for _ in range(n_trials)]) numpy_total = sum([time_numpy(n) for _ in range(n_trials)]) ratios.append(python_total / numpy_total) plt.plot(ns, ratios) plt.xlabel(\"length of list / array\") plt.ylabel(\"python / numpy ratio\") plt.title(\"relative speed of numpy vs pure python\") plt.show() 25 20 15 python / numpy ratio 10 5 0 0 100 200 300 length of list / array relative speed of numpy versus pure python 400 500 21.7 \u00adperformance tip: delete large structures you don\u2019t need at any point in the running of your code, all objects that you have created will be vying for space in the cache."
    },
    {
        "text": "if you get rid of the ones that are no longer needed, then you allow for the ones that you will end up needing to be higher up in the memory hierarchy."
    },
    {
        "text": "the process of deleting data structures that you no longer need, and hence freeing up the memory that they were taking up, is called \u201cgarbage collection.\u201d python will do a certain amount of it automatically for you."
    },
    {
        "text": "for example, after a function finishes running all of the variables that were defined in it that can no longer be accessed are flagged for eventual deletion."
    },
    {
        "text": "21 performance and computer memory 324 however, python is very conservative about when it deletes data structures, so you can free up a lot of space by deleting objects manually."
    },
    {
        "text": "this is done using the \u201cdel\u201d keyword, as follows: >>> del my_object 21.8 \u00adperformance tip: use built-in functions when possible python\u2019s built\u2010in functions are written in very efficient c code, and calling one only incurs the one\u2010time performance hit that is inherent in calling any func- tion."
    },
    {
        "text": "the following code compares the time required to add up all of the num- bers in a list using the sum() function versus the time to add them up using a loop."
    },
    {
        "text": "on my computer, the latter takes about 14 times as long."
    },
    {
        "text": "l = range(10000000) start = time.time() _ = sum(l) stop = time.time() time_fast = stop - start start = time.time() sm = 0.0 for x in l: sm += x stop = time.time() time_loop = stop - start print \"the ratio is\", time_loop / time_fast 21.9 \u00adperformance tip: avoid superfluous function calls every time a python function is called, there is a certain amount of overhead."
    },
    {
        "text": "the following code compares the time to loop over a list and add up all of its values, versus looping over it and calling a function that adds the values."
    },
    {
        "text": "doing the latter takes (on my computer) about twice as long: the overhead required to just call a function is equal to the time required to move to the next iteration of a loop, plus the time to actually perform the addition."
    },
    {
        "text": "add_nums = lambda a, b: a+b l = range(10000000) start = time.time() sm = 0"
    },
    {
        "text": "21.12 glossary 325 for x in l: sm += x stop = time.time() time_fast = stop \u2212 start start = time.time() sm = 0 for x in l: sm = add_nums(sm, x) stop = time.time() time_func = stop \u2212 start print \"the ratio is\", time_func / time_fast 21.10 \u00adperformance tip: avoid creating large new objects a common performance mistake is to create large new objects from existing ones, when they could instead have been updated."
    },
    {
        "text": "for example, saying >>> mylist = mylist + [1, 2, 3] and >>> mylist.extend([1,2,3]) will have the same effect."
    },
    {
        "text": "however, the first one will involve creating a new list, copying all of the contents of mylist into it, and deleting the old mylist."
    },
    {
        "text": "this is an o(n) operation!"
    },
    {
        "text": "the second one is amortized o(1), and i\u2019ve seen to turn computations that always failed to ones that went through quickly."
    },
    {
        "text": "21.11 \u00adfurther reading 1 scott, m, programming language pragmatics, 4th edn, 2015, morgan kaufmann, burlington, ma."
    },
    {
        "text": "2 petzold, c, code: the hidden language of computer hardware and software, 2000, microsoft press, redmond, wa."
    },
    {
        "text": "21.12 \u00adglossary amortized complexity the average o(1) performance of an operation if it is done many times."
    },
    {
        "text": "sometimes, it maybe be o(n) or even worse, but those are rare enough that it is o(1) on average."
    },
    {
        "text": "big\u2010o notation an algorithm is o(f(n)) if its asymptotic performance as n gets big is upper\u2010bounded by f(n), times some constant factor."
    },
    {
        "text": "cache a low\u2010latency piece of memory where certain data in ram can be stored for more rapid access."
    },
    {
        "text": "21 performance and computer memory 326 cache miss when a program looks in the cache for a piece of data but doesn\u2019t find it."
    },
    {
        "text": "the program must then do the much more expensive operation of reading from normal ram."
    },
    {
        "text": "complexity the asymptotic performance of an algorithm as measured in big\u2010o notation."
    },
    {
        "text": "garbage collection deleting data structures that are no longer needed from memory, which frees up space."
    },
    {
        "text": "iterator a programming abstraction that provides values one at a time."
    },
    {
        "text": "often, only a few values are ever actually held in memory at once, so iterators make exceptionally good use of caching."
    },
    {
        "text": "327 the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. part iii the rest of this book will cover several advanced topics."
    },
    {
        "text": "some of the things we discuss really are advanced topics, which are often useful in data science but which many data scientists never need."
    },
    {
        "text": "deep learning is a good example of this."
    },
    {
        "text": "in other cases though, we will be fleshing out topics that we\u2019ve already dis- cussed."
    },
    {
        "text": "there will be less in the way of nuts\u2010and\u2010bolts code and more abstract theory."
    },
    {
        "text": "the big reason for this is that standard techniques often don\u2019t work for one reason or another."
    },
    {
        "text": "for example, you might need to adjust how a machine learning model works in order to accommodate outliers in a particular way."
    },
    {
        "text": "if this happens, you will have to revisit the assumptions that the standard tech- niques are based on and devise new techniques that work for your situation."
    },
    {
        "text": "specialized or advanced topics"
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 329 22 this chapter dovetails with the one on computer performance."
    },
    {
        "text": "it will describe in more detail the way a computer program\u2019s memory is laid out and how data is encoded in memory."
    },
    {
        "text": "it will then move on to some of the most important data structures in common use and explain how their physical layout gives rise to their performance characteristics."
    },
    {
        "text": "to make things concrete, this chapter will be taught using the c language."
    },
    {
        "text": "c is a low\u2010level language that gives you very fine\u2010grained control over how a pro- gram utilizes memory."
    },
    {
        "text": "the main python interpreter, similar to a lot of the most important code in the world, is written in c because it allows you to make things very, very efficient."
    },
    {
        "text": "this chapter doesn\u2019t count as a crash course in c, but it will give you enough to understand how the key data structures are implemented and how they form the basis of python."
    },
    {
        "text": "22.1 \u00advirtual memory, the stack, and the heap one of the most important jobs of an operating system is to allow multiple different processes on the computer to share the same physical ram."
    },
    {
        "text": "it does this by providing each process with a \u201cvirtual address space\u201d (vas), which it can use to store the data it is operating on."
    },
    {
        "text": "the process can refer to data in any location from 0 to 232\u20101 in 32\u2010bit operating systems and 0 to 264\u20101 in 64\u2010bit operating systems."
    },
    {
        "text": "each location contains exactly 1 byte of data, and the finite range of valid addresses puts a hard (but very large) upper limit on the amount of data that the process can be operating on at once."
    },
    {
        "text": "the operating system takes care of which addresses in the vas correspond to which locations in physical ram."
    },
    {
        "text": "it may also shift this mapping around, moving data between ram, the different layers of the cache, and the long\u2010term disk memory."
    },
    {
        "text": "from your program\u2019s perspective though, the vas is all there is."
    },
    {
        "text": "the process cannot just access data in the vas willy\u2010nilly."
    },
    {
        "text": "it must first request that the operating system set aside some physical ram and match it up with addresses in the vas."
    },
    {
        "text": "if the process ever tries to access a location in the computer memory and data structures"
    },
    {
        "text": "22 computer memory and data structures 330 vas that the os hasn\u2019t allocated to it, this is called a \u201csegmentation fault\u201d or \u201cseg fault.\u201d seg faults are a notorious class of bugs that are a constant headache for people who use low\u2010level languages such as c, but that are mercifully impossible in a language such as python."
    },
    {
        "text": "when space in the vas is no longer needed, the process should \u201cfree\u201d the memory, notifying the os that that range of addresses is no longer needed and it can allocate the physical ram to another process."
    },
    {
        "text": "22.2 \u00adexample c program ok, now that you know how memory is laid out, let\u2019s dive into some c code."
    },
    {
        "text": "here is the famed \u201chello, world\u201d program \u2013 it just prints \u201chello, world\u201d on the screen, but it shows you the essential parts of a c program: #include <stdio.h> #include <stdlib.h> int main(void) { printf(\"hello, world\\n\"); return 0; } the most important thing about a c program is the presence of the subrou- tine called \u201cmain.\u201d as the name suggests, this will be the primary routine that the program will start off with running."
    },
    {
        "text": "having a function called \u201cmain\u201d is optional in python, and there is nothing magical about \u201cmain\u201d; in c, this func- tion is an absolute necessity."
    },
    {
        "text": "we say \u201cint main\u201d because the main function returns the integer 0, which gets passed to the operating system when our pro- gram finishes."
    },
    {
        "text": "the first line is saying to include the stdio library, which con- tains the \u201cprintf\u201d function for printing to the screen."
    },
    {
        "text": "there are several ways to compile and run this code."
    },
    {
        "text": "on my computer (a mac, with the developer toolkits installed), it looks like the following: $ # assume the code is in a file called mycode.c $ gcc mycode.c $ # a.out is the default name of the executable file $ ./a.out hello, world 22.3 \u00addata types and arrays in memory the previous code just shows you the basic syntax of a c program, which is nice to know but which i don\u2019t want to dwell on."
    },
    {
        "text": "the really interesting thing is the way that c defines its types."
    },
    {
        "text": "22.3 data types and arrays in memory 331 let\u2019s dive into a more interesting piece of code and see what it\u2019s up to: #include <stdio.h> #include <stdlib.h> int main() { // chars, i.e."
    },
    {
        "text": "single characters char mc = \u2019a\u2019; printf(\"the char is %c\\n\", mc); // doubles, i.e."
    },
    {
        "text": "floating numbers double d = 5.7; printf(\"the double is %d\\n\", d); // arrays of ints int myarray = {1,3,7,9}; for (int i=0; i<4; i++) { printf(\"the %ith number is %i\\n\", i, myarray[i]); } return 0; } there are two big things going on here."
    },
    {
        "text": "first, there are data types other than integers floating around."
    },
    {
        "text": "there\u2019s a \u201cdouble,\u201d which is used to hold decimal numbers such as 5.9. there is also a \u201cchar,\u201d which is short for character, and it can hold a single 1\u2010byte character such as a letter or a digit."
    },
    {
        "text": "each of these atomic data types takes up a fixed number of bytes in memory."
    },
    {
        "text": "how many bytes that is will depend on your computer \u2013 my computer uses 4 bytes for an int \u2013 but the key thing is that the size is fixed."
    },
    {
        "text": "the myarray variable is what\u2019s called an \u201carray of ints,\u201d and you can have arrays of any other fixed\u2010size data type too."
    },
    {
        "text": "under the hood, the bytes of the array will just be the bytes of its constituent integers all concatenated together."
    },
    {
        "text": "in this code, we use a for\u2010loop to loop over the array and print out all of its values."
    },
    {
        "text": "the line for (int i=0; i<4; i++) is the way that c does for\u2010loops."
    },
    {
        "text": "it means that we should have an integer called i, which we use for the index of the loop."
    },
    {
        "text": "i will be initialized to 0 and will be incremented for every new loop."
    },
    {
        "text": "when i is no longer less than 4, the loop will terminate."
    },
    {
        "text": "the fact that a data type takes up a fixed number of bytes is at the corner of how arrays operate."
    },
    {
        "text": "because integers all take up the same number of bytes, and myarray is just those bytes concatenated together, it is very easy to pull out the ith element from myarray."
    },
    {
        "text": "it will just be the group of bytes that it offset by i*(number of bytes in an int) from the start of myarray."
    },
    {
        "text": "this is the way that the computer can tell where one int ends and another begins, and it is an o(1) operation to take out the ith element regardless of the size of the array."
    },
    {
        "text": "22 computer memory and data structures 332 when you use numpy, all of the numbers are stored as c arrays under the hood."
    },
    {
        "text": "this is why they take up so little space and are so quick to operate on: it\u2019s ultimately just c for\u2010loops that operate on the raw bytes."
    },
    {
        "text": "this is also why numpy arrays, unlike python lists, are constrained to only contain data of the same type; all elements must be of the same size, and a particular logical operation on them must correspond to the same operation of the underlying bytes."
    },
    {
        "text": "22.4 \u00adstructs now that you know about atomic, fixed\u2010sized types and arrays of them, let\u2019s look at the simplest compound data type: the struct."
    },
    {
        "text": "here is some example code: #include <stdio.h> #include <stdlib.h> struct person { int age; char gender; double height; }; typedef struct person person; int main() { person bob = {30, \u2019m\u2019, 5.9}; printf(\"bob is %f feet tall\\n\", bob.height); return 0; } look at the following code: struct person { int age; char gender; double height; }; typedef struct person person; the first part defines the new data type, called a \u201cstruct person.\u201d a struct person contains an integer called age, a char called gender, and a double called height."
    },
    {
        "text": "it\u2019s pretty ugly though to write \u201cstruct person\u201d in two words when it\u2019s really just a single thing."
    },
    {
        "text": "so, purely for notational convenience, the \u201ctypedef\u201d line defines the term \u201cperson\u201d to be equivalent to \u201cstruct person.\u201d"
    },
    {
        "text": "22.5 pointers, the stack, and the heap 333 to picture what\u2019s going on in the code, we will often draw the person in this code as follows: bob age gender height 30 m 5.9 similar to the atomic datatypes, a person will take up a fixed number of bytes."
    },
    {
        "text": "the bytes for a particular person will just be the bytes for their age, gender, and height, all concatenated together into a larger array of contiguous bytes."
    },
    {
        "text": "the fields will be in an order that is chosen by the compiler, possibly with some spacer bytes thrown in for performance reasons."
    },
    {
        "text": "compiled code won\u2019t make reference to the \u201cgender\u201d field of a person \u2013 it will just talk about the bytes that are offset a certain distance from the start of the person."
    },
    {
        "text": "similar to the atomic types, we can have arrays of structs in memory, and we can access their data efficiently."
    },
    {
        "text": "if you have a long array of persons and you want the gender of the nth person, then it will simply be the byte that is offset by n*(number of bytes for a person) + (offset for the gender field) from the start of the array."
    },
    {
        "text": "22.5 \u00adpointers, the stack, and the heap there is a very, very important data type that i did not mention in the previ- ous section: the pointer."
    },
    {
        "text": "a pointer is stored as an integer (i.e., it is fixed\u2010size), but it stores the index in the vas of a particular byte of memory."
    },
    {
        "text": "this allows us to refer to data in arbitrary location in the vas, which may be the start of an object or array of arbitrary size and complexity."
    },
    {
        "text": "let\u2019s look at a more complicated version of the previous code."
    },
    {
        "text": "#include <stdio.h> #include <stdlib.h> struct person { int age; char gender; double height; struct person* spouse; }; typedef struct person person; void marry(person* p1, person* p2) { p1\u2010>spouse = p2; p2\u2010>spouse = p1; }"
    },
    {
        "text": "22 computer memory and data structures 334 int main() { person jane = {28, \u2019f\u2019, 5.5, null}; person bob = {30, \u2019m\u2019, 5.9, null}; marry(&jane, &bob); printf(\"janes spouse is %f feet tall\\n\", bob.spouse\u2010>height); printf(\"bob is %f feet tall\\n\", bob.height); return 0; } the first change to notice is that the person struct now has new field called \u201cspouse,\u201d of type person."
    },
    {
        "text": "* the person* data type is not a person, but rather a pointer to some byte in memory, which is to be interpreted as the first byte in a person."
    },
    {
        "text": "this way a person can contain not only data about a single human being but also references to other human beings."
    },
    {
        "text": "the special term null means that the pointer does not actually point to anything; under the hood, it is stored as all 0s."
    },
    {
        "text": "when we first initialize our structures, we will draw them as follows: bob age gender height 30 m 5.9 spouse jane age gender height 28 f 5.5 spouse the next line of code calls the \u201cmarry\u201d function, which we can see takes in two person* as its arguments."
    },
    {
        "text": "\u201c&bob\u201d is special syntax that means a pointer to the field structure, and similarly for &jane, so we are giving it what it wants."
    },
    {
        "text": "the one thing left to note is that we use \u201c\u2010>\u201d rather than \u201c.\u201d to access the mem- ber of a structure that we have a reference to."
    },
    {
        "text": "passing the function a pointer to our structure is called \u201cpassing by reference.\u201d the result of the marry() function will be to modify the two objects in mem- ory so that they look like as follows: bob age gender height 30 m 5.9 spouse jane age gender height 28 f 5.5 spouse"
    },
    {
        "text": "22.5 pointers, the stack, and the heap 335 when we passed bob and jane into the marry() function, we did what\u2019s called \u201cpassing by reference\u201d: we passed pointers to bob and jane into the marry() subroutine, so that any changes marry() makes will happen to the original data structures in memory."
    },
    {
        "text": "when marry() finishes, the changes that it made will persist."
    },
    {
        "text": "this might be ringing a bell for you."
    },
    {
        "text": "in python, objects are always passed by reference."
    },
    {
        "text": "this means that if you pass around a mutable object such as a list or dictionary, it can be changed as follows: >>> a = {1:1, 2:4, 3:9} >>> b = a # pointer to the same dict >>> b[4] = 16 >>> a[4] 16 the alternative to passing by reference is \u201cpassing by value\u201d: whenever you want to pass a data structure into a subroutine, make a copy of the data struc- ture and put it in some place that the subroutine knows where to look."
    },
    {
        "text": "there are situations where this is more performant, because the computer no longer needs to waste time following pointers and fetching the data they point to."
    },
    {
        "text": "however, this incurs the computational cost of copying the entire (potentially massive) data structure over every time you want to pass it into a subroutine."
    },
    {
        "text": "the overwhelming reason to generally use the heap rather than the stack is that, for technical reasons involving the way subroutines work, the compiler has to know how big all data structures are at compile time."
    },
    {
        "text": "this is the case in the previous code, since we only created two person structures."
    },
    {
        "text": "but if we wanted to make an array of persons, and have a user tell us how many there should be every time the code gets run, then that array has to be put into the heap."
    },
    {
        "text": "this involves requesting from the operating system, at runt- ime, that it allocate a block of heap space of a given size."
    },
    {
        "text": "as the program runs, it will also involve telling the os when the memory is no longer needed."
    },
    {
        "text": "let\u2019s jump into some more code, giving the person struct a pointer to the array of its children."
    },
    {
        "text": "the malloc() function is used to request space in the heap \u2013 it allocates the memory somewhere in the heap and returns a pointer to the first byte."
    },
    {
        "text": "struct person { int age; char gender; double height; struct person* spouse; int n_children; struct person* children; };"
    },
    {
        "text": "22 computer memory and data structures 336 typedef struct person person; void marry(person* p1, person* p2) { p1\u2010>spouse = p2; p2\u2010>spouse = p1; } int main() { person jane = {30, \u2019m\u2019, 5.9, null}; person bob = {28, \u2019f\u2019, 5.5, null}; marry(&jane, &bob); printf(\"jane is %f feet tall\\n\", jane.height); printf(\"bobs spouse is %f feet tall\\n\", bob.spouse->height); int num_kids = 5; jane.n_children = num_kids; jane.children = (person*) malloc( num_kids*sizeof(person)); bob.n_children = num_kids; bob.children = jane.children; for (int i=0; i<num_kids; i++) { person* ith_kid = &jane.children[i]; if (i<3) ith_kid\u2010>gender=\u2019m\u2019; else ith_kid\u2010>gender=\u2019f\u2019; } int n_sons = 0; for (int i=0; i<num_kids; i++) { if (jane.children[i].gender==\u2019m\u2019) n_sons++; } printf(\"jane has %i sons\\n\", n_sons); free(jane.children); return 0; } now a person contains not just person* spouse but also person* children."
    },
    {
        "text": "children will point to an array of person structs in memory."
    },
    {
        "text": "it can be an arbi- trarily short or long array, so it will have to be located in the heap."
    },
    {
        "text": "note that when you look at the bytes themselves, there will be no way to tell when the array of persons ends, so we will also have to keep track of how long the array is, so we need the int n_children to keep track."
    },
    {
        "text": "most of the rest of the code should now make sense, with the exception of \u201cfree(jane.children).\u201d this is the way we notify the operating system that the range of memory in the vas is no longer needed."
    },
    {
        "text": "if you don\u2019t free up the memory in the heap, it\u2019s called a \u201cmemory leak,\u201d and it\u2019s a surprisingly easy bug to write."
    },
    {
        "text": "as your program runs, it will constantly"
    },
    {
        "text": "22.6 key data structures 337 allocate new variables in the heap, forgetting that they are there, until all of the memory has been taken up and the program crashes."
    },
    {
        "text": "the following code looks like it should run forever, but it will eventually fail when all the heap space is taken up: person* bob; while(true) { // creates new person in the heap and points bob at it // however, does not free up the previous person bob = malloc(sizeof(person)); } memory that has been allocated in the heap but that there is no longer a pointer to is called \u201corphaned.\u201d before we move on, i should note that my code here has been extremely sloppy."
    },
    {
        "text": "i wrote it to make it easy to understand, but please don\u2019t write profes- sional code such as this!"
    },
    {
        "text": "in particular, i\u2019ve been very cavalier about whether pointers were pointing to valid places in memory."
    },
    {
        "text": "when i allocated the chil- dren, for example, i should have made sure that all of their own spouse and children pointers were either null or pointing at valid locations."
    },
    {
        "text": "again, you generally won\u2019t have to worry about this stuff in your own coding, since a lan- guage such as python takes care of all this boilerplate for you."
    },
    {
        "text": "but if you want to understand how python works, this is what\u2019s going on under the hood."
    },
    {
        "text": "22.6 \u00adkey data structures the previous sections introduced the key concepts of fixed\u2010size structs, arrays of structs, and pointers."
    },
    {
        "text": "this section will show how those ingredients can be mixed together to create a wide range of complex, efficient data structures."
    },
    {
        "text": "i will only show you a few of them, but these structures are the basis of python and every other piece of software you use."
    },
    {
        "text": "22.6.1 strings given that strings are an atomic type in python, it might seem counterintuitive that they are not one in c. this is because strings are of variable length, so they must be created on the heap and the program must keep track of how long they are."
    },
    {
        "text": "strings are universally stored as a char* \u2013 a pointer to an array of bytes that are interpreted as characters (usually with ascii encoding)."
    },
    {
        "text": "there are two main ways to keep track of how long the array of characters is: \u25cf \u25cfthese days it is very common to wrap the char* in a structure that keeps track of how many characters there are and possibly other information."
    },
    {
        "text": "a simple example might look like this:"
    },
    {
        "text": "22 computer memory and data structures 338 struct mystring { char* characters; int n_chars; }; \u25cf \u25cfto save space, in the past it was common to signify the end of a char* by hav- ing the last byte be 0. such a char* is called \u201cnull\u2010terminated\u201d."
    },
    {
        "text": "this approach is trickier to work with and takes more time to process."
    },
    {
        "text": "however, it does take up a little less ram and used to be a lot more important than it is now."
    },
    {
        "text": "22.6.2 adjustable\u2010size arrays the problem with an array in memory is that, while it can modify its elements, its size is fixed."
    },
    {
        "text": "adding a new element will require allocating a new array of sufficient size, copying the original array over, and putting the last element in place."
    },
    {
        "text": "this is an o(n) operation!"
    },
    {
        "text": "adjustable\u2010size arrays turn it into an amortized o(1) operation."
    },
    {
        "text": "the idea is to allocate more space in the array than needed and add new elements into the extra spaces whenever they are appended."
    },
    {
        "text": "the c code might look like this: struct adjustablelist { int array_size; int n_chars; char* characters; }; and the layout of memory might be as follows: array_size n_chars characters 5 3 a b c when the allocated array does finally fill up, then yes, we will have to make a new one and copy the first one\u2019s contents over at high cost."
    },
    {
        "text": "but a common way to do this is to double the size of the array every time."
    },
    {
        "text": "in that case, the copy operations become twice as expensive as the list grows, but half as frequent."
    },
    {
        "text": "this makes a net o(1) cost amortized among all of the additions."
    },
    {
        "text": "the python list of object is, under the hood, and adjustable\u2010size array."
    },
    {
        "text": "the difference though is that it isn\u2019t an array of ints or anything \u2013 it is an array of pointers, which point to arbitrary python objects."
    },
    {
        "text": "so, the code"
    },
    {
        "text": "22.6 key data structures 339 mylist = [1, 2, []] would result in memory layout that looks something like this: array_size n_elements elements 5 3 datatype list array_size n_elements elements 0 0 datatype list data 1 datatype int data 2 datatype int 22.6.3 hash tables the key limitation of arrays is that you can only index them by integers."
    },
    {
        "text": "really, you can think of an array as a map from integers to values."
    },
    {
        "text": "in other cases though, you want to have a map that takes in a more flexible type, such as a string."
    },
    {
        "text": "this is made possible by a data structure called a hash table."
    },
    {
        "text": "the key feature of a hash table is something called a hash function."
    },
    {
        "text": "hash functions are ubiquitous in computing, so they\u2019re important to understand."
    },
    {
        "text": "a hash function f() takes in an object x of whatever input type is desired, and it outputs an integer that falls within some range from 0 to n. the hash function f has three important properties: \u25cf \u25cfit is deterministic, so that it will always return the same integer, given the same input."
    },
    {
        "text": "\u25cf \u25cfit takes o(1) time to compute f (x) for any x."
    },
    {
        "text": "\u25cf \u25cff \u201clooks random.\u201d for a random x, f (x) is about equally likely to be any of the integers."
    },
    {
        "text": "if x != y, then almost certainly f (x) != f ( y)."
    },
    {
        "text": "basically, a hash function is a way to deterministically garble its input into a single number."
    },
    {
        "text": "let\u2019s say we have a key/value pair that we want to store."
    },
    {
        "text": "the key idea of a hash table is that we still store our data in an array, but the index in the array is given by the hash of the key."
    },
    {
        "text": "this means that looking up an element is still o(1) time, since calculating the hash function is o(1)."
    },
    {
        "text": "typically in a hash table, the under- lying array is an array of pointers, and a pointer will be null if there is no"
    },
    {
        "text": "22 computer memory and data structures 340 element that got hashed to that location in the array."
    },
    {
        "text": "as with the aforemen- tioned adjustable\u2010size arrays, the pointers can point to objects of arbitrary and perhaps differing sizes."
    },
    {
        "text": "hash tables play a central role in python."
    },
    {
        "text": "dictionaries are implemented as hash tables under the hood and so is the namespace, which maps your variable names to the objects in memory that contain the data."
    },
    {
        "text": "ditto for instances of user\u2010defined classes that you create."
    },
    {
        "text": "it has been said that the entire python language is just syntactic sugar around hash tables, and it\u2019s really pretty true."
    },
    {
        "text": "it will periodically happen for two distinct x and y, and we will have hash(x) = hash(y)."
    },
    {
        "text": "this inconvenience leads to two caveats in practical imple- mentations of hash tables: \u25cf \u25cfgenerally, we can\u2019t just store the values in the hash table."
    },
    {
        "text": "we must show the key and the values at the location in the array, so that the raw keys them- selves can be compared."
    },
    {
        "text": "\u25cf \u25cfyou could try to hash a key x to a location in the array that already has a dif- ferent key y. sometimes, this is solved by having a protocol for how to look around and find an empty cell in the array."
    },
    {
        "text": "other times, the cells in the array point to yet another structure, whose purpose is to keep track of whatever values mapped to that cell."
    },
    {
        "text": "at some point, a hash table starts to fill up, so that most cells in the underly- ing array are holding many different values."
    },
    {
        "text": "at this point, we must go through a very expensive operation called \u201crehashing.\u201d we allocate a new, larger hash table in memory."
    },
    {
        "text": "it will need a new hash function, which maps values into a larger range of integers."
    },
    {
        "text": "then, we go through all of the key/value pairs in the original table and put them into the new, larger one."
    },
    {
        "text": "this is an o(n) operation, but if you increase the array size by a constant factor every time, then adding new elements becomes amortized o(1)."
    },
    {
        "text": "22.6.4 linked lists a linked list is a lightweight way to implement a list of objects of some known type."
    },
    {
        "text": "you have a struct that has two fields: an instance of whatever it is that you\u2019re making a list of and a pointer to the next element in the list."
    },
    {
        "text": "the last element in the list points to null."
    },
    {
        "text": "for example, here is some potential code for a linked list of integers: struct linkedlistnode { int value; linkedlistnode* next; };"
    },
    {
        "text": "22.6 key data structures 341 here is what the list [5, 4, 7, 8] would look like when store using it: next value 5 next value 4 next value 7 next value 8 linked lists have the huge disadvantage that it is an o(i) operation to find the ith element, rather than o(1)."
    },
    {
        "text": "however, the big advantage of linked lists (besides their simplicity) is that you can add a new element to them in o(1) time, if you have a pointer to the location where you want to put it."
    },
    {
        "text": "this is pure o(1) time, not amortized."
    },
    {
        "text": "the pseudocode looks like this: input: linkedlistnode* currentnode, int n algorithm: linkedlistnode* newnode = new linkedlistnode(n) newnode->next = currentnode->next currentnode->next = newnode and the operation in memory will look something like this: value 4 next currentnode value 4 next oldnextnode value 4 next newnode value 4 next currentnode value 4 next oldnextnode note that the second line of the algorithm must happen before the third."
    },
    {
        "text": "if we had switched the order, we would have pointed currentnode at newnode, and then pointed newnode back at itself, as follows:"
    },
    {
        "text": "22 computer memory and data structures 342 value 4 next newnode value 4 next currentnode value 4 next oldnextnode now the list loops back on itself perversely, making it seem like it is infinitely long, and the remainder of the original list has been orphaned and is floating round in memory."
    },
    {
        "text": "it is notoriously easy to screw up pointer manipulations, and that is one of the things that make high\u2010level languages such as python very appealing."
    },
    {
        "text": "on the other hand, if you\u2019re willing to put in the effort to get all the details right, then c code can achieve performance that python can only dream of."
    },
    {
        "text": "22.6.5 binary search trees there is one other pointer\u2010based data structure i would like to introduce you to: the binary search tree (bst)."
    },
    {
        "text": "in a bst, every node contains a numerical value and has two children rather than one, which we typically call \u201cleft\u201d and \u201cright.\u201d we make sure that at every point in time, the children on the right of every node have values that are greater than or equal to the node in question, and the nodes on the left have smaller or equal values."
    },
    {
        "text": "a bst might look like this: right value 4 left right value 7 left right value 2 left right value 9 left"
    },
    {
        "text": "22.8 glossary 343 assuming that every node in the tree has roughly as many descendents to its left as to its right (a so\u2010called balanced tree), then it is an o(log(n)) operation to see whether a given number is in the tree or add a new one."
    },
    {
        "text": "again, this is not amortized performance."
    },
    {
        "text": "the search algorithm, to find whether a value is in the bst, would look like this in pseudocode: bst_search input: node* currentnode, int n algorithm: if currentnode==null: return false elif n < currentnode->value: return bst_search(currentnode->left, n) else: return bst_search(currentnode->right, n) there is so much more to say about pointers, arrays, and the magic that you can do with them."
    },
    {
        "text": "i would love to get into it, but that\u2019s partly because i spent many years cutting my teeth on low\u2010level coding."
    },
    {
        "text": "in the daily practice of a data scientist, you don\u2019t need to use this information, so use this chapter as a primer in case you ever find yourself writing low\u2010level algorithms."
    },
    {
        "text": "22.7 \u00adfurther reading 1 petzold, c, code: the hidden language of computer hardware and software, 2000, microsoft press, redmond, wa."
    },
    {
        "text": "2 scott, m, programming language pragmatics, 4th edn, 2015, morgan kaufmann, burlington, ma."
    },
    {
        "text": "3 mcdowell, g, cracking the coding interview: 189 programming questions and solutions, 6th edn, 2015, careercup."
    },
    {
        "text": "22.8 \u00adglossary binary tree a data structure where each node has pointers to two children, on its left and right."
    },
    {
        "text": "hash table a data structure that maps hashable keys to values."
    },
    {
        "text": "it does that by hashing the keys into the range [0, n] and using the hash as an index in a length\u2010n array."
    },
    {
        "text": "hash function a deterministic function that garbles a key x into an integer that is probably distinct from hash(y), for y != x. heap a range in the virtual address space where dynamically allocated memory is located."
    },
    {
        "text": "22 computer memory and data structures 344 linked list a data structure where each node has a pointer to the next node."
    },
    {
        "text": "the last node has a null pointer."
    },
    {
        "text": "orphaned memory memory in the vas that there is no longer a pointer to."
    },
    {
        "text": "this makes it impossible to free the memory."
    },
    {
        "text": "memory leak part of a program that clears pointers to memory in the heap without freeing the memory."
    },
    {
        "text": "passing by value passing a copy of a data structure into a subroutine."
    },
    {
        "text": "that way changes made to the copy do not to effect the original version."
    },
    {
        "text": "passing by reference passing a pointer to a data structure into a subroutine."
    },
    {
        "text": "that way changes made to the object do effect the original object."
    },
    {
        "text": "pointer the address of a location in the virtual address space."
    },
    {
        "text": "that address marks the first byte in the data object being pointed to."
    },
    {
        "text": "stack a range in the virtual address space that is involved in low\u2010level subroutines."
    },
    {
        "text": "struct a data type that combines several data fields of more primitive, fixed\u2010size types."
    },
    {
        "text": "the struct is itself fixed\u2010size, and its byte representation is just the concatenated byte representation of its constituent data fields."
    },
    {
        "text": "virtual address space the array of bytes that a computer process has for storing all of its data that it operates on."
    },
    {
        "text": "the operating system handles the mapping between the logical addresses and their physical location in ram."
    },
    {
        "text": "vas short for virtual address space."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 345 23 this section will talk about two topics that form the mathematical and compu- tational underpinnings of much of what we\u2019ve covered in this book."
    },
    {
        "text": "the goal is to help you frame novel problems in a way that makes theoretical sense and that can realistically be solved with a computer."
    },
    {
        "text": "23.1 \u00admaximum likelihood estimation maximum likelihood estimation (mle) is a very general way to frame a large class of problems in data science: \u25cf \u25cfyou have a probability distribution characterized by some parameters that we\u2019ll call \u03b8. in a regular normal distribution, for example, \u03b8 would consist of just two numbers: the mean and the standard deviation."
    },
    {
        "text": "\u25cf \u25cfyou assume that a real\u2010world process is described by a probability distribu- tion from this family, but you do not make any assumptions about \u03b8."
    },
    {
        "text": "\u25cf \u25cfyou have a dataset called x that is drawn from the real\u2010world process."
    },
    {
        "text": "\u25cf \u25cfyou find the \u03b8 that maximizes the probability p(x|\u03b8)."
    },
    {
        "text": "a large fraction of machine learning classification and regression models all fall under this umbrella."
    },
    {
        "text": "they differ widely in the functional form they assume, but they all assume one at least implicitly."
    },
    {
        "text": "mathematically, the process of \u201ctraining the model\u201d really reduces to calculating \u03b8. in mle problems, we almost always assume that the different data points in x are independent of each other."
    },
    {
        "text": "that is, if there are n data points, then we assume p x p x i n i | | \u03b8 \u03b8 ( ) = ( ) =\u220f 1 in practice, it is often easier to find \u03b8 that maximizes the log of the probabil- ity, rather than the probability itself."
    },
    {
        "text": "taking the log turns multiplication into division, so that we are minimizing maximum likelihood estimation and optimization"
    },
    {
        "text": "23 maximum likelihood estimation and optimization 346 log | log | p x p x i n i \u03b8 \u03b8 ( ) ( ) = ( ) ( ) =\u2211 1 there are two significant problems with mle in general."
    },
    {
        "text": "the first is overfitting; especially if your dataset is small, you run the very real risk of getting parameters that predict your specific data very well, but generalize terribly."
    },
    {
        "text": "the most popular alternative to this is a bayesian approach, which is generally much harder to understand, more complicated to implement, and slower to run."
    },
    {
        "text": "plus, bayesian approaches involve the very touchy\u2010feely issue of how you pick your prior."
    },
    {
        "text": "the other problem with mle is the logistical problem of actually calculating the optimal \u03b8. in some cases, there is a tidy closed\u2010form solution \u2013 those cases tend to be the most historically important ones, if only because people could actually solve the problems back in the days of paper and pencil."
    },
    {
        "text": "in general though, there is no closed\u2010form solution to an mle problem, and we must rely on numerical algorithms that give us good approximations."
    },
    {
        "text": "this falls under the umbrella of numerical optimization, the other topic of this chapter."
    },
    {
        "text": "23.2 \u00ada simple example: fitting a line to illustrate, let\u2019s reproduce simple least\u2010squares line fitting in the mle con- text."
    },
    {
        "text": "in this case, there is, luckily, a closed\u2010form solution that we can derive with some algebra and calculus."
    },
    {
        "text": "let\u2019s assume that y is a linear function of x plus some random noise, and let the noise be normally distributed."
    },
    {
        "text": "then we see that the probability density of y, for a given value of x, is f y mx b y y ( ) = \u2212 + ( )\u2212 ( ) \uf8f1 \uf8f2\uf8f4 \uf8f3\uf8f4 \uf8fc \uf8fd\uf8f4 \uf8fe\uf8f4 exp 2 2 2\u03c3 the mle expression we want to minimize is l p x p x p x m b i n i i n i i = ( ) ( ) = ( ) ( ) = ( ) ( ) = = = = \u2211 \u2211 log | log | log | , , , \u03b8 \u03b8 \u03bc \u03c3 1 1 1 2 2 2 n i i mx b y \u2211 \u2212 + ( )\u2212 ( ) \uf8f1 \uf8f2\uf8f4 \uf8f3\uf8f4 \uf8fc \uf8fd\uf8f4 \uf8fe\uf8f4 \uf8eb \uf8ed \uf8ec \uf8ec \uf8f6 \uf8f8 \uf8f7 \uf8f7 log exp \u03c3"
    },
    {
        "text": "23.2 a simple example: fitting a line 347 in order for this expression to be at its maximum, the derivates of l with respect to m and b must be 0. so, we say 0 0 1 2 1 = \u2202 \u2202 = \u2202 \u2202 + ( )\u2212 ( ) \u221d + ( )\u2212 ( ) = \u2202 \u2202 = = = = \u2211 \u2211 l m m mx b y mx b y x l b i n i i i n i i i i * 1 2 1 n i i i n i i b mx b y mx b y \u2211 \u2211 \u2202 \u2202 + ( )\u2212 ( ) \u221d + ( )\u2212 ( ) = if we divide each side by n, these equations then become 0 1 1 0 1 2 1 = \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7+ + \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = + + = = \u2211 \u2211 m n x b n x y m b y x x i n i i n i i we can solve these equations to find the complicated (but closed\u2010form!)"
    },
    {
        "text": "solutions m n x y y n x b y m x x x i n i i i n i = \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7\u2212 \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7\u2212 = \u2212 \u2212 = = \u2211 \u2211 1 1 1 1 2 2 * this process illuminates plain old vanilla least squares."
    },
    {
        "text": "it is no longer just a standard black\u2010box technique; it is the right technique if you make certain assumptions about the world."
    },
    {
        "text": "in particular, we assumed that y is a linear func- tion of x plus some normally distributed noise."
    },
    {
        "text": "but is that really the correct assumption?"
    },
    {
        "text": "if x is somebody\u2019s income and y is the price of their home, then i might want to use a different model, where the noise term has standard deviation propor- tional to x. this is because a difference of $20k in home price is very significant if you make $50k a year, but small if you make $500k."
    },
    {
        "text": "in that case, i would want to use = \u2212 + ( )\u2212 ( ) = \u2212 + ( )\u2212 ( ) = = \u2211 \u2211 i n i i i n i i mx b y mx b y 1 2 2 2 1 2 2 1 2 \u03c3 \u03c3"
    },
    {
        "text": "23 maximum likelihood estimation and optimization 348 l mx b y x i n i i i = \u2212 + ( )\u2212 ( ) ( ) \uf8f1 \uf8f2\uf8f4 \uf8f3\uf8f4 \uf8fc \uf8fd\uf8f4 \uf8fe\uf8f4 \uf8eb \uf8ed \uf8ec \uf8ec \uf8f6 \uf8f8 \uf8f7 \uf8f7 =\u2211 1 2 2 2 log exp \u03b2 where the standard deviation in home price will be \u03b2 times a person\u2019s income."
    },
    {
        "text": "this expression probably doesn\u2019t have a closed\u2010form solution, but you can get an approximate numerical one."
    },
    {
        "text": "an alternative variation on least squares would be to use something other than a normal distribution."
    },
    {
        "text": "in real life, people sometimes buy houses that are well above or below their means, so we might want a distribution that allows more outliers than the normal distribution."
    },
    {
        "text": "the sky is the limit with the variations you can take."
    },
    {
        "text": "but the bottom line is this: rather than blindly trusting standard techniques, mle allows us to take an understanding of the real world, translate it into probabilistic models, and bake those directly into the model."
    },
    {
        "text": "23.3 \u00adanother example: logistic regression a more complicated example is the logistic regression classifier."
    },
    {
        "text": "recall that in logistic regression, x will be a vector and y will be either 0 or 1. the score that it gives out is p x w x b w x b ( ) = \u22c5 + ( ) = + \u22c5 + ( ) \u03c3 1 1 exp where w is a vector and b is a constant."
    },
    {
        "text": "logistic regression is based on the probability model that p(x) is the proba- bility that a real\u2010world point at x will be a 1, rather than a 0. conversely, 1 \u2212 p(x) is the probability that it will be a 0. given the overall set of x values in our training data, the likelihood of the particular y values that we saw is l p x p x y i y i i i = ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u2212( ) ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 = = \u220f \u220f 1 0 1 the training phase in logistic regression finds the w and b that maximize this expression."
    },
    {
        "text": "23.4 \u00adoptimization optimization is a way to solve the following problem: you are given a function f(\u03b8), where \u03b8 is a vector of d dimensions."
    },
    {
        "text": "you might also be given several functions gi(\u03b8)."
    },
    {
        "text": "find the \u03b8 that minimizes f(\u03b8), consistent with gi(\u03b8) \u2264 0 for all i."
    },
    {
        "text": "23.4 optimization 349 in my experience, students go through three distinct stages when first learn- ing about optimization: \u25cf \u25cfwhat the heck?"
    },
    {
        "text": "the concept of optimization is so general as to be meaning- less."
    },
    {
        "text": "where\u2019s the beef?"
    },
    {
        "text": "\u25cf \u25cfoh my gosh, everything is an optimization problem!"
    },
    {
        "text": "this is the solution to life, the universe, and everything!"
    },
    {
        "text": "\u25cf \u25cfok, it turns out that lots of things aren\u2019t optimization problems."
    },
    {
        "text": "and most of the ones that are optimization problems in principle can\u2019t be solved in prac- tice."
    },
    {
        "text": "but there are still a lot of problems that can."
    },
    {
        "text": "numerical optimization is the way that many mle problems get solved in the real world, but it is useful in many other domains of application as well."
    },
    {
        "text": "as a simple example, mle is prone to overfitting, as we discussed previously."
    },
    {
        "text": "this can be ameliorated by adding in penalty terms that punish parameters that are likely to be overfitted."
    },
    {
        "text": "with logistic regression, for example, we can add in a term to punish large feature weights: l p x p x w y i y i i i i i = ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u2212( ) ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7\u2212 = = \u220f \u220f \u2211 1 0 1 \u03bb where \u03bb is a positive parameter that you set."
    },
    {
        "text": "this is no longer a valid mle problem, but it still fits perfectly well under the umbrella of optimization."
    },
    {
        "text": "in fact, it is now lasso regression."
    },
    {
        "text": "by taking the absolute value of the feature weights, lasso regression pun- ishes small weights and large weights moderately."
    },
    {
        "text": "however, we might also want to punish large weights very harshly to make sure that no one feature tends to domain the classification."
    },
    {
        "text": "this can be done by adding a penalty term that squares the weights, rather than taking their absolute value: l p x p x w w y i y i i i i i i i = ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u2212( ) ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7\u2212 \u2212 = = \u220f \u220f \u2211 \u2211 1 0 1 2 2 1 \u03bb \u03bb this variant is known as elastic net regularization."
    },
    {
        "text": "most of the time as a data scientist, you can use off\u2010the\u2010shelf algorithms and just know how to interpret each of them individually."
    },
    {
        "text": "if you have to formulate your own approaches though, optimization is one of the most powerful tools available for thinking critically about the problem."
    },
    {
        "text": "any good numerical computing package will have numerical optimization routines that you can treat as a black box."
    },
    {
        "text": "you input a handle to the function to be minimized and possibly an initial guess as the optimal value."
    },
    {
        "text": "the optimizer will, through some black magic, gradually adjust the initial guess until it is approximately at the optimal value and return it."
    },
    {
        "text": "all numerical optimization"
    },
    {
        "text": "23 maximum likelihood estimation and optimization 350 routines work this way, by generating a sequence of guesses that (hopefully!)"
    },
    {
        "text": "converge to the best solution."
    },
    {
        "text": "the problem is that this doesn\u2019t always work."
    },
    {
        "text": "there are two main ways that optimization fails: \u25cf \u25cfthe sequence of guesses will shoot off infinitely in some direction, rather than converging to an optimal value."
    },
    {
        "text": "\u25cf \u25cfthe guesses will converge on a value, but it is a \u201clocal optimum\u201d \u2013 a guess that is better than anything nearby it, but worse than another guess at some other location."
    },
    {
        "text": "it\u2019s pretty clear when the first of these happens, and your best bet in that case is to restart it using a different initial guess."
    },
    {
        "text": "the second problem is much more insidious."
    },
    {
        "text": "the easiest thing to do is try a variety of different initial guesses and see if all the best\u2010performing ones con- verge to the same place."
    },
    {
        "text": "if so, then this is likely the optimum."
    },
    {
        "text": "then again, it might not be \u2013 you can\u2019t really know."
    },
    {
        "text": "the next section will give you an intuitive idea of what\u2019s going on under the hood when a numerical optimization routine runs."
    },
    {
        "text": "it will also explain what\u2019s called \u201cconvex optimization,\u201d which is a large class of optimization problems where algorithms are guaranteed to converge on the correct solution."
    },
    {
        "text": "if you can figure out a way to formulate your optimization problem so that it is con- vex, you\u2019re golden."
    },
    {
        "text": "23.5 \u00adgradient descent and convex optimization when trying to understand how an optimization algorithm works, the best picture to have in your mind is to that of walking over a range of hills with your eyes blindfolded."
    },
    {
        "text": "the height of the hills is the value of your objective function, and the x/y coordinates of where you are standing are the components of your guess vector \u2013 we are implicitly assuming that d = 2 in this example."
    },
    {
        "text": "there are two questions to answer for every step in the optimization algorithm: \u25cf \u25cfwhat direction should you move?"
    },
    {
        "text": "presumably, you want to move downhill, but there are a variety of ways you could decide the exact path."
    },
    {
        "text": "\u25cf \u25cfhow far should you move in that direction?"
    },
    {
        "text": "pretty much every optimization algorithm boils down to some methodology for answering these two questions."
    },
    {
        "text": "answer them well, and the hope is that you will find yourself at the bottom of the valley in as few time steps as possible."
    },
    {
        "text": "before we go any further, let me define a bit of terminology: \u25cf \u25cfx will denote a vector of real numbers of length d that we are plugging into the objective function."
    },
    {
        "text": "23.5 gradient descent and convex optimization 351 \u25cf \u25cff() will be the objective function."
    },
    {
        "text": "\u25cf \u25cfthe \u201cfeasible region\u201d is the set of all x for which all of the gi(x) are \u22640."
    },
    {
        "text": "\u25cf \u25cf\u2207( ) f x will be the gradient of f at x. if you haven\u2019t seen gradients before, they\u2019re a multivariable calculus thing."
    },
    {
        "text": "\u2207( ) f x will be a vector pointing in the direction in which it is increasing most steeply, starting from x; basically, the gradient points directly uphill."
    },
    {
        "text": "the longer the gradient vector, the steeper the increase is."
    },
    {
        "text": "\u25cf \u25cfas the numerical algorithm progresses, x0, x1, ... will be the best guesses at each step."
    },
    {
        "text": "\u25cf \u25cfx* will be the solution to the problem."
    },
    {
        "text": "if the algorithm goes well, then |xn+1 \u2212 xn| and |x* \u2212 xn| will reach 0 as n gets large."
    },
    {
        "text": "if we imagine that x is two dimensional, hopefully the progression of the algorithm will look something like the following: x* x1 x0 here are some of the ways that a direction gets picked: \u25cf \u25cfin some cases, you are able to break out some calculus and derive a closed formula for the gradient of the objective function."
    },
    {
        "text": "if you multiply the gradi- ent by \u20101, then you have the direction that is pointing the most steeply down- hill."
    },
    {
        "text": "this is called \u201cgradient descent,\u201d and you might remember it from multivariable calculus."
    },
    {
        "text": "\u25cf \u25cfin general, you can\u2019t get the gradient \u2013 all you can do is evaluate the objective function as any point of your choice."
    },
    {
        "text": "in this case, a naive implementation would be to take each dimension in turn, increment your guess for that dimension by a tiny amount, and recalculate the objective function."
    },
    {
        "text": "then, move in the direction of whichever dimension decreased the objective func- tion the most."
    },
    {
        "text": "this direction will hopefully be relatively close to the gradient."
    },
    {
        "text": "23 maximum likelihood estimation and optimization 352 \u25cf \u25cfif you\u2019re really lucky, you can use calculus to derive the \u201chessian\u201d of the objective function, which is a matrix giving all of its second partial deriva- tives."
    },
    {
        "text": "this is also a calculus concept, somewhat more advanced than the gradient."
    },
    {
        "text": "between the gradient and the hessian, you can approximate the objective function locally as a paraboloid and move toward the center of the paraboloid."
    },
    {
        "text": "\u25cf \u25cfthe gradient at xn+1 is probably pretty close to the gradient at xn."
    },
    {
        "text": "it is compu- tationally expensive to sample points around xn+1 in the hopes of estimating the gradient."
    },
    {
        "text": "but maybe we can keep a running estimate, evaluating f() at only a few points in the neighborhood of xn+1 and using that to update our estimated gradient at xn."
    },
    {
        "text": "\u25cf \u25cfin a similar way, we can maintain a running estimate of the hessian."
    },
    {
        "text": "many algorithms will start with their best attempt at gradient descent, but gradually switch over to using the hessian as they near the optimal value (the convergence tends to be faster that way)."
    },
    {
        "text": "as to how far we should travel, we typically want to head in the direction we\u2019ve chosen for as long as it\u2019s traveling downhill."
    },
    {
        "text": "intuitively, we would expect that the distance will be farther if it is currently going downhill more steeply."
    },
    {
        "text": "for this reason, a common technique is to take our estimate of the gradient and use it as the initial step size and try out xn + \u2207( ) f xn ."
    },
    {
        "text": "if that lowers f(), then try out xn + \u03b1\u2207( ) f xn , where \u03b1 is a parameter greater than 1. keep multiplying the step size by \u03b1 for as long as f is decreasing."
    },
    {
        "text": "conversely, if f was higher xn + \u2207( ) f xn , then divide the step size by alpha until f starts to decrease."
    },
    {
        "text": "in terms of step size, there are two big problems that we want to avoid."
    },
    {
        "text": "the first is to take small steps and decrease their size so quickly that we take a long time to converge."
    },
    {
        "text": "if we decrease them too quickly, we might even converge to a point that falls short of the minimum as in this picture: x0 x1 x2 x3 the second problem is if we are regularly overshooting the minimum, as follows:"
    },
    {
        "text": "23.6 convex optimization 353 x0 x2 x4 x5 x3 x1 this is a highly inefficient way to reach the minimum."
    },
    {
        "text": "and if we overshoot too far, we can actually increase f()!"
    },
    {
        "text": "even with a solid algorithm that makes all these choices wisely, disaster can still occur."
    },
    {
        "text": "the algorithm will generally converge to some locally optimal solu- tion, but nothing i have said here guarantees that it will be the best solution."
    },
    {
        "text": "if you start off with x0 being close to a local optimum, then your algorithm will probably fall into its pit, as in this picture: x0 x1 x2 x3 x* if we want to guarantee that this won\u2019t occur, then we will have to look at convex optimization."
    },
    {
        "text": "23.6 \u00adconvex optimization an optimization problem is said to be \u201cconvex\u201d if it satisfies certain mathemati- cal constraints that guarantee any reasonable numerical algorithm will"
    },
    {
        "text": "23 maximum likelihood estimation and optimization 354 converge to x* if it exists."
    },
    {
        "text": "i will give you the technical definition of a convex problem in a second, but intuitively, it means the following: \u25cf \u25cfthe objective function is \u201cbowl\u2010shaped.\u201d \u25cf \u25cfif you follow a straight line from any point in the feasible region to any other point in the feasible region, you will stay entirely within the feasible region."
    },
    {
        "text": "the first constraint means that the function has no local minima expect for the single global minimum, sitting in the middle of the \u201cbowl.\u201d you can\u2019t fall into a trap like in the previous figure."
    },
    {
        "text": "the second constraint means that there is a line from xn to x* that lies within the feasible region."
    },
    {
        "text": "if that weren\u2019t the case, then you could have a situation like the following: x0 x* where i\u2019ve drawn the border of the feasible region in a solid line."
    },
    {
        "text": "gradient descent will probably bring our algorithm up against the border of the feasible region, and it will stop there."
    },
    {
        "text": "the function f(x) does not have a local minimum, but it does if we limit ourselves to this nonconvex feasible region."
    },
    {
        "text": "if f is bowl\u2010shaped, then it is called \u201cconvex.\u201d if the feasible region meets our criteria, then it is also called \u201cconvex.\u201d yes, people overuse this word."
    },
    {
        "text": "a region of space is convex if for any two points x and y in it, the line segment from x to y lies in the region as well."
    },
    {
        "text": "think of it this way: if you wrapped saran wrap tightly over the region, the saran wrap would touch the region at every point."
    },
    {
        "text": "the function f is said to be convex if, for any points x and y in space and any number \u03b1 between 0 and 1, f x y f x f y \u03b1 \u03b1 \u03b1 \u03b1 + \u2212 ( ) ( ) \u2264 ( )+ \u2212 ( ) ( ) 1 1"
    },
    {
        "text": "23.8 further reading 355 convexity is a very, very restrictive condition."
    },
    {
        "text": "if you write out a random function, it is very unlikely to be convex."
    },
    {
        "text": "a lot of people spend a lot of time reformulating nonconvex problems into equivalent ones that are convex or proving that certain classes of problems are convex."
    },
    {
        "text": "theoretically, there\u2019s almost no middle ground: if the problem is convex, then any half\u2010way decent optimization algorithm will converge to the right answer (the good algorithms will just do it faster)."
    },
    {
        "text": "if it is not convex, then you have no guarantees."
    },
    {
        "text": "that doesn\u2019t mean that you should give up hope though if your problem isn\u2019t convex!"
    },
    {
        "text": "much of the work in machine learning involves problems that aren\u2019t convex (or at least, nobody has proven that they are), and a lot of excellent work has been done."
    },
    {
        "text": "it\u2019s just that in place of rock\u2010solid theorems, we must make do with rules of thumb and empirical findings."
    },
    {
        "text": "for some classes of problems, opti- mization algorithms work shockingly well for something that is not convex, and it\u2019s an open question why."
    },
    {
        "text": "23.7 \u00adstochastic gradient descent in many optimization problems, the objective function is a sum of many much simpler functions."
    },
    {
        "text": "for example, in mle, we often try to optimize log | log | p x p x i n i \u03b8 \u03b8 ( ) ( ) = ( ) ( ) =\u2211 1 where \u03b8 is the collection of parameters for our model (of which there are prob- ably only a few), and n is the (often very large) number of points in our training dataset."
    },
    {
        "text": "calculating the gradient of log | p x \u03b8 ( ) ( ) is probably computationally prohibitive."
    },
    {
        "text": "the idea of stochastic gradient descent (sgd) is that we can approximate the gradient by picking a single value for i and then calculating the gradient of log | p xi \u03b8 ( ) ( ) ."
    },
    {
        "text": "each step in the algorithm picks a new i, either by selecting randomly or sweeping through the entire dataset."
    },
    {
        "text": "a variation of sgd is \u201cmini batch\u201d sgd."
    },
    {
        "text": "in this, we take a selection of more than one data point and calculate the gradient based on them."
    },
    {
        "text": "the idea is to pick enough points that we get a better idea of the actual gradient at a point, but few enough that calculating the gradient is still computationally feasible."
    },
    {
        "text": "this tends to converge faster than vanilla sgd."
    },
    {
        "text": "23.8 \u00adfurther reading 1 boyd, s & vandenberghe, l, convex optimization, 2004, cambridge university press, cambridge, uk."
    },
    {
        "text": "2 nocedal, j & wright, s, numerical optimization, 2nd edn, 2006, springer, new york, ny."
    },
    {
        "text": "23 maximum likelihood estimation and optimization 356 23.9 \u00adglossary convex optimization an optimization problem where the feasible region and the objective function are both convex."
    },
    {
        "text": "this guarantees that most optimization algorithms will converge to the global optimum."
    },
    {
        "text": "convex function intuitively, this is a function that is \u201cbowl\u2010shaped.\u201d convex region a region of d\u2010dimensional space such that if two points x and y are within the region, then the line segment between x and y is also in the region."
    },
    {
        "text": "global optimum the location in a feasible region that minimizes the objective function."
    },
    {
        "text": "gradient say, you have a function f that takes in a d\u2010dimensional vector x and outputs a number."
    },
    {
        "text": "then, the gradient of f at x is a vector pointing in the direction in which f is increasing most steeply."
    },
    {
        "text": "the longer the gradient is, the more steeply f increases in that direction."
    },
    {
        "text": "the gradient of a function can often be calculated using the tools of multivariable calculus."
    },
    {
        "text": "gradient descent an optimization algorithm that attempts to calculate the gradient of the objective function at a guessed point x, then reduce the objective function by traveling along that gradient to a new point, and repeat."
    },
    {
        "text": "generally, this process will converge to a locally optimal solution."
    },
    {
        "text": "optimization a field in numerical computing that focuses on finding the input vector that minimizes some objective function, possibly subject to certain constraints on the input."
    },
    {
        "text": "objective function a function that takes in a vector and outputs a real number."
    },
    {
        "text": "the goal in optimization is to find the input that minimizes the objective function."
    },
    {
        "text": "local optimum a point x in the feasible region where the objective function is lower than (or equal to) its value at any point in a certain radius of x. it may not, however, be the best point in the entire feasible region."
    },
    {
        "text": "maximum likelihood estimation fitting a probability distribution to real\u2010world data by setting the parameters of the distribution (such as the mean and standard deviation, in the case of fitting a gaussian) so as to maximize the probability of observing the data we got."
    },
    {
        "text": "mle maximum likelihood estimation stochastic gradient descent an optimization method where we estimate the gradient by taking a random selection of data points and calculating the gradient only based on those points."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 357 24 this chapter will dive deeper into some more cutting\u2010edge machine learning algorithms."
    },
    {
        "text": "in the first part of this book, i had a chapter on machine learning classification, so why didn\u2019t i put this material there?"
    },
    {
        "text": "my rule was that the classifiers in the earlier chapter could be used pretty much as black boxes."
    },
    {
        "text": "yes, it was certainly possible to dissect them, analyze why they worked well or poorly, use them to extract fancier features, and so on."
    },
    {
        "text": "but you could just use them out of the box, and they would probably work pretty well."
    },
    {
        "text": "the classifiers in this chapter are something of a dark art and really designed more for machine learning specialists than normal data scientists."
    },
    {
        "text": "they have a lot of internal structure, which must be planned out by the user, there are many different parameters to tune (arbitrarily many, depending on how com- plex the structure is), and there is no good precanned answer about how to make those decisions."
    },
    {
        "text": "you will have to think critically about the problems you plan on solving, design a classifier that is appropriate for those problems, and then experiment with different parameters and layouts to find what works best."
    },
    {
        "text": "and if you screw it up somehow, your classifier is liable to per- form horribly."
    },
    {
        "text": "in exchange for this extra work, you get to use the most powerful types of classifiers in the world today."
    },
    {
        "text": "this is the kind of stuff going on the heart of google and microsoft."
    },
    {
        "text": "it is used for hard problems such as identifying the peo- ple in a random facebook photo, siri figuring out what you want based on a garbled command spoken into your phone, and deciding whether an image is safe enough to show random people on an image search."
    },
    {
        "text": "soon, algorithms such as this will probably be driving your car."
    },
    {
        "text": "the two types of classifiers i will discuss here are deep learning and bayesian networks."
    },
    {
        "text": "deep learning is a broad class of next\u2010generation neural networks, which were made possible by several advancements in the numerical tech- niques required for training them."
    },
    {
        "text": "the second class of models is bayesian networks."
    },
    {
        "text": "advanced classifiers"
    },
    {
        "text": "24 advanced classifiers 358 since these are deep topics, which data scientists rarely need anyway, the coverage in this chapter will be relatively cursory."
    },
    {
        "text": "i will focus mostly on exam- ple code that illustrates the key ideas and how to play around with them."
    },
    {
        "text": "finally, i should give you a personal qualifier."
    },
    {
        "text": "i want to make it clear that i am not an expert in these techniques."
    },
    {
        "text": "data science is a big field, and the subjects in this chapter are things that i haven\u2019t had a lot of occasion to use when solving real problems."
    },
    {
        "text": "but i do know enough to give you the key ideas, show you how to write some simple applications, and point you in the right direction if you want to learn more."
    },
    {
        "text": "24.1 \u00ada note on libraries i\u2019ve tried hard in this book to give example code using libraries that were rela- tively standardized."
    },
    {
        "text": "they work well, they are widely used and trusted, and i expect that people will still be using them 5 years from now."
    },
    {
        "text": "there are no such libraries for the topics i\u2019m discussing in this chapter."
    },
    {
        "text": "there are multiple competing ones, and it remains to be seen which software pack- ages will come out on top."
    },
    {
        "text": "i will still show you example code, written with the libraries that i myself use, but you should be aware that these libraries are still in their infancy."
    },
    {
        "text": "the documentation is bad, there are bugs, and the apis are likely to change in the future."
    },
    {
        "text": "and there is a good chance that these libraries will become obsolete as momentum shifts over to one of their competitors."
    },
    {
        "text": "the leading libraries for deep learning are theano and tensorflow."
    },
    {
        "text": "however, they are extremely low\u2010level libraries designed for people with a deep knowl- edge of the math that underlies deep learning."
    },
    {
        "text": "instead, i will use keras, which is a more user\u2010friendly (but not too user\u2010friendly \u2013 this is still deep learning) wrapper around them."
    },
    {
        "text": "keras lets you focus on how to structure your deep net- work, rather than the numerical details of how it gets trained."
    },
    {
        "text": "for bayesian networks, i use pymc."
    },
    {
        "text": "this library was originally created to help people use a numerical technique called markov chain monte carlo (mcmc) simulation."
    },
    {
        "text": "mcmc is useful for a variety of applications, and one of them is bayesian networks."
    },
    {
        "text": "this is the application that really caught on for pymc, and today you can use the library with almost no awareness of the mcmc algorithms running in the background."
    },
    {
        "text": "24.2 \u00adbasic deep learning recall from the previous chapter that the simplest neural network is the per- ceptron."
    },
    {
        "text": "a perceptron is a network of \u201cneurons,\u201d each of which takes in multi- ple inputs and produces a single output."
    },
    {
        "text": "an example is shown in the following figure:"
    },
    {
        "text": "24.2 basic deep learning 359 x1 x2 x3 y1 y2 y3 the labeled nodes correspond to either the input variables to a classification or a range of output variables."
    },
    {
        "text": "the other nodes are neurons."
    },
    {
        "text": "the neurons in the first layer take all of the raw features as inputs."
    },
    {
        "text": "their outputs are fed as inputs to the second layer and so on."
    },
    {
        "text": "ultimately, the outputs of the final layer constitute the output of your program."
    },
    {
        "text": "all layers of neurons before the last one are called \u201chidden\u201d layers."
    },
    {
        "text": "a perceptron has only a single hidden layer."
    },
    {
        "text": "the simplest type of deep learn- ing is what are called \u201cdeep networks\u201d \u2013 neural nets with multiple hidden layers."
    },
    {
        "text": "rather than getting into any more theory for now, let\u2019s show the code for building the simple neural net in the picture: from keras.models import sequential from keras.layers.core import dense, activation model = sequential([ dense(3, input_dim=3, activation='sigmoid'), dense(3, activation='sigmoid') ]) model.compile( loss='categorical_crossentropy', optimizer='adadelta') at this point, the model will have the normal train() and test() methods that we are familiar with from machine learning."
    },
    {
        "text": "let\u2019s step through what\u2019s happening: \u25cf \u25cfthe simplest type of neural network in keras is the sequential model, which means that there are layers of neurons stacked on each other with each layer feeding into the next."
    },
    {
        "text": "24 advanced classifiers 360 \u25cf \u25cfthe first layer we see is a dense."
    },
    {
        "text": "the input_dim = 3 means that there are three nodes in the previous layer."
    },
    {
        "text": "each of those nodes will have its output directed to every node in the current layer."
    },
    {
        "text": "there will also be three nodes in the current layer."
    },
    {
        "text": "\u25cf \u25cfthe activation parameter says how each node should condense all of its inputs into a single output."
    },
    {
        "text": "in this case, we use the usual sigmoid activation function, though there are others."
    },
    {
        "text": "this completes our hidden layer of neurons."
    },
    {
        "text": "\u25cf \u25cfthere is another dense layer, routing all the hidden outputs to each of three nodes."
    },
    {
        "text": "note that we don\u2019t need to specify the input_dim here, since it is equal to the dimensionality of the output of the previous layer."
    },
    {
        "text": "\u25cf \u25cffinally, we compile the model into a low\u2010level implementation in theano or tensorflow."
    },
    {
        "text": "this is not the same thing as training it \u2013 we are just configuring the number\u2010crunching machinery so that it can be trained."
    },
    {
        "text": "the \u201closs\u201d param- eter says what type of objective function should be minimized when it comes time to train the model."
    },
    {
        "text": "the \u201coptimizer\u201d specifies an algorithm for doing the actual training."
    },
    {
        "text": "here is a more complete code example, which constructs a neural net with 50 neurons in a single hidden layer and uses it to classify the data points in the iris dataset."
    },
    {
        "text": "note the preprocessing step where i turn the flower\u2019s species into a three\u2010dimensional vector, with the dimension corresponding to the correct species set to 1. import sklearn.datasets from keras.models import sequential from keras.layers.core import dense, activation import pandas as pd from sklearn.cross_validation import train_test_split ds = sklearn.datasets.load_iris() x = ds['data'] y = pd.get_dummies(ds['target']).as_matrix() x_train, x_test, y_train, y_test = \\ train_test_split(x, y, test_size=.2) model = sequential([ dense(50, input_dim=4, activation='sigmoid'), dense(3, activation='softmax') ]) model.compile( loss='categorical_crossentropy', optimizer='adadelta') model.fit(x_train, y_train, nb_epoch=5) proba = model.predict_proba(x_test, batch_size=32)"
    },
    {
        "text": "24.3 convolutional neural networks 361 pred = pd.series(proba.flatten()) true = pd.series(y_test.flatten()) print \"correlation:\", pred.corr(true) to give you a little idea of how finicky neural networks can be, it took me a while to put this example code together."
    },
    {
        "text": "if i had 10 neurons in the hidden layers rather than 50, for example, the performance would vacillate randomly between different times i trained/tested."
    },
    {
        "text": "sometimes, the correlations would be up to 0.6. other times, they would be very close to 0 or even negative a few times."
    },
    {
        "text": "i found empirically that the having 50 nodes makes the performance much more stable around 0.6."
    },
    {
        "text": "24.3 \u00adconvolutional neural networks one of the most important extensions of neural networks, which is especially important when classifying images or sounds, is the convolutional network."
    },
    {
        "text": "in software, they take the form of having a \u201cconvolutional\u201d layer."
    },
    {
        "text": "convolutional networks are partly inspired by human neurology."
    },
    {
        "text": "in your brain, in the earliest stages of processing images, there are neurons that spe- cialize in very primitive patterns in very specific parts of the visual field."
    },
    {
        "text": "there might be one that activates whenever there is the shape of a line sloping up in a particular part of your upper\u2010right field of view."
    },
    {
        "text": "in fact, neurons correspond- ing to nearby regions of the visual field tend to be nearby each other in your cerebral cortex."
    },
    {
        "text": "in this way, their activation patterns take what you\u2019re looking at and paint a distorted version of it right onto the surface of your brain."
    },
    {
        "text": "later parts of your visual system will do the same thing to the previous layers."
    },
    {
        "text": "if you\u2019ll permit me to grossly oversimplify, stage 1 might have neurons that detect straight lines at various slopes, in different parts of the visual field."
    },
    {
        "text": "stage 2 will have neurons that combine adjacent neurons from stage 1 and fire if those sloped lines fit together to form a square and so on."
    },
    {
        "text": "at the highest levels, you will have neurons that fire in response to something as specific as a picture of homer simpson."
    },
    {
        "text": "a convolutional layer in a neural network has several \u201cfilters,\u201d each of which looks for a particular pattern in the image."
    },
    {
        "text": "each filter has a \u201ckernel\u201d: a small matrix of numbers, which, when seen as an image, resemble the pattern that this filter is detecting."
    },
    {
        "text": "to detect the pattern at various parts of the image, we slide the kernel all over the image and take the dot product of the kernel with the pixels that it is overlapping."
    },
    {
        "text": "typically, we will slide it up/down by a fixed number of pixels and left/right by a fixed (possibly different) number of pixels."
    },
    {
        "text": "this process is called a \u201cconvolution\u201d between the original image and the kernel."
    },
    {
        "text": "24 advanced classifiers 362 a simple version of convolution is shown in this image, where we move the kernel by three pixels every time we slide it: kernel image convolution 0 0 1 1 1 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 2 3 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 the values in the convolved image now represent the degree to which the image, at that point, looked like the kernel."
    },
    {
        "text": "in a neural network, you can have one or more convolutional layers feeding into each other, and each of them can have several different filters."
    },
    {
        "text": "the image can also have more than one number associated with each pixel."
    },
    {
        "text": "for example, in a typical colored image, there will be three values giving the amounts of red, green, and blue at that point."
    },
    {
        "text": "in this case, the kernels will be three\u2010dimensional arrays."
    },
    {
        "text": "i have also presented this in the context of convolutions over images."
    },
    {
        "text": "while that\u2019s the most famous application, the mathematics of convolution works equally well for a one\u2010dimensional signal such as a time series."
    },
    {
        "text": "in areas such as this, convolutional nets are useful for areas such as speech recognition."
    },
    {
        "text": "24.4 \u00addifferent types of layers."
    },
    {
        "text": "what the heck is a tensor?"
    },
    {
        "text": "the name of the software tensorflow speaks to the mathematical underpin- nings of neural nets, and it\u2019s important to have at least some idea of what\u2019s going on there."
    },
    {
        "text": "the word \u201ctensor\u201d has a number of technical definitions, but"
    },
    {
        "text": "24.5 example: the mnist handwriting dataset 363 colloquially, it is often used to describe multidimensional arrays of numbers that we do linear operations on."
    },
    {
        "text": "understanding this perspective is impor- tant for actually constructing complicated neural nets, so i would like to review it."
    },
    {
        "text": "the input to a basic neural net is a one\u2010dimensional array, and its output is also a one\u2010dimensional array."
    },
    {
        "text": "the input to a two\u2010dimensional convolutional network is a two\u2010dimensional array, and its output is one\u2010dimensional."
    },
    {
        "text": "in gen- eral, every layer of a network takes a tensor as input and produces a tensor as its output."
    },
    {
        "text": "most of what goes on in a neural net consists of linear operations on these tensors."
    },
    {
        "text": "take a plain\u2010vanilla layer with a sigmoid activation function."
    },
    {
        "text": "it oper- ates in the following way: \u25cf \u25cfit takes in inputs, which we can think of as a tensor."
    },
    {
        "text": "\u25cf \u25cfevery node takes a weighted combination of its inputs."
    },
    {
        "text": "alternatively, it takes a weighted combination of all the inputs, except that many of the coefficients are 0."
    },
    {
        "text": "\u25cf \u25cfif there are d inputs and n nodes, and each node takes a linear combination of the inputs, this is really just multiplying an n\u2010by\u2010d matrix by the input vector."
    },
    {
        "text": "\u25cf \u25cffinally, we apply the sigmoid function to every element in the weighted\u2010sum vector."
    },
    {
        "text": "this is the only part that\u2019s not linear."
    },
    {
        "text": "the layer of the network is completely characterized by the matrix of its weight coefficients."
    },
    {
        "text": "in terms of implementation, most of what you\u2019re doing boils down to linear algebra."
    },
    {
        "text": "a convolutional layer in a neural network is entirely linear."
    },
    {
        "text": "conceptually, it takes in a two\u2010dimensional tensor, multiplies it by a four\u2010dimensional tensor, and has the output that is another two\u2010dimensional tensor."
    },
    {
        "text": "note that the input to a convolutional net is 2d, but the final output will typically be 1d."
    },
    {
        "text": "so, there will need to be some layer where we switch from two to one dimension."
    },
    {
        "text": "typically, this layer is also completely linear, a flattening layer that takes in a two\u2010dimensional tensor of dimensions d\u2010by\u2010n and outputs a 1d tensor of length d*n containing all the same values."
    },
    {
        "text": "this is just multiply- ing the input tensor by a three\u2010dimensional tensor, all of whose values are either 0 or 1."
    },
    {
        "text": "24.5 \u00adexample: the mnist handwriting dataset the standard dataset for learning about convolutional nets is called mnist, which contains 70,000 images of handwritten digits, each one 28 pixels by 28 pixels."
    },
    {
        "text": "this section will walk through running a convolutional network on the mnist data."
    },
    {
        "text": "24 advanced classifiers 364 a gotcha that i should clarify upfront is that in keras, 2\u2010d convolutional neu- ral nets expect an input array with four dimensions: \u25cf \u25cfthe first dimension is for which data point you\u2019re looking at."
    },
    {
        "text": "\u25cf \u25cfthe second dimension is for the \u201cchannel.\u201d in my code, there is only one channel, since my images have only one floating number for each pixel."
    },
    {
        "text": "in color images, you are likely to have three channels, for red, green, and blue."
    },
    {
        "text": "in general, you could have more."
    },
    {
        "text": "\u25cf \u25cfthe third and fourth dimensions are the number of rows and columns in the input images."
    },
    {
        "text": "this example will use all the layer types we have discussed so far and once other: dropout."
    },
    {
        "text": "a dropout layer is a heavy\u2010handed way to reduce overfitting in neural nets."
    },
    {
        "text": "a dropout layer takes in a parameter p, denoting the probability of zeroing\u2010out a value."
    },
    {
        "text": "during each training iteration, the dropout layer will take in a tensor, set a random fraction p of its values to 0, and then pass the array on the next layer for training."
    },
    {
        "text": "the following code will fetch the mnist dataset and train a convolutional neural network on it."
    },
    {
        "text": "be aware that you must be connected to the internet for this code to run, and it will take quite a while to do so, since it fetches the mnist dataset at runtime."
    },
    {
        "text": "import theano import statsmodels.api as sm import sklearn.datasets as datasets import keras from keras.models import sequential from keras.layers.core import dense, activation, dropout, flatten import pandas as pd from matplotlib import pyplot as plt import sklearn.datasets from keras.layers.convolutional import convolution2d, maxpooling2d from sklearn.cross_validation import train_test_split from sklearn.decomposition import pca from sklearn.cluster import kmeans from sklearn.metrics import silhouette_score, adjusted_rand_score from sklearn import metrics from sklearn.cross_validation import train_test_split from sklearn import datasets import sklearn from sklearn.datasets import fetch_mldata"
    },
    {
        "text": "24.5 example: the mnist handwriting dataset 365 datadict = datasets.fetch_mldata('mnist original') x = datadict['data'] y = datadict['target'] x_train, x_test, y_train, y_test = \\ train_test_split(x, y, test_size=.1) x_train = x_train.reshape((63000,1,28,28)) x_test = x_test.reshape((7000,1,28,28)) y_train = pd.get_dummies(y_train).as_matrix() # convolution layers expect a 4-d input so we reshape our 2-d input nb_samples = x_train.shape[0] nb_classes = y_train.shape[1] # we set some hyperparameters batch_size = 16 kernel_width = 5 kernel_height = 5 stride = 1 n_filters = 10 # we fit the model model = sequential() model.add(convolution2d( nb_filter=n_filters, input_shape=(1,28,28), nb_row=kernel_height, nb_col=kernel_width, subsample=(stride, stride)) ) model.add(activation('relu')) model.add(maxpooling2d(pool_size=(5,5))) model.add(dropout(0.5)) model.add(flatten()) model.add(dense(nb_classes)) model.add(activation('softmax')) model.compile(loss='categorical_crossentropy', optimizer='adadelta') print 'fitting model' model.fit(x_train, y_train, nb_epoch=10) probs = model.predict_proba(x_test) preds = model.predict(x_test) pred_classes = model.predict_classes(x_test) true_classes = y_test (pred_classes==true_classes).sum()"
    },
    {
        "text": "24 advanced classifiers 366 24.6 \u00adrecurrent neural networks one of the big things about neural nets so far in that, similarly to normal machine learning models, they handle the points that they classify indepen- dently."
    },
    {
        "text": "if they are fed a series of points, they will classify each one with no refer- ence to any of the others."
    },
    {
        "text": "essentially, they have no memory after they\u2019re done processing a point."
    },
    {
        "text": "recurrent neural networks (rnns) have a primitive type of memory, in the form of \u201crecurrent\u201d layers."
    },
    {
        "text": "a recurrent layer takes in two kinds of input: the output of the preceding layer and the output of the same recurrent layer from the last point it processed."
    },
    {
        "text": "that is, the output of a recurrent layer when classifying a point will be passed back into this layer when classifying the next point."
    },
    {
        "text": "recurrent nets are thus ideal for dealing with time series data."
    },
    {
        "text": "ascribing specific meaning to the output of a recurrent layer is very difficult, but just to give you an idea: the output could encode a prediction about what the next point will be, given the current point."
    },
    {
        "text": "it could also encode a long\u2010term memory of some point that was seen a long time ago."
    },
    {
        "text": "there are many types of recurrent layers available, which keep a variety of different types of memory between classifications."
    },
    {
        "text": "the ones you are most likely to see are as follows: \u25cf \u25cfbasic recurrent nets: this will just recall the last output of the layer and feed it back as input during the next classification."
    },
    {
        "text": "\u25cf \u25cflstms."
    },
    {
        "text": "this stands for \u201clong short\u2010term memory,\u201d and it is very good at clearly remembering select events from very far in the past."
    },
    {
        "text": "this is in con- trast to basic rnns, for which the memory of an event decays over time."
    },
    {
        "text": "the following example code uses a built\u2010in time series dataset as an example for training and testing a recurrent neural net in keras."
    },
    {
        "text": "the training data is a tensor of shape (num_sequences, timestamps_per_sequence, num_dimen- sions)."
    },
    {
        "text": "in this case, each vector is only of dimension 1, but there are 11 meas- urements in each sequence."
    },
    {
        "text": "import sklearn.datasets from keras.models import sequential from keras.layers.core import dense, activation from keras.layers.recurrent import lstm, gru import statsmodels as sm df = sm.datasets.elnino.load_pandas().data x = df.as_matrix()[:,1:-1] x = (x-x.min()) / (x.max()-x.min()) y = df.as_matrix()[:,-1].reshape(61) y = (y-y.min()) / (y.max()-y.min())"
    },
    {
        "text": "24.7 bayesian networks 367 x_train, x_test, y_train, y_test = ( train_test_split(x, y, test_size=0.1) model = sequential() model.add(gru(20, input_shape=(11,1))) model.add(dense(1, activation='sigmoid')) model.compile(loss='mean_squared_error', optimizer='adadelta') model.fit(x_train.reshape((54,11,1)), y_train, nb_epoch=5) proba = model.predict_proba(x_test.reshape((7,11,1)), batch_size=32) pred = pd.series(proba.flatten()) true = pd.series(y_test.flatten()) print \"corr."
    },
    {
        "text": "of preds and truth:\", pred.corr(true) 24.7 \u00adbayesian networks let\u2019s briefly recap about bayesian statistics from earlier in the book."
    },
    {
        "text": "we have a random variable d = (d1, d2, ..., dd) that is considered to represent our data, and we have a random variable y that represents whatever underlying thing we are trying to infer."
    },
    {
        "text": "intuitively, we want to get our hands on p(y|x), the probabil- ity of y having a certain value, given the data x that we observe."
    },
    {
        "text": "the key equa- tion of bayesian statistics is p y d p d y p y p d | | ( ) = ( ) ( ) ( ) taking the terms on the right, we have the following: \u25cf \u25cfp(y) is called the \u201cprior,\u201d and it is our initial confidence in the different values y could have."
    },
    {
        "text": "this is the philosophical controversial part of bayesian stats, because p(y) is really just a measure of our subjective confidence, but we treat it as a mathematical probability."
    },
    {
        "text": "\u25cf \u25cfp(d|y) is where the magic happens."
    },
    {
        "text": "this is the probability of the data we got, assuming an underlying value of y. in practice, this is a lot easier to think about and to model than p(y|d)."
    },
    {
        "text": "most of the work in doing bayesian analysis involves the question of how we model p(d|y) and fit the model to our data."
    },
    {
        "text": "\u25cf \u25cfp(d) is the probability of seeing our data d. to get this probability, you must average over the different possible values of y, which can be quite compli- cated."
    },
    {
        "text": "however, in practice, you almost never need to actually calculate p(d), since it\u2019s just a normalization term so that all of the p(y|d) add up to 1."
    },
    {
        "text": "24 advanced classifiers 368 usually, you\u2019re only interested in the relative probabilities of the different values of y. the term p(d|y) is the big complicated one, since in general, the various di could have complicated dependency structures between them."
    },
    {
        "text": "the naive bayes assumption is that p d y p d y p d y p d y d | | * | * * | ( ) = ( ) ( ) ( ) 1 2 that is, the di are all conditionally independent of each other, if we know y. in general though, this is not true, and we represent the dependencies between the variables with a \u201cbayesian network\u201d such as the following: gender hair length shampoo time wears hairtie this network says that knowing somebody\u2019s gender tells you something about how long their hair is likely to be, and hair length in turn can be used to predict whether somebody wears a hair tie."
    },
    {
        "text": "it may be that a woman is more likely to wear a hair tie than a male, but that\u2019s only because women are more likely to have long hair."
    },
    {
        "text": "among the people who have long hair, there is no rela- tionship between gender and hair ties, and similarly among those who have short hair."
    },
    {
        "text": "when you are using bayesian networks, generally it is up to you as the user to pick the topology of the network, along with some probabilistic model of how each variable depends on the ones that influence it."
    },
    {
        "text": "training data is used to fit the actual parameters of the dependencies."
    },
    {
        "text": "network topology is a place where expert knowledge or intuition can be inserted into the model."
    },
    {
        "text": "in this sense, bayesian networks are almost the opposite of neural nets; they lend themselves to the introduction of intuition and deep understanding, whereas neural nets are nearly impossible to make sense of."
    },
    {
        "text": "similar to other machine learning models, there are two ways you might want to use bayesian network models: either you can use them to make predic- tions or you can dissect the trained models to get insights about whatever it is that you\u2019re studying."
    },
    {
        "text": "24.9 markov chain monte carlo 369 24.8 \u00adtraining and prediction when we discussed naive bayes classifiers in a previous chapter, we had a prior confidence p(y) over all classifications, but then we had precise models p(d|y) of what the data would look like conditioned on different classes."
    },
    {
        "text": "the model parameters that characterize the p(d|y) are known precisely."
    },
    {
        "text": "typically, those parameters are learned using maximum likelihood estimators on the training data, which opens the door to overfitting issues."
    },
    {
        "text": "in general, with bayesian networks though, the model parameters them- selves have a confidence distribution over them."
    },
    {
        "text": "before training, we have pri- ors over what those parameters might be, and we refine our confidence in light of the training data."
    },
    {
        "text": "when it comes time to classify points in the future, we must average our predictions over all possible model configurations, weighting them by our confidence."
    },
    {
        "text": "this means that the probability will be proportional to a very complicated integral: p y d p y p d y | | , ( ) \u221d( ) ( ) ( ) \u222bconfidence d \u03b8 \u03b8 \u03b8 \u03b8 where confidence(\u03b8) bakes in both our priors over \u03b8 and our training data."
    },
    {
        "text": "this is a very complicated (and sometimes impossible) integral to perform, and practitioners of bayesian networks often use numerical approximations instead of just calculating it directly."
    },
    {
        "text": "24.9 \u00admarkov chain monte carlo pymc is a python library for fitting bayesian networks to real data."
    },
    {
        "text": "unfortunately, it is not a classifier in the normal sense."
    },
    {
        "text": "if you want to use the fitted model to actually make predictions, then you will have to do it on your own."
    },
    {
        "text": "however, pymc will do the heavy lifting of fitting a potentially extremely complicated model, and that\u2019s where the bulk of the work is."
    },
    {
        "text": "let\u2019s call \u03b8 the correctly trained distribution over all values of \u03b8, taking into account both our priors and the data we train on."
    },
    {
        "text": "the probability density for \u03b8 at \u03b8 will be confidence(\u03b8) from the integral in the previous section."
    },
    {
        "text": "instead of giving us the function confidence(\u03b8) directly, which is computationally intrac- table, pymc gives us random samples from the distribution \u03b8. you can then estimate the integral by simply summing over all of the samples."
    },
    {
        "text": "i don\u2019t want to get too far into the weeds here."
    },
    {
        "text": "without giving you too much theory, pymc fits \u03b8 using a technique called \u201cmarkov chain monte carlo\u201d (mcmc)."
    },
    {
        "text": "running an mcmc simulation will give us a sequence of guesses at \u03b8 that will, over many time stamps, be sampled from the trained distribution \u03b8. let\u2019s call the ith guess \u03b8i."
    },
    {
        "text": "that means, for example, that the average of many mcmc \u03b8i is guaranteed to converge to the actual average value of \u03b8."
    },
    {
        "text": "24 advanced classifiers 370 what mcmc does not guarantee, however, is that subsequent guesses are independent samples from \u03b8. quite to the contrary, each guess is correlated with the ones before and after it."
    },
    {
        "text": "this means that \u03b8n will be correlated with \u03b8n+1 , which will in turn be correlated with \u03b8n+2 ."
    },
    {
        "text": "then \u03b8n will also be correlated with \u03b8n+2 , just not as strongly as it is with \u03b8n+2 ."
    },
    {
        "text": "if the correlations decay quickly, then the \u03b8i will be nearly independent of each other."
    },
    {
        "text": "but if they are very highly correlated, then it might take many, many samples before we get a decent approximation of \u03b8. if you think of the probability distribution of \u03b8 as a landscape, imagine the mcmc process as taking a random walk over the landscape that is guaranteed to spend more time in the likelier regions."
    },
    {
        "text": "you have a mathematical guarantee that, in the limit of walking for an infinite amount of time, the amount of time you spend in any area will be proportional to the probability of \u03b8 in that area."
    },
    {
        "text": "but in practice, depending on where you start and how large your steps are, it could take you arbitrarily long to actually get to a particular high\u2010probability area."
    },
    {
        "text": "this autocorrelation of the \u03b8i is the biggest problem with mcmc models."
    },
    {
        "text": "there are internal mechanisms of a mcmc model that you can tune, which might reduce the autocorrelation."
    },
    {
        "text": "but doing this well is tricky, and there\u2019s not much that has any guarantees of working."
    },
    {
        "text": "now as with all theoretical boogey men, this is often not a problem in prac- tice."
    },
    {
        "text": "in many cases, your first few \u03b8i will be grossly wrong because you started at an unlikely location, but soon they converge reliably to a best\u2010fit value that they rarely deviate far from."
    },
    {
        "text": "starting from an unlikely situation though is a problem, since it will skew any statistics that you care to calculate."
    },
    {
        "text": "so, what is often done is that you pick a suitably large number n and then discard \u03b81 to \u03b8n, the so\u2010called burn in."
    },
    {
        "text": "after that the distribution is assumed to be \u201cstable enough.\u201d 24.10 \u00adpymc example scikit\u2010learn has a toy dataset built in that gives housing statistics for several towns in the boston area."
    },
    {
        "text": "i will pull out several of those statistics and show you pymc code that fits the following bayesian network: pupil/teacher ratio in schools crime rate median home price average number of rooms per house"
    },
    {
        "text": "24.10 pymc example 371 the idea here is that a poor educational system leads to higher crime rates, which in turn will reduce home prices."
    },
    {
        "text": "in reality, bad schools will also impact home prices directly, but i\u2019m ignoring that for this model."
    },
    {
        "text": "i will make the following assumptions about the variables and their relationships: \u25cf \u25cfthe pupil/teacher ratio is an exponential distribution, whose mean i know from the data."
    },
    {
        "text": "\u25cf \u25cfthe crime for a city will also be exponentially distributed."
    },
    {
        "text": "however, its mean will be some constant a times the pupil/teacher ratio."
    },
    {
        "text": "we will need to fit a from the data."
    },
    {
        "text": "\u25cf \u25cfthe average number of rooms for a town will be normally distributed, with the mean and standard deviation present in the data."
    },
    {
        "text": "\u25cf \u25cfthe median home price for a town will be normally distributed, with a mean of b*{avg."
    },
    {
        "text": "number of rooms}*(c\u2010{crime rate})."
    },
    {
        "text": "here b and c are unknown parameters that must be fitted."
    },
    {
        "text": "\u25cf \u25cfa, b, and c will all have exponentially distributed priors with mean 1. in order to use pymc, you will need to declare the model in its entirety in a python file, which we will call mymodel.py."
    },
    {
        "text": "put the following code into mymodel.py: import pandas as pd import sklearn.datasets as ds import pymc # make pandas dataframe bs = ds.load_boston() df = pd.dataframe(bs.data, columns=bs.feature_names) df['medv'] = bs.target # unknown parameters are a, b and c a = pymc.exponential('a', beta=1) b = pymc.exponential('b', beta=1) c = pymc.exponential('c', beta=1) ptratio = pymc.exponential( 'ptratio', beta=df.ptratio.mean(), observed=true, value=df.ptratio) crim = pymc.exponential('crim', beta=a*ptratio, observed=true, value=df.crim) rm = pymc.normal('rm', mu=df.rm.mean(), tau=1/(df.rm.std()**2), value=df.rm, observed=true) medv = pymc.normal('medv', mu=b*rm*(c-crim), value=df.medv, observed=true)"
    },
    {
        "text": "24 advanced classifiers 372 note a few things about this code: \u25cf \u25cfwhenever we declare a variable, we specify its family and the parameters that characterize it."
    },
    {
        "text": "\u25cf \u25cfyou can make the parameters for a variable take on a fixed value."
    },
    {
        "text": "or, you can make them take on a value that is defined by other variables."
    },
    {
        "text": "\u25cf \u25cfif the data were actually observed, you can say observed = true and pass in the observed values."
    },
    {
        "text": "once this file is in place, you can train it this way: import pymc import mymodel import matplotlib.pyplot as plt s = pymc.mcmc(mymodel) s.sample(iter = 40000, burn = 30000) pymc.matplot.plot(s) plt.show() this says to discard the first 30 thousand simulations and then calculate sta- tistics based on the next 40 thousand."
    },
    {
        "text": "it will produce plots such as the follow- ing one, which shows the evolution of b over the course of the simulation: 0.0625 b trace b hist 0.0620 0.0615 0.0610 0.0605 0.0600 0.0595 0.0590 0 1000 2000 3000 4000 1000 800 600 400 200 100 80 60 40 20 0 1.0 0.8 0.6 0.4 0.2 0.0 0 0.0590 0.0595 0.0600 0.0605 0.0610 0.0615 0.0620 0.0625 5000 frequency b 8 corr we can see on the right, a histogram of all b values, which are centered pretty tightly around 0.0605. on the upper left is the evolution of b over the course of the data we gathered."
    },
    {
        "text": "from the plot it seems that the simulation has equili- brated, and we can probably make use of the numbers."
    },
    {
        "text": "technically speaking,"
    },
    {
        "text": "24.12 glossary 373 there is a risk that there is still some large area of \u03b8 that our simulation hasn\u2019t wandered into yet, and running it a little longer would change the dynamics dramatically."
    },
    {
        "text": "but looking at the graphs that sure seems unlikely."
    },
    {
        "text": "24.11 \u00adfurther reading koller, d & friendman, n, probabilistic graphical models: principles and techniques, 2009, the mit press, cambridge, ma."
    },
    {
        "text": "goodfellow, i, bengio, y & courville, a, deep learning, 2016, mit press, http:// www.deeplearningbook.org/."
    },
    {
        "text": "24.12 \u00adglossary bayesian network \u00adan arrangement of random variables into a directed graph structure, where each graph is conditionally independent of all nodes above it except its immediate parents."
    },
    {
        "text": "convolutional neural network a neural net where one of the layers takes the convolution of the input with a kernel."
    },
    {
        "text": "typically, a kernel will capture a specific, relatively low\u2010level pattern in the data."
    },
    {
        "text": "the output of the convolutional layer will be an indication of how much that pattern is present at each part of the input data."
    },
    {
        "text": "deep neural net a neural net with an unusually high number of layers."
    },
    {
        "text": "deep learning a broad term for a wide range of developments that have happened recently in neural networks."
    },
    {
        "text": "markov chain monte carlo a simulation technique that is useful for fitting the parameters of bayesian network models."
    },
    {
        "text": "neural network a class of machine learning models inspired by the wiring of neurons in biological brains."
    },
    {
        "text": "recurrent neural network a neural network where the output of a layer can be fed back into that layer."
    },
    {
        "text": "tensor an array of floating\u2010point numbers."
    },
    {
        "text": "there can be arbitrarily many dimensions to the array."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 375 25 stochastic modeling refers to a collection of advanced probability tools for studying not a single random variable, but instead, a process that happens ran- domly over time."
    },
    {
        "text": "this could be the movement of a stock price over time, visi- tors arriving at a web , or a machine moving between internal states over the course of its operation."
    },
    {
        "text": "you\u2019re not locked in to using time either; anything that is sequential can be studied."
    },
    {
        "text": "this includes which words follow which others in a piece of text, changes from one generation to the next in a line of animals, and how tempera- ture varies across a landscape."
    },
    {
        "text": "the first place i ever used stochastic analysis was studying the sequence of nucleotides in dna."
    },
    {
        "text": "this chapter will give you an overview of several of the main probability models, starting with the most important one: the markov chain."
    },
    {
        "text": "i will discuss how they are related to each other, what situations they describe, and what kinds of problems you can solve with them."
    },
    {
        "text": "25.1 \u00admarkov chains by far the most important stochastic process to understand is the markov chain."
    },
    {
        "text": "a markov chain is a sequence of random variables x1, x2, ... that are interpreted as the state of a system at sequential points in time."
    },
    {
        "text": "for now, assume that the xi are discrete rvs that can take on only a finite number of values."
    },
    {
        "text": "each of the xi has the same set of states that it can take on."
    },
    {
        "text": "the definitive feature of a markov chain is that the distribution of xi+1 can be influenced by xi, but it is independent of all the previous rvs if you know xi."
    },
    {
        "text": "that is, p x x x x x p x x x i i i i + + = ... ( ) = = ( ) 1 1 2 1 | , , , | the cartoonish way i like to think of this is that you have a collection of lily pads and a frog jumping between them randomly."
    },
    {
        "text": "at any point in time, the frog stochastic modeling"
    },
    {
        "text": "25 stochastic modeling 376 knows which lily pad it is on, and this knowledge determines how likely it is to jump to every other lily pad at the next step (or it could just stay on the current lily pad)."
    },
    {
        "text": "but the frog has amnesia; it has no memory of what lily pad it was on at any previous step or how long it has been hopping around."
    },
    {
        "text": "so, the probabil- ity distribution of its next hop is only a function of where it is now."
    },
    {
        "text": "if there are k different lily pads, we arrange these transition probabilities into a k\u2010by\u2010k matrix."
    },
    {
        "text": "the ith row corresponds to being on the ith lily pad."
    },
    {
        "text": "the jth entry in that row will be the probability of jumping to the jth lily pad, given that we are currently on the ith one."
    },
    {
        "text": "this means that every entry in the transition matrix must be greater than or equal to 0, and every row sums to 1.0. knowing the transition matrix and the initial probability distribution of x1 completely characterizes the markov chain."
    },
    {
        "text": "it is common to draw our markov chain, especially ones where k is small, in diagrams with arrows pointing between the states."
    },
    {
        "text": "here is one for a particular markov chain i concocted describing the weather: 0.8 0.2 0.5 0.5 sunny rainy in this model, a sunny day has an 80% probability of being followed by another sunny day, and the day after it rains will be 50/50 rain or shine."
    },
    {
        "text": "the transition matrix for the markov chain looks like the following: sunny 0.5 0.5 0.8 0.2 rainy sunny rainy if there are k states in the system and we are in the ith state, then we can express that as a k\u2010by\u20101 row vector that is equal to 1.0 at i and 0 everywhere else."
    },
    {
        "text": "call the state vector p. the vector p can also have several nonzero compo- nents, just so longer as they are all nonnegative and they sum to 1. in this case, p represents a probability distribution over the possible states."
    },
    {
        "text": "if we want to know the distribution at the next time step, we simply multiply p by the transi- tion matrix t: p pt i i + = 1"
    },
    {
        "text": "25.2 two kinds of markov chain, two kinds of questions 377 and in general p pt i m i m + = the fact that xi+1 is only influenced by xi is called the \u201cmarkov assumption.\u201d it is the key to making markov chains tractable, both mathematically and com- putationally."
    },
    {
        "text": "the markov assumption is implicit in the state transition diagram, because each state has no indication of how you got there."
    },
    {
        "text": "it only shows where you might be going next."
    },
    {
        "text": "in many applications, the markov assumption does not hold."
    },
    {
        "text": "for instance, it is common to crudely model natural language by having the xi represent words in a piece of text."
    },
    {
        "text": "if there is a word, this is missing or ambiguous, a markov chain can be used to find what the word most likely is (more on that later)."
    },
    {
        "text": "the problem is that a markov chain that only incorporates one word of context is woefully inadequate."
    },
    {
        "text": "the typical solution is to have xi+1 depend not only on xi, but also on several of the values before that back to xi\u2212m\u22121."
    },
    {
        "text": "this is called a markov chain of order m. strictly speaking though, an order\u2010m markov chain is equivalent to a beefed\u2010 up markov chain of order 1. if we define a new set of random variables y x x x y x x x y x x x m m m 1 1 2 2 2 3 1 3 3 4 2 = ... ( ) = ... ( ) = ... ( ) + + , , , , , , , , , then the yi will behave as an order\u2010m markov chain."
    },
    {
        "text": "it will just be one with km states, and all of the numbers in the state transitions that would overwrite the previous xi have probability 0. in industrial scale, markov chains for natural language, something such as m = 6 is common."
    },
    {
        "text": "there are a lot of words in the english language."
    },
    {
        "text": "that number raised to the sixth power is astronomically large."
    },
    {
        "text": "this should give you an idea of some of the practical problems in working with large markov chains."
    },
    {
        "text": "when the number of states gets very large, it is impossible to get enough data to fit the transition matrix, and it becomes computationally intractable to explore them all."
    },
    {
        "text": "in these situations, there are a variety of heuristics and performance optimiza- tions that get utilized."
    },
    {
        "text": "25.2 \u00adtwo kinds of markov chain, two kinds of questions most practical markov chains come in two types: \u201cirreducible aperiodic\u201d and \u201cabsorbing.\u201d they have different mathematical properties and are used for modeling very different types of systems."
    },
    {
        "text": "25 stochastic modeling 378 an irreducible aperiodic markov chain has two key properties: \u25cf \u25cfit is possible to get from any state to any other state in a finite number of time steps."
    },
    {
        "text": "\u25cf \u25cfno state is \u201cperiodic.\u201d that is, you don\u2019t have a weird situation where you can only ever return to a particular state after an odd number of steps or some- thing similar."
    },
    {
        "text": "if you look far enough into the future, it is possible to be in any state at any time."
    },
    {
        "text": "these properties guarantee that no matter what state the markov chain starts in it will, in the very long run, have a \u201csteady state\u201d behavior where it could be in any state."
    },
    {
        "text": "the long\u2010term distribution of how likely it is to be in each state is also called the \u201cequilibrium distribution.\u201d an irreducible aperiodic markov chain is guaranteed to have a unique equilibrium distribution."
    },
    {
        "text": "the distribution is a k\u2010dimensional vector of probabilities, where the ith compo- nent gives the probability of being in the ith state."
    },
    {
        "text": "typically, this vector is referred to as \u03c0. the weather diagram i drew was irreducible and aperiodic: no matter what the weather is today, the probability of it being rainy in 1000 days is function- ally just the fraction of all days when it is sunny."
    },
    {
        "text": "natural language is another example."
    },
    {
        "text": "no matter what word we are looking at now, in a million words, it could be anything."
    },
    {
        "text": "and the picture is functionally the same for a million and first word and so on."
    },
    {
        "text": "if you want to find the steady\u2010state distribution \u03c0, you can do so by solving the linear algebra equation \u03c0 \u03c0 = t along with the constraint i i \u2211 = \u03c0 1 a conceptually simple (although potentially computationally expensive) way to do this approximately is to recall that p pt i m i m + = in the limit that m is large, pi m + will converge to \u03c0 regardless of pi ."
    },
    {
        "text": "since this is true regardless of pi, it means that the rows of t m must be approxi- mately equal to each other and hence to \u03c0. so, if you multiply t by itself repeatedly until it converges, the rows of the resulting matrix will approxi- mately equal \u03c0. in an absorbing markov chain, there is some state (possibly several) that always goes to itself with probability 1.0. if the frog lands on this lily pad, then he stays there forever."
    },
    {
        "text": "absorbing markov chains are used for modeling"
    },
    {
        "text": "25.3 markov chain monte carlo 379 processes that terminate, such as the behavior of a visitor to a website."
    },
    {
        "text": "there could be a variety of pages they navigate through and maybe return to."
    },
    {
        "text": "but eventually, they will end up in the \u201cmade a purchase\u201d state or the \u201cleft our site\u201d state."
    },
    {
        "text": "absorbing markov chains can also be used to model the lifetime of a physical machine that eventually breaks down or a medical patient who even- tually dies."
    },
    {
        "text": "sometimes, you will see absorbing markov chains for describing situations that usually use irreducible chains."
    },
    {
        "text": "natural language typically uses irreducible chains, but if you are trying to model short pieces of text such as e\u2010mails or text mes- sages, then you might want to add in an absorbing \u201cthe message is done\u201d state."
    },
    {
        "text": "with irreducible markov chains, we tend to ask questions such as the following: \u25cf \u25cfwhat is the long\u2010term equilibrium distribution \u03c0?"
    },
    {
        "text": "\u25cf \u25cfgiven the state that i\u2019m in now, how many steps will it take before the prob- ability distribution of my state approximates \u03c0?"
    },
    {
        "text": "\u25cf \u25cfhow many steps will it take, on average, for me to get from state a to state b?"
    },
    {
        "text": "\u25cf \u25cfgiven the state that i\u2019m in now, what is the probability distribution for where i will be in 5 time steps?"
    },
    {
        "text": "irreducible markov chains are also building blocks in a range of other proba- bilistic models, which most of the rest of this chapter will be devoted to."
    },
    {
        "text": "with an absorbing markov chain, we are more likely to ask the following: \u25cf \u25cfgiven where i am now, how long until i enter an absorbing state?"
    },
    {
        "text": "\u25cf \u25cfgiven where i am now, how likely am i to end up in each absorbing state?"
    },
    {
        "text": "\u25cf \u25cfhow many times can i expect to visit state a before i finally get absorbed?"
    },
    {
        "text": "25.3 \u00admarkov chain monte carlo in the previous chapter, we have actually already seen a fairly advanced applica- tion of markov chains."
    },
    {
        "text": "the details of it are outside the scope of this book, but since we\u2019re talking about markov chains in detail, i wanted to point out the connection."
    },
    {
        "text": "recall that we used a technique called \u201cmarkov chain monte carlo\u201d (mcmc) to estimate the trained parameters of a bayesian network given observations."
    },
    {
        "text": "let me review what we did: \u25cf \u25cfwe had a bayesian network describing the relationships between several variables."
    },
    {
        "text": "let\u2019s use \u03b8 to denote the parameters that characterize the network."
    },
    {
        "text": "the topology of the network was specified, but it was untrained, so \u03b8 was not known."
    },
    {
        "text": "\u25cf \u25cfwe had prior distributions on \u03b8, reflecting our initial confidences about what the \u201creal\u201d value of \u03b8 was."
    },
    {
        "text": "25 stochastic modeling 380 \u25cf \u25cfwe had a dataset that we considered to be generated by that network."
    },
    {
        "text": "\u25cf \u25cftaking the priors and the data into account, there is a natural way to \u201cscore\u201d any particular set of parameters \u03b8. it is the prior probability of \u03b8 times the probability of the data we saw given \u03b8: s data \u03b8 \u03b8 \u03b8 ( ) = ( ) ( ) pr pr | \u25cf \u25cfthese scores do not define a probability distribution, since we have no guar- antee that they sum up (or integrate) to 1.0. however, if we were to divide each score by the sum of all the scores, then we would have a probability distribution over all possible \u03b8, indicating how likely (in the bayesian sense) they are to be the real parameters."
    },
    {
        "text": "let\u2019s use \u03c0 to denote this distribution over all the possible values of \u03b8. knowing \u03c0 completely characterizes our beliefs and guesses about the correct value of \u03b8. however, all we have as a guide to \u03c0 is this potentially extremely complicated function s(\u03b8)."
    },
    {
        "text": "we know that s(\u03b8) is proportional to pr(\u03b8), but we don\u2019t know the constant of proportionality."
    },
    {
        "text": "knowing only the function s(\u03b8), how are we to make inferences about \u03c0?"
    },
    {
        "text": "the most general solution to this problem is an mcmc simulation."
    },
    {
        "text": "it gener- ates a sequence of samples \u03b8i, which form a markov chain, whose steady\u2010state distribution is equal to \u03c0. this means that it you generate enough samples, they will give you a representative sampling of \u03c0, and you can use them to estimate the average value of \u03b8 or any other statistic you desire."
    },
    {
        "text": "there are a number of mcmc algorithms, and i won\u2019t get into their details here."
    },
    {
        "text": "roughly though, to generate \u03b8i+1 we start with \u03b8i and then add some random perturbation \u03b4 and set \u03c6 = \u03b8i+\u03b4."
    },
    {
        "text": "\u03c6 is a candidate value for \u03b8i+1."
    },
    {
        "text": "using score(\u03b8) and score(\u03c6), we probabilistically either accept \u03c6 as the new \u03b8i+1, or we sample another \u03b4 and try again."
    },
    {
        "text": "in this way, \u03b8i+1 is a probabilistic function of \u03b8i but only \u03b8i."
    },
    {
        "text": "if you use an mcmc algorithm for accepting/rejecting \u03c6, and sample \u03b4 in a way that the every potential \u03b8 can be reached from every other, then you are guaranteed that the steady state of this markov chain will be \u03c0. if you use a clever way to sample \u03b4, then it will, on average, converge to \u03c0 quickly and the \u03b8i will be closer to independent samples."
    },
    {
        "text": "25.4 \u00adhidden markov models and the viterbi algorithm one of the most important uses of markov chains is as a building block in \u201chidden markov models\u201d or hmms."
    },
    {
        "text": "let me start with an example."
    },
    {
        "text": "imagine you are reading through some blurry text, trying to figure out what it says."
    },
    {
        "text": "let the correct words be denoted by random variables xi, and let the blurry pic- tures be denoted by the random variables yi; your goal is to guess the sequence of the xi given the known yi."
    },
    {
        "text": "25.4 hidden markov models and the viterbi algorithm 381 your first thought might be to just guess that xi is equal to whatever letter yi looks the most like, but that\u2019s not a perfect way to do it."
    },
    {
        "text": "imagine that several of the words look clearly like \u201cthe duck said,\u201d but the next word looks ambiguous; it could be either \u201cquick\u201d or \u201cquack,\u201d but it looks slightly more like \u201cquick.\u201d in this case, you would probably say that the word is \u201cquack,\u201d since it is vastly more likely given the context of talking about ducks."
    },
    {
        "text": "to take another example, let\u2019s say that i have two coins: one is fair, and the other is heads 90% of the time."
    },
    {
        "text": "after each coin toss, i have a 10% chance of switching coins for the next toss and 90% of staying with the same coin."
    },
    {
        "text": "let xi denote whether the ith toss was using the biased coin and yi denote whether it came up heads."
    },
    {
        "text": "can i identify the places where i switched from the fair to the weighted coin?"
    },
    {
        "text": "the situation where we have an underlying markov chain xi and observa- tions yi that are dependent only on their associated xi is called a \u201chidden markov model\u201d (hmm), and the xi are called the \u201chidden states.\u201d the depend- ency structure of an hmm is often visualized as follows: x1 x2 x3 x4 y1 y2 y3 y4 the hmm is characterized by \u25cf \u25cfthe initial probability distribution of x1 \u25cf \u25cfthe transition matrix of the xi \u25cf \u25cfthe conditional probability function pr(y|x)."
    },
    {
        "text": "there are a number of different analytical questions you can ask about markov chains, but the most popular one is what is the most likely sequence of the xi, given the observed yi."
    },
    {
        "text": "in our natural language example, knowing this sequence would give us a prediction of whether the ambiguous word is \u201cquick\u201d or \u201cquack.\u201d when throwing fair and biased coins, knowing the sequence would let us see when we switched from one coin to the other."
    },
    {
        "text": "i should note that finding the most likely sequence of xi is not the same as finding the likeliest value for each of the xi."
    },
    {
        "text": "to take a coin\u2010flipping example, say that the following sequences have the following probabilities, and all other sequences have probability 0: sequence probability hht 0.4 htt 0.3 hth 0.3"
    },
    {
        "text": "25 stochastic modeling 382 in this case, the likeliest sequence is hht with a probability of 40%, so the viterbi algorithm will give us x2 = h. however, x2 actually has a 60% chance of being t. it\u2019s just that that 60% probability mass is spread out among several different sequences."
    },
    {
        "text": "in the next section, i will discuss the viterbi algorithm, which lets us find this optimal sequence."
    },
    {
        "text": "it\u2019s the most complicated algorithm that i describe in this book, so i wanted to devote a full section to it."
    },
    {
        "text": "the times that hmms really shine are when the observations are very ambiguous, but the xi change only rarely."
    },
    {
        "text": "in these cases, even a human eyeball- ing the data often can\u2019t discern when the change happens, because the transi- tions are so subtle."
    },
    {
        "text": "i should note that in these cases, it is implicit in hmms that the length of time x has a particular value is geometrically distributed."
    },
    {
        "text": "this is a very strong assumption, which often isn\u2019t satisfied."
    },
    {
        "text": "in practice, this usually doesn\u2019t end up being much of a problem, so long as the length of time at a particular x tends to be long."
    },
    {
        "text": "25.5 \u00adthe viterbi algorithm the viterbi algorithm is the easiest to understand if you look at the possible under- lying states as what\u2019s called a \u201ctrellis diagram,\u201d showing all the possible hidden states and all the transitions between them."
    },
    {
        "text": "for concreteness, let\u2019s use the example of fair and biased coins."
    },
    {
        "text": "in that case, the trellis diagram looks like the following: x1 x2 fair fair fair biased biased biased x3 a sequence of hidden states corresponds to a path through this graph, from the first layer to the last."
    },
    {
        "text": "the probability associated with any particular path is probability = ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 ( ) \uf8eb \uf8ed\uf8ec \uf8f6 \uf8f8\uf8f7 \u220f \u220f + i i i i i i x x y x pr | pr | 1 this would be a good point to back up for a second."
    },
    {
        "text": "no large\u2010scale imple- mentation of the viterbi algorithm uses probabilities, because the probability of even the likeliest path is usually so small that the computer can\u2019t distinguish it from 0. so instead, we solve the equivalent problem of maximizing the log of this probability: score probability = ( ) = ( ) ( )+ ( ) ( ) \u2211 \u2211 + log log pr | log pr | i i i i i i x x y x 1"
    },
    {
        "text": "25.5 the viterbi algorithm 383 looking at it this way, you can see that a particular path\u2019s score is just the sum of the \u201cscores\u201d for all of it edges and nodes."
    },
    {
        "text": "here the score of an edge is the log of its transition probability, and the score of a node is the log of the probability of the observation we made (assuming that we are in that hidden state)."
    },
    {
        "text": "let\u2019s introduce two critical pieces of terminology: \u25cf \u25cfp(i, x) is the highest\u2010scoring path that goes up through the ith layer and ends at xi = x."
    },
    {
        "text": "\u25cf \u25cfs(i, x) is the score of p(i, x)."
    },
    {
        "text": "that is, p(i,x) is the best path through the trellis diagram that ends at a par- ticular node in its ith layer."
    },
    {
        "text": "given that we end up at (say) x\u2010fair on step 50, what is the highest\u2010scoring path that would take us to that point?"
    },
    {
        "text": "the key insight behind the viterbi algorithm (as applied to our case) is to find p(i,fair) by conditioning on whether the (i \u2212 1)th state is fair or biased."
    },
    {
        "text": "we then see that either p i p i ,fair 1,fair fair ( ) = \u2212 ( )+[ ] or p p i,fair i 1,biased fair ( ) = ( )+[ ] \u2212 whichever one has higher score."
    },
    {
        "text": "the score s(i, fair) will then be given by s i s i s i ,fair fair fair fair biased ( ) ( ) ( ) ( ) ( ) = \u2212 + \u2192 \u2212 + max , log pr , l 1 1 oog pr log pr | fair biased fair \u2192 + ( ) ( ) \uf8f1 \uf8f2 \uf8f3 \uf8fc \uf8fd \uf8fe ( ) ( ) yi so, we have now reduced the problem of finding p(i,fair) and p(i,biased) to the problems of finding p(i \u2212 1,fair) and p(i \u2212 1,biased)."
    },
    {
        "text": "if we can find those, then it is straightforward to find for i instead of i \u2212 1. the viterbi algorithm works by walking through the trellis diagram, filling in all of the s(i,x) one layer at a time."
    },
    {
        "text": "whenever it calculates s(i,x), it keeps track of that node\u2019s \u201cparent,\u201d that is, the node in the previous layer that led to it."
    },
    {
        "text": "when these have all been filled out, the pseudocode for the algorithm is then input: observations y1, y2, ..., yn initial probabilities pr(x1) transition probabilities pr(xi->xj) observation probabilities pr(y|x) initialization: construct the trellis diagram for j=1...k: node(1,j).score = pr(y1|j)"
    },
    {
        "text": "25 stochastic modeling 384 processing: for i=2...n for j=1...k node i j parent argmax node i x score ln x j x ,( ) = \u2212 ( ) + ( ) ( ) { } ."
    },
    {
        "text": ", ."
    },
    {
        "text": "pr | 1 node i j score node i j parent score ln y j i , , ( ) = ( ) + ( ) ( ) ."
    },
    {
        "text": ". . pr | constructing the output: outputstates n argmax node n x score x ( ) = ( ) { } , ."
    },
    {
        "text": "for i=n\u20101,...,1: outputstates i outputstates i parent ( ) = + ( ) 1 . output: outputstates 25.6 \u00adrandom walks one of the simplest stochastic process is called a random walk or sometimes a \u201cdrunkard\u2019s walk.\u201d the idea is that x1 is some integer, usually 0. then, in general, x x p x p i i i + = + \u2212 \u2212 \uf8f1 \uf8f2\uf8f4 \uf8f3\uf8f4 1 1 1 1 with probability with probability the motivation that is given for this model is that you have somebody who is very drunk and doesn\u2019t know where they are going."
    },
    {
        "text": "at each point in time, they have a probability p of taking a step to the right and 1 \u2212 p of stepping to the left."
    },
    {
        "text": "if p = 0.5, the walk is called \u201cunbiased.\u201d in this case, the walker will drift around through space aimlessly."
    },
    {
        "text": "in the long term, they will pass through the origin an infinite number of times."
    },
    {
        "text": "their average location will always be at x = 0, but that\u2019s just because they are equally likely to be on the left or right."
    },
    {
        "text": "their average distance from the origin will scale as the square root of the num- ber of steps they\u2019ve taken."
    },
    {
        "text": "to be more precise, each step is a bernoulli(p) random variable, so x x p n n i n i + \u2212 = \u2217 \u2212 2 binomial ( , ) 25.7 \u00adbrownian motion brownian motion is a generalization of random walks from the integers to the real numbers defined by x x i i + \u2212 = ( ) 1 2 0 normal ,\u03c3 the only parameter to the model is \u03c3, which measures how far it is likely to move in a single time step."
    },
    {
        "text": "25.8 arima models 385 historically, brownian motion was the first stochastic process to be studied extensively."
    },
    {
        "text": "it was used to model the location of a particle suspended in a liq- uid, as it floats around aimlessly."
    },
    {
        "text": "the most notable aspect of brownian motion mathematically is that the dif- ference between nonconsecutive locations is still a brownian motion, just one with wider spread: x x t i t i + \u2212 = ( ) normal 0 2 , *\u03c3 this is important for many applications."
    },
    {
        "text": "the motion of a particle, for instance, isn\u2019t really in discrete time steps."
    },
    {
        "text": "it moves continuously throughout time."
    },
    {
        "text": "but if we measure its location every millisecond, every second, or every hour, we will still see a brownian motion."
    },
    {
        "text": "at very small time scales, the motion of a particle is more like a random walk, as it gets jostled about by discrete collisions with water molecules."
    },
    {
        "text": "but virtually any random movement on small time scales will give rise to brownian motion on large time scales, just so long as the motion is unbiased."
    },
    {
        "text": "this is because of the central limit theorem, which tells us that the aggregate of many small, independent motions will be normally distributed."
    },
    {
        "text": "outside of physics, the most important application of brownian motion is probably finance."
    },
    {
        "text": "when modeling the movement of a stock\u2019s price, we want to capture the idea that, absent any hunches or business developments, it will wander around aimlessly."
    },
    {
        "text": "this sounds like brownian motion, but there is a catch; a 10% drop in price is equally significant no matter the starting price, and the price can never go negative."
    },
    {
        "text": "to achieve this \u201cscale\u2010free brownian motion,\u201d we let x denote the logarithm of the security\u2019s price, rather than the price itself."
    },
    {
        "text": "is this an accurate model of how the prices of real stocks and bonds evolve?"
    },
    {
        "text": "no, it isn\u2019t."
    },
    {
        "text": "however, it is a very common \u201cnull hypothesis\u201d model, where we don\u2019t assume that there are any long\u2010term trends, and we don\u2019t assume that there is any \u201ccorrect\u201d price that the motion tends to stay around."
    },
    {
        "text": "25.8 \u00adarima models for data science applications, the most important problem with brownian motion is that it moves around without any sense of a \u201cnormal\u201d value."
    },
    {
        "text": "real processes tend to have a baseline that they hover around."
    },
    {
        "text": "the price of a secu- rity is a great example; it will fluctuate up and down, but not too far in either direction."
    },
    {
        "text": "this behavior is called \u201cmean reverting,\u201d since you can think of it as an elastic force pulling the random variable back toward its average value."
    },
    {
        "text": "the classical way to model this is called an \u201cautoregressive moving average\u201d or arima."
    },
    {
        "text": "in general, an arima model is defined by"
    },
    {
        "text": "25 stochastic modeling 386 x c x x x i i i k i k + \u2212 \u2212 = + + + + + ( ) 1 0 1 1 2 0 \u03c6 \u03c6 \u03c6 normal ,\u03c3 however, you will typically just see this taken out to a single term: x c x i i + = + + ( ) 1 0 2 0 \u03c6 normal ,\u03c3 where 0 < \u03c6 < 1. in this case, there is a long\u2010term average value of e x c [ ] = \u2212 1 \u03c6 x will fluctuate around this average value, sometimes randomly moving fairly far from it."
    },
    {
        "text": "but it will always be pulled back to the mean."
    },
    {
        "text": "25.9 \u00adcontinuous-time markov processes let\u2019s move back to markov chains that have only a finite number of states \u2013 the frog hopping between lily pads."
    },
    {
        "text": "a limitation of these markov chains is that they occur at discrete time steps."
    },
    {
        "text": "sometimes, this granularity is appropriate, such as words in text."
    },
    {
        "text": "in other situations though, we are monitoring a system in real time, and it could change its state at any moment."
    },
    {
        "text": "for example, a server that is hosting a website could get a new visitor to handle at any point in time, and a visitor could leave at any point too."
    },
    {
        "text": "basically, our amnesiac frog has no idea how long it has been since he arrived at his current lily pad, and he could hop away at any moment."
    },
    {
        "text": "the best way to think of a continuous\u2010time markov process is that you have a collection of rates \u03bbij from state i to state j. imagine time to be broken up into very small moments of length \u03b4. then, for each distinct i and j, pr | x j x i t t ij + = = ( ) = \u2206 \u2206 \u03bb the probability of staying in state i is just 1 minus the sum of all of these probabilities of changing."
    },
    {
        "text": "\u03bbij is the rate at which probability mass flows from state i to state j, as a fraction of the mass in state i. in calculus terms, if we let p(t) denote the probability distribution over states at time t, then d dt p t p t ( ) = ( )\u03bb where \u03bb is a matrix of all the \u03bbij (with 0s on the diagonal)."
    },
    {
        "text": "we can find the steady\u2010state distribution \u03c0 by solving"
    },
    {
        "text": "25.10 poisson processes 387 \u03c0 \u03c0 \u03bb = = \u2211 0 1 i i similar to what we did with discrete\u2010time markov chains."
    },
    {
        "text": "there is a critical difference between \u03c0 for discrete and continuous chains though."
    },
    {
        "text": "for discrete markov chains, \u03c0 measured how often we transition into each state."
    },
    {
        "text": "in continuous processes, \u03c0 measures the fraction of all time that is spent in each state."
    },
    {
        "text": "these are not the same thing, because it is possible for a lot of probability mass to flow into a particular state, but to then flow out very quickly."
    },
    {
        "text": "so, the system transitions into this state very often but only stays there briefly before moving on to a more long\u2010lived state."
    },
    {
        "text": "continuous\u2010time processes and discrete chains are both characterized by matrices, but those matrices are quite different."
    },
    {
        "text": "both need all nonnegative entries, but the similarity stops there."
    },
    {
        "text": "for markov chains, each row must sum to 1. for continuous\u2010time processes, the sums are irrelevant, just so long as the diagonal of the matrix is always 0."
    },
    {
        "text": "25.10 \u00adpoisson processes a poisson process is used to model a stream of events that occur at random intervals."
    },
    {
        "text": "it is characterized by a single parameter \u03bb, which is the average num- ber of events per unit time."
    },
    {
        "text": "alternatively, it is sometimes characterized by \u03b8 = 1/\u03bb, the average time between events."
    },
    {
        "text": "there are a number of ways to think about a poisson process, but in my mind, the easiest is this: break time up into many small intervals of size \u03b4. each interval, independently of all the others, has probability \u03bb\u03b4 of having an event."
    },
    {
        "text": "in the limit of \u03b4 being small, we converge to a poisson process."
    },
    {
        "text": "it has the fol- lowing properties: \u25cf \u25cfthe interarrival times between consecutive events and exponentially distrib- uted, with mean = \u03b8. this means that the time can be arbitrarily long but is weighted toward short time."
    },
    {
        "text": "\u25cf \u25cfconsecutive interarrival times are independent."
    },
    {
        "text": "\u25cf \u25cffor any time interval of length t, the number of events that occur in it is a poisson distribution with mean \u03bbt."
    },
    {
        "text": "this means there can be arbitrarily many events, but huge outliers are not likely."
    },
    {
        "text": "\u25cf \u25cfif two intervals don\u2019t overlap, the number of events in them are independent of each other."
    },
    {
        "text": "poisson processes are a fantastic way to model many real\u2010world systems."
    },
    {
        "text": "it is illustrative though to highlight several types of system that are not poisson:"
    },
    {
        "text": "25 stochastic modeling 388 \u25cf \u25cfone event tends to precipitate other events in rapid succession."
    },
    {
        "text": "an example might be trades that are made of a stock, where one person making a trade causes many other people to trade in reaction."
    },
    {
        "text": "\u25cf \u25cfan external force causes events to come in bursts."
    },
    {
        "text": "for example, visitors to a website might come in bursts because they all saw the same link that was posted."
    },
    {
        "text": "\u25cf \u25cfthere is a tendency toward even spacing between events."
    },
    {
        "text": "if we replace a device whenever it wears out and breaks, we will probably not have to replace it again for a while because its parts are not worn out."
    },
    {
        "text": "this would mean that the time between events is not exponentially distributed."
    },
    {
        "text": "\u25cf \u25cfthere is a small population of events that can happen, and they happen only once within a period."
    },
    {
        "text": "for example, an array of machines might fail at irregular intervals, and i fix any broken ones every morning."
    },
    {
        "text": "this means that if there are many failures early in the day, there will be fewer failures later in the day, because there are fewer machine running that could poten- tially fail."
    },
    {
        "text": "the first two cases tend to be larger problems in practice, and their key prop- erty is that the different events are not independent of each other."
    },
    {
        "text": "25.11 \u00adfurther reading harchol\u2010balter, m, performance modeling and design of computer systems: queueing theory in action, 2013, cambridge university press, cambridge, uk."
    },
    {
        "text": "ross, s, introduction to probability models, 9th edn, 2006, academic press, waltham, ma."
    },
    {
        "text": "feller, w, an introduction to probability theory and its applications, vol."
    },
    {
        "text": "1, 3rd edn, 1968, wiley, hoboken, nj."
    },
    {
        "text": "25.12 \u00adglossary absorbing state a state in a markov chain that goes only to itself."
    },
    {
        "text": "absorbing markov chain a markov chain with at least one absorbing state."
    },
    {
        "text": "ergodic markov chain a markov chain in which every state can be reached from any other."
    },
    {
        "text": "equilibrium distribution a probability distribution over the states of a markov chain that stays the same as the chain evolves by one time step."
    },
    {
        "text": "markov property the key assumption for markov chain."
    },
    {
        "text": "a state in a markov chain can depend probabilistically on the state right before it, but only the one right before it."
    },
    {
        "text": "if you condition on knowing xi, then xi+1 is independent of all xj with j < i."
    },
    {
        "text": "25.12 glossary 389 markov chain a collection of states and transition probabilities between them, where each state depends only on the one before."
    },
    {
        "text": "many markov chains also require a probability distribution over the starting state."
    },
    {
        "text": "poisson process a way to model sequences of events that happen at random intervals."
    },
    {
        "text": "the times between consecutive events are i.i.d and exponentially distributed."
    },
    {
        "text": "stationary distribution synonym for equilibrium distribution."
    },
    {
        "text": "transition matrix a matrix specifying the probability of transitioning from every state to every other in a discrete markov chain."
    },
    {
        "text": "the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. 391 so you\u2019ve read this book, and let\u2019s assume for the sake of argument that you thought the subject matter was pretty cool (and that the writing was brilliant, of course)."
    },
    {
        "text": "what now?"
    },
    {
        "text": "my most important advice is to get out there and start tackling some real problems."
    },
    {
        "text": "i\u2019ve done work as a software engineer and an academic, and i\u2019m constantly impressed by how much more intellectually dynamic data science is than anything else i\u2019ve done."
    },
    {
        "text": "in a single day, i will flit between low\u2010level debug- ging, designing software architecture, helping clients to translate a business problem into math, and brushing up on my linear algebra."
    },
    {
        "text": "in data science, there is always something new that you can learn, and usually something new that you have to learn, and no book can substitute for real experience in that kind of environment."
    },
    {
        "text": "as far as broadening your knowledge base, there are several directions (not mutually exclusive) that you might consider growing: \u25cf \u25cfreally the best, if you have a particular area of application in mind, is to become more of a domain expert in whatever it is you want to apply data science to."
    },
    {
        "text": "remember that the key to doing great data science is to ask the right questions, and the only way to do this is to have a deep understanding of the domain you\u2019re studying."
    },
    {
        "text": "\u25cf \u25cfa lot of data scientists almost double as software engineers."
    },
    {
        "text": "i would probably put myself in this category."
    },
    {
        "text": "they know a number of additional programming languages, they\u2019ve written multithousand\u2010line pieces of code with many interacting parts, and they are well versed in their computer science funda- mentals."
    },
    {
        "text": "this is a great direction to go if you want to work in a start\u2010up\u2010type environment or in big data."
    },
    {
        "text": "\u25cf \u25cfsome people get deeply immersed in machine learning."
    },
    {
        "text": "this will serve you well if you want work that is more academic in flavor or if you specialize in solving a few extraordinarily hard problems at a large company."
    },
    {
        "text": "\u25cf \u25cfsome data scientists grow in the direction of statistics, learning more about a/b testing, how to design experiments, and all the various things you can do parting words: your future as a data scientist"
    },
    {
        "text": "parting words: your future as a data scientist 392 to eke insights out of small datasets."
    },
    {
        "text": "this approach is more common in large companies, where people have the ability to specialize."
    },
    {
        "text": "the range of datasets available to study is changing quickly, but i want to point out a few trends that will probably be relevant to you: \u25cf \u25cfdata science to date has been dominated by data from the internet (web logs, click\u2010throughs on ads, etc."
    },
    {
        "text": "), but the emphasis is shifting toward data from the physical world."
    },
    {
        "text": "\u25cf \u25cfsmartphones produce a lot of cool data and will continue to play a more central role in data analysis."
    },
    {
        "text": "in particular, they measure a number of different things at once (such as location, motion, and whatever you are looking at on your phone), and very few people have done much to bring these different data streams together in a unified analysis."
    },
    {
        "text": "\u25cf \u25cfthe \u201cinternet of things\u201d refers to the idea that sensors are becoming ubiqui- tous, and many devices that are neither computers nor cell phones will be producing data that can be accessed through the cloud."
    },
    {
        "text": "this means sensor nets, machines in factories, toasters \u2013 you name it."
    },
    {
        "text": "\u25cf \u25cfthe \u201cquantified self\u201d movement refers to the fact that there is more and more measurement of one\u2019s own biological stats."
    },
    {
        "text": "this means everything from heart rate, to blood glucose, to breathing rate."
    },
    {
        "text": "as wearable sensors become cheaper and more ubiquitous, more of this data is becoming fine\u2010 grained time series."
    },
    {
        "text": "the software and hardware tools we use are also evolving."
    },
    {
        "text": "for example: \u25cf \u25cfi didn\u2019t have a chance to get to this in the book, but graphics processing units (gpus) are specialized hardware that include many processing cores for massively parallel processing."
    },
    {
        "text": "they aren\u2019t always helpful, but they can be extraordinarily powerful in many cases."
    },
    {
        "text": "deep learning is a good example of an area that really benefits from them."
    },
    {
        "text": "\u25cf \u25cfpeople will probably realize that cluster computing is very overhyped right now, and some of the wind will probably get lost from the big data sails."
    },
    {
        "text": "\u25cf \u25cfdon\u2019t get me wrong though: big data is still going to remain a huge deal!"
    },
    {
        "text": "for many datasets, a cluster will make the analysis much quicker; in other cases, it will be the only way to get the job done."
    },
    {
        "text": "the big difference is that people will be better at identifying when a cluster is or isn\u2019t the right tool for the job."
    },
    {
        "text": "i also expect that big data will become much more seamlessly integrated with local machines and that it will often be invisible to the user whether their computation is running on their local machine or a cluster in the cloud."
    },
    {
        "text": "data science in the coming years is going to be a very exciting journey."
    },
    {
        "text": "hopefully, this book has gotten you off on the right foot."
    },
    {
        "text": "best regards, field cady"
    },
    {
        "text": "393 the data science handbook, first edition."
    },
    {
        "text": "field cady."
    },
    {
        "text": "\u00a9 2017 john wiley & sons, inc. published 2017 by john wiley & sons, inc. a absorbing markov chain 377\u2013379 adjusted rand index 138\u2013139, 150 agglomerative clustering 147\u2013148 agile development 225\u2013226 amortized complexity 318\u2013320 anonymous function 33 area under the roc curve 116\u2013117 artificial intelligence 88\u201389 ascii 180\u2013182 b bag-of-words 231\u2013235 bayesian network 110\u2013112, 293\u2013294, 358, 367\u2013373 bayesian statistics 282\u2013283, 291\u2013295 bernoulli random variable 261\u2013262 big data 2, 13, 22, 33, 45, 167, 185\u2013201, 211, 281, 284, 301, 391, 392 big-o notation 314\u2013320 binomial random variable 271\u2013272 bit 177\u2013178 bonferroni correction 285\u2013286 bson 212 byte 177\u2013178, 180\u2013182, 331\u2013333 c cache (computer memory) 175, 321\u2013324 central limit theorem 272\u2013273 cluster (analytics) 135\u2013139, 144\u2013150 cluster (of computers) 94, 185, 185\u2013213, 392 combiner 199 comic sans 128 compiler 301, 305\u2013308, 333, 335 complexity 314 compression 166\u2013168, 174\u2013175 confusion matrix 114\u2013115 consistent estimator 286\u2013287 constructor 304 continuous random variable 263\u2013265 convex function 354\u2013355 convex optimization 350\u2013355 convex region 354 convolutional neural network 361\u2013362 correlation of residuals 160 covariance matrix 147, 274 c.r.a.p design 125\u2013128 cumulative distribution function 263 d dataframe 38\u201342 decision tree 101\u2013103 deep learning 112, 357\u2013367 denoising 251\u2013252 dict (python class) 24\u201325, 32, 34 discrete random variable 262 document store 211\u2013212 index"
    },
    {
        "text": "index 394 duck typing 308 dummy variables 93 dynamic typing 308 e eigenface 138\u2013142 ensemble classifier 104 entity recognition 240 entropy 277\u2013278, 294\u2013295 equilibrium distribution (markov chains) 378\u2013379 estimator 286 expectation value 267\u2013269 exploratory analysis 13\u201314 exponential random variable 274\u2013276, 387 f f1 score 118 false positive rate 114\u2013116 fourier transform 256\u2013258 functional programming 298\u2013301 g garbage collection 323 gaussian distribution 272\u2013274 gaussian mixture model 146\u2013147 git 220\u2013222 global optimum 354 gradient 350\u2013351 gradient descent 351\u2013355 h hadoop 187\u2013198 hashable type 36\u201337 hash function 339\u2013340 hash table 339\u2013340 heap 329, 335\u2013337 heavy-tailed distribution 269\u2013271 hypothesis testing 283\u2013286 i imperative programming 298\u2013301 inheritance (classes) 304\u2013305 integration test 224\u2013225 interpolation 247, 249\u2013251 iterator 56 j join 30, 40\u201342, 193, 207\u2013208 just-in-time compilation 307 k kendall correlation 78\u201379 key\u2013value store 210\u2013211 k-means clustering 145\u2013146 knowledge base 240\u2013241 l latex 128 least squares 156\u2013159 lemma 234, 236\u2013237 linked list 340\u2013342 list (python class) 25\u201331 local optimum 350 logarithmic plot 74\u201376 logistic regression 108\u2013110 log-normal distribution 276\u2013277 m machine learning 87\u201391 mapreduce 197\u2013200 markov chain 375\u2013381 markov chain monte carlo 379\u2013380 maximum likelihood estimation 345\u2013348 memory leak 336\u2013337 memoryless distribution 275\u2013276 mixture model 145\u2013146 mongodb 212\u2013214 multiple hypothesis testing 285\u2013286 mutual information 150 mysql 204\u2013210 n naive bayes 110\u2013112, 293\u2013294 neural network 112\u2013114, 358\u2013366"
    },
    {
        "text": "index 395 normal distribution 272\u2013273 null hypothesis 284\u2013285 numpy 37\u201338 o object-oriented programming 298, 301\u2013305 optimization (numerical) 348\u2013355 orphaned memory 337 overfitting 89\u201390 p pandas 36\u201342 part-of-speech 237\u2013238 passing by reference 334\u2013335 passing by value 334\u2013335 pearson correlation 78 pointer 333\u2013337 poisson process 387\u2013388 poisson random variable 272 precision 114 principal component analysis 138\u2013144 prior 283, 292\u2013295 probability density function 263 probability mass function 262 programming paradigm 297\u2013298 project requirements document 11 pseudorandom numbers 263\u2013265 pure function 32 p-value 284\u2013286 pymc 358, 370\u2013373 pyspark 189\u2013197 q quantile 69\u201370 query language 204 r r2 159\u2013160 rand index 149\u2013150 random forest 103\u2013104 rdd see resilient distributed dataset (rdd) recall 114 recurrent neural network 366\u2013367 reduce 197\u2013200 regular expression 55\u201360 relational database 204\u2013210 resampling 249\u2013251 resilient distributed dataset (rdd) 190\u2013194 roc curve 115\u2013117 s seasonal decomposition 252\u2013253 sentiment analysis 230\u2013232, 239\u2013240 set (python class) 32 side effect 32\u201333, 298 sigmoid function 108\u2013110, 113, 363 silhouette score 148\u2013149 sliding window 244, 255 spark 185, 189\u2013197, 199\u2013200 spearman correlation 79 spline 250 sql 204 stack 329\u2013330 standard deviation 69\u201370, 80, 267\u2013277 static typing 308 stem 236\u2013237 stochastic gradient descent 355 stop word 236 struct 332\u2013333 supervised learning 89 support vector machine 105\u2013108 t tabular data 14, 70, 87\u201388 technical debt 226 tensor 362\u2013363 term frequency\u2013inverse document frequency (tf-idf) 235 test-driven development 225\u2013226 testing data 89\u201390 test statistic 284, 288"
    },
    {
        "text": "index 396 tf-idf see term frequency\u2013inverse document frequency (tf-idf) tokenization 233 topic modeling 240 training data 89\u201390 transition matrix 276 true positive rate 114 t-test 287\u2013289 tuple 29, 31 u unbiased estimator 286 unhashable type 36\u201337 uniform random variable 263\u2013265 unit test 223\u2013225 unsupervised learning 89 utf-8 182 v variance 268 virtual address space 329\u2013330 virtual machine 307 w wide column store 211"
    },
    {
        "text": "wiley end user license agreement go to www.wiley.com/go/eula to access wiley\u2019s ebook eula."
    }
]
